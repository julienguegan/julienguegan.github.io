<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-12-18T23:15:39+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Blog du Julien</title><subtitle>Blog personnel de Julien GUEGAN, ingénieur calcul et IA. Vous y trouverez divers articles sur des sujets qui l&apos;intéressent, la plupart du temps scientifiques et techniques.</subtitle><author><name>Julien Guégan</name></author><entry xml:lang="fr"><title type="html">Kalman : filtre, tracking, IMU</title><link href="http://localhost:4000/posts/fr/2022-02-24-filtre_kalman/" rel="alternate" type="text/html" title="Kalman : filtre, tracking, IMU" /><published>2022-02-25T02:00:00+01:00</published><updated>2022-02-25T02:00:00+01:00</updated><id>http://localhost:4000/posts/fr/filtre_kalman</id><content type="html" xml:base="http://localhost:4000/posts/fr/2022-02-24-filtre_kalman/">Le filtre de Kalman est une méthode très répandue dans le milieu de l&apos;ingénieurie puisqu&apos;elle posséde de nombreuses applications en localisation, navigation, pilotage automatique, suivi d&apos;objet, fusion de données ...  Il fut introduit en 1960 par l&apos;ingénieur *Rudolf E. Kálmán* et fut notamment utilisé pour l&apos;estimation de trajectoire pour le programme Apollo. En effet, à partir d&apos;une série de mesures observées au cours du temps (bruitée et biaisée), il permet de calculer une estimation de ces variables inconnues souvent plus précise que les mesures en se basant sur les théories du contrôle et des statistiques. L&apos;une des forces de ce filtre est sa capacité à s&apos;améliorer au cours du temps en intégrant un terme d&apos;erreur du modèle lui-même. Il a de nombreux autres avantages : fusionner les mesures de capteurs différents, fonctionner en ligne, facile à implémenter ...

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/kalman_header.png&quot; width=&quot;80%&quot;/&gt;
&lt;/p&gt;

# Principe

La construction du filtre de Kalman part de 3 hypothèses importantes : 
 - le système modélisé est **linéaire** : il peut être modélisé comme une multiplication entre l&apos;état $t$ et $t-1$.
 - le bruit des mesures est **blanc** : il est non corrélé avec le temps.
 - le bruit est **gaussien** : il est décrit par une moyenne et une covariance 

L&apos;idée consiste à construire un modèle pour l&apos;état du système qui *maximise la probabilité a posteriori de ces mesures précédentes*. Cela signifie que le nouveau modèle que nous construisons après avoir effectué une mesure (en tenant compte à la fois de notre modèle précédent avec son incertitude et de la nouvelle mesure avec son incertitude) est le modèle qui a la plus forte probabilité d&apos;être correct. De plus, on peut maximiser la probabilité *a posteriori* sans conserver un long historique des mesures précédentes elles-mêmes. Au lieu de cela, on met à jour de manière itérative le modèle de l&apos;état du système et on ne garde que ce modèle pour la prochaine itération. Cela simplifie grandement l&apos;implication de calcul de cette méthode. 

## Cas unidimensionnel statique

Supposons qu&apos;on veuille savoir où est positionner un point fixe sur une ligne et qu&apos;on ait 2 mesures bruitées $x_1$ et $x_2$. Chacune de ces mesures suit une distribution gaussienne :

$$p_i(x) = \frac{1}{\sigma_i\sqrt{2\pi}} e^{-\frac{(x-\bar{x}_i)^2}{2\sigma_i^2} } \quad (i=1,2)$$

On peut alors montrer que la combinaison de ces 2 mesures gaussiennes est équivalente à une seule mesure gaussienne caractérisée par la moyenne $\bar{x}_{12}$ et la variance $\sigma\_{12}^2$ :

$$
\begin{aligned}
  \bar{x}_{12} &amp;= \left( \frac{\sigma_2^2}{\sigma_1^2+\sigma_2^2} \right)x_1 + \left( \frac{ \sigma_1^2}{\sigma_1^2+\sigma_2^2} \right)x_2 \\ \\
  \sigma_{12}^2 &amp;= \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2}
\end{aligned}
$$

Ainsi, la nouvelle valeur moyenne $\bar{x}_{12}$ n&apos;est qu&apos;une combinaison pondérée des deux mesures par les incertitudes relatives. Par exemple, si l&apos;incertitude $\sigma_2$ est particulièrement plus grande que $\sigma_1$, alors la nouvelle moyenne sera très proche de $x_1$ qui est plus certaine.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/merge_gaussian.gif&quot; width=&quot;50%&quot;/&gt;
&lt;/p&gt;

Maintenant, si on part du principe que l&apos;on effectue nos 2 mesures l&apos;une après l&apos;autre et qu&apos;on cherche donc à estimer l&apos;état courant de notre système $(\hat{x}_t,\hat{\sigma}_t)$. Au temps $t=1$, on a notre première mesure $\hat{x}_1=x_1$ et son incertitude $\hat{\sigma}_1^2=\sigma_1^2$. En substituant ceci dans nos équations d&apos;estimation optimales et en les réarrangeant pour séparer l&apos;*ancienne* informations de la *nouvelle*, on obtient :

$$
\begin{aligned}
  \hat{x}_2 &amp;= \hat{x}_1 + \frac{\hat\sigma_1^2}{\hat{\sigma_1^2}+\sigma_2^2}(x_2 - \hat x_1 ) \\ \\
  \hat \sigma_2^2 &amp;= \left( 1 - \frac{\hat \sigma_1^2}{\hat\sigma_1^2+\sigma_2^2} \right) \hat \sigma_1^2
\end{aligned}
$$

En pratique, on appelle couramment le terme $x_2 - \hat x_1$ l&apos;**innovation** et on note le facteur $K = \frac{\hat \sigma_1^2}{\hat \sigma_1^2+\sigma_2^2}$ le **gain de mise à jour**. Au final, on peut écrire la relation de récurrence au temps $t$ : 

$$
  \begin{aligned}
    \hat x_t &amp;= \hat x_{t-1} + K (x_t - \hat x_{t-1}) \\ \\
    \hat \sigma_t^2 &amp;= (1 - K) \hat \sigma_{t-1}^2
  \end{aligned}
$$

**Attention** : Dans la littérature, on voit plus souvent l&apos;indice $k$ pour décrire le pas de temps (ici noté $t$).
{: .notice--warning}

Si on regarde la formule et à la valeur du gain de Kalman $K$, on comprend que si le bruit de mesure est élevé ($\sigma^2$ élevé) alors $K$ sera proche de $0$ et l&apos;influence de la nouvelle mesure $x_t$ sera faible. Au contraire, si $\sigma^2$ est petit, l&apos;état du système $\hat x_t$ sera ajusté fortement vers l&apos;état de la nouvelle mesure. 


## Cas unidimensionnel dynamique

On a considéré précemment le cas d&apos;un système statique dans un état $x$ ainsi qu&apos;une série de mesures de ce système. Dans un cas dynamique où l&apos;état du système varie au cours du temps, on divise l&apos;estimation du filtre de Kalman en 2 étapes : 
 - la **phase de prédiction** : on utilise les informations passées et le modèle dynamique pour prédire l&apos;état prochain du système. On prédit également la covariance de l&apos;erreur. Elle modélise l&apos;état du système.
 - la **phase de correction** ou **mise à jour** : on combine la prédiction faite avec une nouvelle mesure pour affiner le modèle et l&apos;estimation de l&apos;état du système ainsi que la covariance de l&apos;erreur. Elle modélise la mesure du système.

 Par exemple, si on mesure la position d&apos;une voiture au temps $t-1$ puis au temps $t$. Si la voiture a une vitesse $v$ alors on n&apos;intègre pas directement la nouvelle mesure directement. D&apos;abord, on *fast-forward* notre modèle basé sur ce qu&apos;on savait au temps $t-1$ pour avoir une prédiction de l&apos;état au temps $t$. De cette manière, la nouvelle mesure acquise au temps $t$ est fusionnée non pas avec l&apos;ancien modèle du système mais avec l&apos;ancien modèle du système projeté vers l&apos;avant au temps $t$

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/schemas_kalman.png&quot; width=&quot;90%&quot;/&gt;
&lt;/p&gt;

En partant de l&apos;hypothèse que la dynamique du système modélisé est linéaire, la **phase de prédiction** s&apos;écrit alors :

$$
\left.
  \begin{aligned}
    \hat x_t        &amp;= a \ \hat x_{t-1} \\
    \hat \sigma_t^2 &amp;= a^2 \ \hat \sigma_{t-1}^2 \\
    &amp;\scriptsize \color{blue}  \text{car $var(ax+b) = a^2 var(x)$}
  \end{aligned}
\right.
$$

Et la **phase de correction** calculée dans la section précédente :

$$
\left.
  \begin{aligned}
    \hat x_t        &amp;= \hat x_{t-1} + K (z_t - \hat x_{t-1}) \\
    \hat \sigma_t^2 &amp;= (1 - K) \hat \sigma_{t-1}^2 \\
  \end{aligned}
\right.
$$

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/kalman_1D.gif&quot; width=&quot;90%&quot;/&gt;
&lt;/p&gt;


## Généralisation

On peut étendre les équations précédentes au cas multidimensionnel où l&apos;état de notre système est défini par plusieurs grandeurs. On cherche alors à estimer l&apos;état du système $\hat X \in \mathbb{R}^d$ à l&apos;instant $t$ ainsi que sa matrice de covariance associée $\hat P \in \mathbb{R}^{d \times d}$ (avec $d$ la dimension du système). Les équations deviennent :

**phase de prédiction**

$$
\left.
  \begin{aligned}
    \hat X_t &amp;= A \hat X_{t-1} \\
    \hat P_t &amp;= A P_{t-1} A^T + Q
  \end{aligned}
\right.
$$

où $A \in \mathbb{R}^{d \times d}$ est la matrice de transition d&apos;état modélisant la dynamique du système et $Q \in \mathbb{R}^{d \times d}$ la matrice de covariance du bruit de processus capturant les erreurs non modélisées par $A$ (plus elle est grande, plus on fait confiance aux mesures plutôt qu&apos;aux prédictions du modèle dynamique).

**phase de correction**

$$
\left.
  \begin{aligned}
    \hat X_t &amp;= \hat X_t + K(Z_t - \hat X_t) \\
    \hat P_t &amp;= \hat P_t - K \hat P_t
  \end{aligned}
\right.
$$

avec le gain de Kalman $ K = \hat P_t (\hat P_t + R)^{-1} $

où $Z \in \mathbb{R}^d$ est la mesure du système (ou observation) et $R \in \mathbb{R}^{d \times d}$ la matrice de covariance de la mesure qui modélise l&apos;erreur des mesures.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/kalman_2d_line.gif&quot; width=&quot;90%&quot;/&gt;
&lt;/p&gt;

Il existe des versions plus élaborées du filtre de Kalman qui peuvent prendre en entrée une commande $U$ envoyée au système. On trouve également fréquemment la matrice d&apos;observation $H \in \mathbb{R}^{d \times m}$ reliant l&apos;état réel du système au variables observées, en effet on peut modéliser un système à $d$ dimensions mais seulement observer $m$ de ses variables ($m&lt;d$). La phase de prédiction reste la même mais la phase de correction est alors : 

$$ 
\begin{aligned}
  \hat X_t &amp;= \hat X_t + K(\textcolor{blue}{H} Z_t - \hat X_t) \\
  \hat P_t &amp;= \hat P_t - K \textcolor{blue}{H} \hat P_t 
\end{aligned}$$

avec le gain de Kalman $ K = \hat P_t \textcolor{blue}{H^t} (\textcolor{blue}{H} \hat P_t \textcolor{blue}{H^T} + R)^{-1} $

# Exemples

Le filtre de Kalman est un outil générique et ces équations peuvent facilement s&apos;implémenter en quelques lignes :

```python
def kalman_estimation(X_est, P_est, Z_obs):
    # state prediction
    X_est = A @ X_est
    P_est = A @ P_est @ A.T + Q  
    # observation
    Z_pred = H @ X_est
    # kalman gain
    K = P_est @ H.T @ np.linalg.inv(H @ P_est @ H.T + R)
    # correction phase
    if Z_obs:
      X_est = X_est + K @ (Z_obs - Z_pred)
      P_est = P_est - K @ H @ P_est
    # return final estimation
    return X_est, P_est
```

**Note:** Si l&apos;observation n&apos;est pas disponible à l&apos;instant $t$, on execute seulement la phase de prédiction du filtre de Kalman pour avoir une estimation grâce au modèle dynamique.
{: .notice--info}

Cependant, la partie cruciale du problème réside la plupart du temps dans la définition des paramètres du système $A$, $Q$, $H$ et $R$ afin que le filtre fonctionne correctement. De plus, il peut devenir puissant lorsqu&apos;il permet d&apos;estimer une grandeur qui n&apos;est pas mesurée comme dans les exemples ci-dessous avec la vitesse d&apos;un objet et le biais d&apos;un gyroscope.


## Suivi d&apos;objet (Tracking)

On veut implémenter un filtre de Kalman appliqué à un problème de suivi d&apos;objet sur une image. L&apos;objet est repéré par un détecteur d&apos;objet basique en regardant un intervalle de couleur dans l&apos;[espace HSV](https://en.wikipedia.org/wiki/HSL_and_HSV) qui retourne la position $(x,y) \in \mathbb{N}^2$ en pixel dans l&apos;image. 

```python
def simple_detector(image):
    # go to HSV space
    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    # look into interval
    mask = cv2.inRange(image, np.array([155, 50, 50]) , np.array([170, 255, 255]))
    # sort by contour
    obj = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
    obj = sorted(obj, key=lambda x:cv2.contourArea(x), reverse=True)
    # keep center point
    if obj:
        (cX, cY), _ = cv2.minEnclosingCircle(obj[0])
        Z = [cX, cY]
    # return empty if no detection 
    else:
        Z = []
    return Z
```

À chaque instant $t$, les déplacements de l&apos;objet sont modélisés par l&apos;équation de mouvement suivante :

$$x_{t+1} = x_t + \Delta t \ \dot x_t$$

où $\Delta t \in \mathbb{R}$ est le pas de temps, $x_t$ la position et $\dot x_t$ la vitesse de l&apos;objet à $t$. L&apos;état du système est décrit par la position et la vitesse en 2 dimensions : $ X_t = \begin{bmatrix} x_t &amp; y_t &amp; \dot x_t &amp; \dot y_t \end{bmatrix}^T $. Pour obtenir la matrice de transition d&apos;état $A$, on écrit la dynamique du système sous forme matricielle  :

$$
\begin{array}{cc}
  &amp;\Rightarrow&amp; 
    \left\{
      \begin{array}{cc}
        x_{t+1}      &amp;=&amp;    x_t &amp;+&amp; 0 y_t  &amp;+&amp; \Delta t \ \dot x_t &amp;+&amp;    0       \dot y_t \\
        y_{t+1}      &amp;=&amp;  0 x_t &amp;+&amp;   y_t  &amp;+&amp;     0      \dot x_t &amp;+&amp; \Delta t \ \dot y_t \\
        \dot x_{t+1} &amp;=&amp;  0 x_t &amp;+&amp;  0 y_t &amp;+&amp;           \dot x_t  &amp;+&amp;    0       \dot y_t \\
        \dot y_{t+1} &amp;=&amp;  0 x_t &amp;+&amp;  0 y_t &amp;+&amp;    0      \dot x_t  &amp;+&amp;    1       \dot y_t
      \end{array}
    \right. 
  \\ \\ \\
  &amp;\Rightarrow&amp;
    \begin{array}{cc}
    X_{t+1} &amp;=&amp; 
    \underbrace{
      \begin{bmatrix} 1 &amp; 0 &amp; \Delta t  &amp;     0    \\
                      0 &amp; 1 &amp;     0     &amp; \Delta t \\
                      0 &amp; 0 &amp;     1     &amp;     0    \\
                      0 &amp; 0 &amp;     0     &amp;     1    \\
      \end{bmatrix}}_A
    X_t
    \end{array}
  \end{array}
$$

De plus, le détecteur permet d&apos;obtenir seulement la position $(x,y)$ mais pas la vitesse $(\dot x, \dot y)$, la matrice d&apos;observation est donc $H = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\ 0 &amp; 1 &amp; 0 &amp; 0 \end{bmatrix} $ pour relier seulement les 2 premières variables du système à notre observation. Les paramètres $Q$ et $R$ peuvent être affiner également en fonction du bruit de notre détecteur et du modèle. En python, on a : 

```python
# Initial state
X = np.array([500, 500, 0, 0]) # can be adjusted with 1st observation
P = np.eye(4)

# Kalman parameters
dt = 0.1
A = np.array([[1, 0, dt, 0], [0, 1, 0, dt], [0, 0, 1, 0], [0, 0, 0, 1]])
H = np.array([[1, 0, 0, 0],  [0, 1, 0, 0]])
Q = np.eye(4)
R = np.eye(2)
```

On ouvre la vidéo et pour chaque frame, on détecte la position de l&apos;objet puis on applique le filtre de Kalman. Il nous permet d&apos;avoir accès à la vitesse du système qui n&apos;est pas observé. La vitesse est représentée dans l&apos;exemple ci-dessous par la flèche. De plus, si la détection échoue et que la position de l&apos;objet n&apos;est pas disponible à l&apos;instant $t$, on exécute seulement la phase de prédiction du filtre de Kalman pour avoir malgré tout une estimation grâce au modèle dynamique.

```python
# open video
cap = cv2.VideoCapture(r&apos;path\to\my\video.mp4&apos;)
# loop on video frame
while True:
    # get frame
    ret, frame = cap.read()
    if not ret:
        break
    # simple object detection
    Z_obs = simple_detector(frame)
    # kalman filter
    X, P = kalman_estimation(X, P, Z_obs)
    # displaying
    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    plt.plot(Z_obs[0], Z_obs[1], &apos;r.&apos;, label=&quot;detected&quot;)
    plt.plot(X[0], X[1], &apos;g.&apos;, label=&quot;kalman&quot;)
    plt.arrow(X[0], X[1], X[2], X[3], color=&apos;g&apos;, head_length=1.5)
    plt.legend(loc=&quot;upper right&quot;)
    plt.axis(&quot;off&quot;)
# close video
cap.release()
```

&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;80%&quot; preload autoplay controls&gt;
    &lt;source src=&quot;/assets/images/kalman_object_tracking.mp4&quot; type=&quot;video/mp4&quot;&gt;
    Votre navigateur ne supporte pas la vidéo.
  &lt;/video&gt;
&lt;/p&gt;


## Central inertiel (IMU)

Les *Inertial Measurement Unit* sont des capteurs électroniques qu&apos;on trouve de nos jours un peu partout (téléphones, drones, avion, nintendo wii ...), ils permettent de mesurer l&apos;accélération et la vitesse angulaire d&apos;un objet pour ensuite estimer l&apos;orientation et la position par intégration. L&apos;accélération est mesurée par un accéléromètre et la vitesse angulaire par un gyroscope mais la difficulté vient du fait que le gyroscope possède un biais évoluant avec le temps. Si on ne corrige pas cette dérive, on aura l&apos;impression que l&apos;objet s&apos;incline lentement alors qu&apos;en réalité, celui-ci ne bouge pas !

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/kalman_imu_sensor.jpg&quot; width=&quot;20%&quot;/&gt;
&lt;/p&gt;

La dynamique du système se modélise comme dans l&apos;exemple précédent avec l&apos;équation de mouvement angulaire :

$$ \alpha_{t+1} = \alpha_t + \Delta t \ \dot \alpha_t $$

où $\Delta t \in \mathbb{R}$ est le pas de temps, $\alpha_t$ l&apos;angle et $\dot \alpha_t$ la vitesse angulaire. L&apos;état du système est, au final, décrit par l&apos;angle, la vitesse angulaire et le biais $b$ (qu&apos;on n&apos;observe pas) : $X_t = \begin{bmatrix} \alpha_t &amp; \dot \alpha_t &amp; b_t \end{bmatrix}^T$. Ne connaissant pas le modèle d&apos;évolution du biais $b$ du gyroscope, on le considére fixe ici. On a alors sous forme matricielle  :

$$ X_{t+1}
  = \underbrace{\begin{bmatrix}
    1 &amp; \Delta t &amp; 0 \\ 
    0 &amp;     1    &amp; 0 \\
    0 &amp;     0    &amp; 1
  \end{bmatrix}}_A
  X_t
$$

**Remarque:** On modélise ici un cas simpliste où l&apos;accéléromètre nous donne directement $\alpha$, l&apos;angle d&apos;inclinaison par rapport à la force de gravitation (qui n&apos;est rien d&apos;autre qu&apos;une accélération !), l&apos;objet tourne mais ne se déplace pas. En l&apos;orientant vers le sol (selon l&apos;axe Z), on obtient une accélération de 9.8 m/s², la constante $g$. Pour obtenir une mesure de l&apos;angle d&apos;orientation, il suffit donc de prendre $-\arcsin(a_{mesure}/g)$.
{: .notice--warning}

Comme énoncé précédemment, le biais $b$ n&apos;est pas une grandeur observée par notre IMU. La matrice d&apos;observation apparait clairement en écrivant la relation reliant l&apos;observation biaisé de la vitesse angulaire $\dot \alpha_{observé} = \dot \alpha_{vraie} + b$. C&apos;est-à-dire :

$$ \underbrace{\begin{bmatrix}
    \alpha \\
    \dot \alpha + b
  \end{bmatrix}}_{observation}
  = \underbrace{\begin{bmatrix}
    1 &amp; 0 &amp; 0 \\ 
    0 &amp; 1 &amp; 1 
  \end{bmatrix}}_H \quad
  \underbrace{\begin{bmatrix}
    \alpha \\
    \dot{\alpha} \\
    b 
  \end{bmatrix}}_X
$$

Enfin, il faut déterminer comment remplir $R$, le bruit de mesure et $Q$, le bruit du modèle. La matrice $R$ est simplement composé du bruit des capteurs sur la diagonale (pas de covariance car capteurs décorrélés entre eux) soit $R = \begin{bmatrix} \sigma_1^2 &amp; 0 \\\ 0 &amp; \sigma_2^2\end{bmatrix}$. La matrice $Q$ représente les erreurs de modélisation de $A$ : par exemple on a modélisé que le biais $b$ et que la vitesse angulaire $\dot \alpha$ étaient constants, ce qui est faux, on mettra des termes assez élevés à ces endroits et on les affinera empiriquement en fonction des données du problème. Par contre, l&apos;erreur du modèle sur l&apos;angle $\alpha$ peut être fixée à $0$ puisque l&apos;équation d&apos;état détermine parfaitement sa valeur en fonction de $\dot \alpha$. Ici, on a $Q = \begin{bmatrix} 0 &amp; 0 &amp; 0 \\\ 0 &amp; \epsilon_{\dot \alpha} &amp; 0 \\\ 0 &amp; 0 &amp; \epsilon_b\end{bmatrix} = \scriptsize \textcolor{blue}{\begin{bmatrix}0 &amp; 0 &amp; 0 \\\ 0 &amp; 3 &amp; 0 \\\ 0 &amp; 0 &amp; 5 \end{bmatrix} \leftarrow \textit{déterminé empiriquement}}$

En python, je commence par générer synthétiquement les données du problèmes. L&apos;angle est une fonction sinusoïdale avec un biais qui évolue au cours du temps et la vitesse angulaire est sa dérivée, on ajoute un bruit gaussien à ces données pour avoir notre vecteur d&apos;observation, la mesure qui sort des capteurs.

```python
dt = 0.05      # time step
T  = 10        # total time
N  = int(T/dt) # number of data
times = np.arange(0, T, dt) # time array
# Define state vector to be estimated (normally unknown)
X_true = np.zeros((N, 3))
X_true[:, 0] = -1 * np.pi * np.cos(np.pi * times) + np.pi
X_true[:, 1] = np.diff(X_true[:,0], prepend=-np.pi*dt)/dt # velocity as derivative of position
X_true[:, 2] = 10 * times + 20 * np.sin(2 * np.pi * 0.1 * times)
# Noise sensors
noise = np.zeros((N,2))
s = np.array([np.pi**2 * 0.06, np.pi * 0.2])
noise[:,0] = s[0] * np.random.randn(N)
noise[:,1] = s[1] * np.random.randn(N)
# Generate observation
X_obs = np.zeros((N, 2))
X_obs[:, 0] = X_true[:, 0] + noise[:,0]
X_obs[:, 1] = X_true[:, 1] + X_true[:, 2] + noise[:,1]
```

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/kalman_imu_data.png&quot; width=&quot;90%&quot;/&gt;
&lt;/p&gt;

On peut ensuite déclarer les paramètres de Kalman définis précédement et initialiser le variable d&apos;état et sa covariance estimés par le filtre (ici, ```X_est``` et ```P_est```). De plus, je sauvegarde l&apos;historique des valeurs dans ```X_kalman```.

```python
# Kalman filter parameter
H = np.array([[1,0,0],[0,1,1]])
R = np.array([[s[0]**2,0],[0,s[1]**2]])
A = np.array([[1,dt,0],[0,1,0],[0,0,1]])
Q = np.array([[0,0,0],[0,3,0],[0,0,5]])
# initial state
X_est = np.zeros(3)
P_est = np.zeros((3,3))
X_kalman = np.zeros((N, 3))
# loop over time
for t in range(2,N):
    X_est, P_est = kalman_estimation(X_est, P_est, X_obs[t, :].T)
    # save history
    X_kalman[t, :] = X_est
```

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/kalman_imu_filter.png&quot; width=&quot;90%&quot;/&gt;
&lt;/p&gt;

Ayant généré synthétiquement les données, l&apos;état réel du système est disponible (ce qui n&apos;est pas le cas en pratique). On peut donc comparer l&apos;erreur du filtre à l&apos;erreur si on prenait directement la mesure, on calcule les résidus $RSS = \sum_i^N (y_i - \hat y_i)^2$ et on a, pour le filtre de kalman, $RSS_{kalman} = [38; 2007]$ et, sans filtre, $RSS_{observation} = [80; 575873]$. La différence est flagrante pour la 2e variable $\dot \alpha$ où le biais est pris en compte par Kalman.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/kalman_imu_bias.png&quot; width=&quot;90%&quot;/&gt;
&lt;/p&gt;


# Pour aller plus loin

Une limite importante du filtre présenté ici est qu&apos;il modélise une dépendance linéaire avec le temps ce qui assez rare en pratique. On peut malgré tout l&apos;utiliser et obtenir de bon résultat comme montré dans les exemples ci-dessu mais il existe une version non linéaire appelé *[filtre de Kalman étendu](https://en.wikipedia.org/wiki/Extended_Kalman_filter)* où $A$ est remplacé par une fonction $f$ non linéaire et différentiable dans l&apos;équation de prédiction d&apos;état et sa jacobienne $F=\frac{\partial f}{\partial x}$ dans l&apos;équation de prédiction de la covariance. Cette solution est généralement utilisée dans les système de navigation et les GPS mais on note qu&apos;elle peut parfois être instable (divergence) selon l&apos;initialisation  de l&apos;état initial contrairement à la version linéaire.

Un autre avantage du filtre de Kalman est d&apos;être capable de faire de la [fusion de capteurs](https://en.wikipedia.org/wiki/Sensor_fusion). Un exemple très simple serait d&apos;avoir un système où l&apos;on a accès à 2 capteurs bruités mesurant la même grandeur, par exemple un radar et un GPS qui mesurent la position $x$ on aurait la matrice d&apos;observation $H$ via $\begin{bmatrix} x_{radar} \\\ x_{GPS} \end{bmatrix} = \underbrace{\begin{bmatrix} 1 &amp; 0 \\\ 1 &amp; 0 \end{bmatrix}}_H \begin{bmatrix} x \\\ \dot x \end{bmatrix}$, et on profiterait ainsi des 2 informations des 2 capteurs différents dans nos prédictions.


---

[![Generic badge](https://img.shields.io/badge/écrit_avec-Jupyter_notebook-orange.svg?style=plastic&amp;logo=Jupyter)](https://jupyter.org/try) [![Generic badge](https://img.shields.io/badge/License-MIT-blue.svg?style=plastic)](https://lbesson.mit-license.org/) [![Generic badge](https://img.shields.io/badge/acces_au_code-github-black.svg?style=plastic&amp;logo=github)](https://github.com/julienguegan/notebooks_blog/blob/main/filtre_kalman.ipynb) [![Generic badge](https://img.shields.io/badge/execute_le_code-binder-ff69b4.svg?style=plastic&amp;logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAMAAAC%2BRQ9vAAACOlBMVEX%2F%2F%2F9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olJXmsq%2FdJX1olLVa4pXmsrmZYH1olL1olJXmspXmsrmZYH1olJXmsr1olJXmspXmsr1olJXmsr1olJXmsrmZYH1olL1olL1olJXmspXmsrmZYH1olL1olL1olJXmsrmZYH1olL1olL1olJXmsrmZYHqdnT1olJXmsq6dZf1olJXmsrKk3rmZYH1olJXmsrCc5RXmsr0n1TtgWz1olJXmspXmsrmZYH1olJXmsqNhq%2Fzmlj1olJXmspZmshXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olL1olJXmsr1olJXmsrtgGz1olL1olJXmsr1olJXmsrmZYH1olJXmsrbaYf1olJXmsr1olJXmsr1olLIcJFXmsr1olJXmsr1olJXmsr1olJXmsr1olL1olJXmspZmshZmsldmsZemsVfl8Zgl8Zom71pk8Frm7tvm7dxkL1ykLx0m7R4m7F6jbh7jbh8nK6CnKmDirOEibOGnKaInKWNhq%2BNnKGSnZ2Vg6qegKaff6WfnZSnfKGnno6ofKGvnoeweZyxeZy3noG5dpjCcpPDcpPGn3bLb4%2FPoG%2FVa4rXoGnYoGjdaIbeaIXhoWHmZYHnaX7obXvpcHjqdHXreHLroVrtgGzuhGnuh2bxk17yl1vzm1j0nlX1olIgJPdZAAAAfnRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hYWFtgYGBkZnBwcHFxdHx8fn6AgICHiIuQkJCSnKCgoKavsLCwsLO4uMDAwMDBwcTFxsjO0NDQ09TW1tjY3Nzd4ODg4uLl5%2Bjo6uvr7O3v8PDw8%2FPz9vb39%2Fj5%2Bfv7%2FPz9%2Ff5K%2BfZ5AAAI4ElEQVR42uzWAWfDQBjG8Yc4qoihEApBIIoOOpaiFAUBBB3EjFDKRImZy0d7vtuYYWN36Zq4u5v7fYO%2FB%2B%2BLwENBEARBEAR32Zc0gpcWRXmS%2FO7SHPI5PDIvaip01TrypKGlXr2B6%2FKaV%2BirGA67v%2FBa9dKrCLWXGA5anvhXlYBjopI36DdwStrxNo2AO%2Fa8WZ%2FBEaLhGHs4YdFxnGME%2B5KeY7UCtq160v%2BOFUn%2FOxLyH3QkPafSwhrxzukcYcsrp7SFHSWnlcGGnEOaQ57i0ywrqo4DpIB5QlLruI7w07w4U%2BsZ5j1R420n8Ju46qmxhmkZ1WQBJVHq6gUM66hUCujEJ3e%2B3YIqMsWQLZVmMCmSVDgLDEskFR5h0m7kLRatC3NEckSFosPCHA%2FqitEdMxjzwbxZN7eRNGG8tcpr%2BS2vA3KFmZODoFLlDaOS4%2FXxleVj9OqYacLMzMzYR%2BHsZwtz5hnvSNOSf%2F97Vc%2F0NI%2B%2FBwM0q%2FQJMsjoynXfYFr%2BPxe9SgtVijdiLT3Jjrmxlu5UIf5wlLq%2BraqTD9dfqbSjFrhY1T5jLNkzMdbRUMVy6nsqgdpYx4TKbMViHXA2bm%2BOJqoEY7QlNpVEfayDKoD3eqzhBSqNpqo4R7dcyJdjDX%2BHuW7Ouq%2BhshqCiG9yTfPDV%2FgmUWCvpLbCmSMzqsC3%2BSvWcInvEOUyZEeL5mtzxUQEfI9%2FYw3%2F8X2mZsuOVUVxEUDGP%2FwQeZ%2BSM7pSocrL8cNciDXwowQeJaWhQjK6RfwIFzU%2Fe5UfIxpiI0M%2B4npTmduWcZmfIJ%2FU1yshIxtxiTI46tZuZAxhTipDQ659yPACLksG5712IMMLuUwZHHriMuxVYBlXGBD50pHKXgWWEbNJh72MtKgKnMX%2Fxjq8KmZxrALXVNb%2BIV9TBQyAFS4mrFqFO4oNxMDHIUGV%2Bo0sGwDdHxvoT5ChcmNcL2ITl2INF9hAlKlGLz6VjXwSgxoXE%2BI7JRZvu7GJwO8Y63jRaMJRpGcCnlNJXqkgg6aGX3ij7K9Vuig2NQwYkvcNe4GhlMkzZCrOfSKbgQxDhpjGhvH7RNQfWzKLPUMi%2BeUTVEd%2Fwgc4fggtifc0Alkjm6SmeEd%2FivWgikHmGCC3bQoSqKCBsZamtKbXwuaoL4rdqQxUATYcmusQJjNHuikW227kWEvBS7YXH22qjgOQvwX24iDS%2BI%2FHe%2FQqasBtk4KveNoCXcDB%2B6NIC2IMsEc3%2FBl4o%2B7RIFZN5eETAw0T0%2FA74YOEAVW4aDU81pKx%2Bo%2BNpvp7BQ38UPdijKgXKQpxWfdZjCiOJhpluFXp6TFkolg5FXlgooFpafAiWFiNLsaQopMSvWAzwpweG5g7je9y5sgtztw5EUoPbRF%2FUOyhCw2LbMw1PrJnx9qV6gEr1%2B48MAf%2FDfZvJ66RJ0T3GHJi21KlZ%2Fn2U%2FhK1crNQ%2FoTZEKs5dia%2BcrEos2n5GpCFO0zdrv589sWqrZZtPu83FOREKaspO5xeo1KyPz156S2yDZxSldrn16tbHhUSFNaQAZ0Dezm5zcoS%2BZvPw8zRulkEzQJuIPbP1%2FZs%2BjYg85RVIZHiXScX6FKY%2FN5tyqADDJyr847tECVysITcdxUS5WTgf18iyqHvRbeLSgj9ZYqj%2BepHcjo8Lkql5dTVZfR4RtVPp%2Bn5GXIq8A6xPMGUFF9HR5r6Gb27i%2BVK94mV6BGHPOuskY%2BXhVA1wSZp1wyjtyQt%2FTxkcotncgJOTvnSP2o2mDxxp2Hjxxn5uNHDu%2FcuFi1wXdu3Ly%2F3W5%2BijKycs9xfpTjO5YoI6%2BSC3y2qXH7mQPoD6yhd6M5tA0iF0Ro1Kch1aowH%2Fbqz8DRRpiE%2FJwSmykUSEuj4Y4PIwrxsKjxVwWZIeUcwBx1CjIv1cY0uKZZIT4mB2SSP%2ByarQC%2FD4NjVPbbNuWzAiMePB3pogA%2FdnpkcIeu59MK0JoSeXcL6kNkjG866EKe5jg6%2FSpoDi%2Fhe8E6qMK0w8xQAh3Ngg9G8snC1O%2F%2Ft%2FjICKWnn0DPoc%2FlKaWnh0kF9092FrMln4wECRL4OBC1Uf55U2mpEUgdWh2vGI4xSP7gMKV3j%2FESTYfm3XwNPkUv4MTGQGG3WfbVZ%2BFe9hoMI6UfWr3%2BBHG7RsA7NMXEFJS3Rtk8msRZdLCbigRTuH2mrXpjZMF9BBkUm2OKuxUgFgKOsG%2BeDQQ2TUurw%2BUZFvLcKvU4y3Z9xRj4RABZtk6gC9Rw8uDWdeoeq7buO8lmDA39eIFEDipEwNFbnOUE5AjSBQU9qTawdEIy0CpVj%2BAa1R6zY6BY9Qo5IhO5U%2BGTiWeVBnKF70yHT0a6CsgQ0NGfMNDH6yR1CKgAvUsXalc6oiy1ibQM8kMx7xaQgfHyXA6hRy5lCJSJVrm7%2BjJw9Y2x%2B6%2F3morIIC%2FHpTDVo2R0Een%2FNGTtPb2gi1AWHQeJ0N%2FuZkVDKDnjgYxqC4lGeWTBbJEKFwvJcxLC%2FmRFCjTjcmRyBTYT5XyypCtom0TxR4XYDrksWYEHuV1JHC878%2BjJx3vzo7te86gUUq2Vibdg7bdq3aZdd9i0blUZP90PTj%2Fl0Z5gI5VCM%2FyUPI3OJq%2F9xBY1Jf94oytjCLkGiPUO6rlnlY5XSBjzo5fmlH2ssB%2Boi98q22uVekVpSVGlaLVfouJIIV%2BJWJWlloOZwcrCxWSoUXputGuHuLKEQBSGDwaDQmAxrVFtyuDaswB2UIs4a395ueKKCcyd7g4wSX%2B%2BxJ8cWequDpMVA8nVjsiGiIEsGzReWiUrhrr0SmQOtkQMZZUtxaIvdG4xWGJbMmizmW0eo1W2aTPECjsEw3n2qDi8Cpk9ajDezr66B4NfNoqyL2CGwrf0kPRfPpRv7ZjCKe9UMEngjdRilo23UYd5hHeJmEkGVIwgwyrW74iYL%2FEi9VhBVF5RHdbgKs%2FLBqswmWdtWElQnlEc1mKEH9MN63EHPyMGS%2FKfhIjFsnzmn6hYLM2myndKNFif2yvbymbxLWyUwlfHHgy%2BjfMp5eOHpOQtHo%2FH4%2FEY7x8MZ7AAyatDDgAAAABJRU5ErkJggg%3D%3D)](https://hub.gke2.mybinder.org/user/julienguegan-notebooks_blog-z8qd9bd5/notebooks/filtre_kalman.ipynb)</content><author><name>Julien Guégan</name></author><category term="blog" /><category term="estimateur" /><category term="navigation" /><category term="dynamique" /><category term="gaussienne" /><summary type="html">Le filtre de Kalman est une méthode très répandue dans le milieu de l’ingénieurie puisqu’elle posséde de nombreuses applications en localisation, navigation, pilotage automatique, suivi d’objet, fusion de données … Il fut introduit en 1960 par l’ingénieur Rudolf E. Kálmán et fut notamment utilisé pour l’estimation de trajectoire pour le programme Apollo. En effet, à partir d’une série de mesures observées au cours du temps (bruitée et biaisée), il permet de calculer une estimation de ces variables inconnues souvent plus précise que les mesures en se basant sur les théories du contrôle et des statistiques. L’une des forces de ce filtre est sa capacité à s’améliorer au cours du temps en intégrant un terme d’erreur du modèle lui-même. Il a de nombreux autres avantages : fusionner les mesures de capteurs différents, fonctionner en ligne, facile à implémenter … Principe La construction du filtre de Kalman part de 3 hypothèses importantes : le système modélisé est linéaire : il peut être modélisé comme une multiplication entre l’état $t$ et $t-1$. le bruit des mesures est blanc : il est non corrélé avec le temps. le bruit est gaussien : il est décrit par une moyenne et une covariance L’idée consiste à construire un modèle pour l’état du système qui maximise la probabilité a posteriori de ces mesures précédentes. Cela signifie que le nouveau modèle que nous construisons après avoir effectué une mesure (en tenant compte à la fois de notre modèle précédent avec son incertitude et de la nouvelle mesure avec son incertitude) est le modèle qui a la plus forte probabilité d’être correct. De plus, on peut maximiser la probabilité a posteriori sans conserver un long historique des mesures précédentes elles-mêmes. Au lieu de cela, on met à jour de manière itérative le modèle de l’état du système et on ne garde que ce modèle pour la prochaine itération. Cela simplifie grandement l’implication de calcul de cette méthode. Cas unidimensionnel statique Supposons qu’on veuille savoir où est positionner un point fixe sur une ligne et qu’on ait 2 mesures bruitées $x_1$ et $x_2$. Chacune de ces mesures suit une distribution gaussienne : \[p_i(x) = \frac{1}{\sigma_i\sqrt{2\pi}} e^{-\frac{(x-\bar{x}_i)^2}{2\sigma_i^2} } \quad (i=1,2)\] On peut alors montrer que la combinaison de ces 2 mesures gaussiennes est équivalente à une seule mesure gaussienne caractérisée par la moyenne $\bar{x}_{12}$ et la variance $\sigma_{12}^2$ : \[\begin{aligned} \bar{x}_{12} &amp;amp;= \left( \frac{\sigma_2^2}{\sigma_1^2+\sigma_2^2} \right)x_1 + \left( \frac{ \sigma_1^2}{\sigma_1^2+\sigma_2^2} \right)x_2 \\ \\ \sigma_{12}^2 &amp;amp;= \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2} \end{aligned}\] Ainsi, la nouvelle valeur moyenne $\bar{x}_{12}$ n’est qu’une combinaison pondérée des deux mesures par les incertitudes relatives. Par exemple, si l’incertitude $\sigma_2$ est particulièrement plus grande que $\sigma_1$, alors la nouvelle moyenne sera très proche de $x_1$ qui est plus certaine. Maintenant, si on part du principe que l’on effectue nos 2 mesures l’une après l’autre et qu’on cherche donc à estimer l’état courant de notre système $(\hat{x}_t,\hat{\sigma}_t)$. Au temps $t=1$, on a notre première mesure $\hat{x}_1=x_1$ et son incertitude $\hat{\sigma}_1^2=\sigma_1^2$. En substituant ceci dans nos équations d’estimation optimales et en les réarrangeant pour séparer l’ancienne informations de la nouvelle, on obtient : \[\begin{aligned} \hat{x}_2 &amp;amp;= \hat{x}_1 + \frac{\hat\sigma_1^2}{\hat{\sigma_1^2}+\sigma_2^2}(x_2 - \hat x_1 ) \\ \\ \hat \sigma_2^2 &amp;amp;= \left( 1 - \frac{\hat \sigma_1^2}{\hat\sigma_1^2+\sigma_2^2} \right) \hat \sigma_1^2 \end{aligned}\] En pratique, on appelle couramment le terme $x_2 - \hat x_1$ l’innovation et on note le facteur $K = \frac{\hat \sigma_1^2}{\hat \sigma_1^2+\sigma_2^2}$ le gain de mise à jour. Au final, on peut écrire la relation de récurrence au temps $t$ : \[\begin{aligned} \hat x_t &amp;amp;= \hat x_{t-1} + K (x_t - \hat x_{t-1}) \\ \\ \hat \sigma_t^2 &amp;amp;= (1 - K) \hat \sigma_{t-1}^2 \end{aligned}\] Attention : Dans la littérature, on voit plus souvent l’indice $k$ pour décrire le pas de temps (ici noté $t$). Si on regarde la formule et à la valeur du gain de Kalman $K$, on comprend que si le bruit de mesure est élevé ($\sigma^2$ élevé) alors $K$ sera proche de $0$ et l’influence de la nouvelle mesure $x_t$ sera faible. Au contraire, si $\sigma^2$ est petit, l’état du système $\hat x_t$ sera ajusté fortement vers l’état de la nouvelle mesure. Cas unidimensionnel dynamique On a considéré précemment le cas d’un système statique dans un état $x$ ainsi qu’une série de mesures de ce système. Dans un cas dynamique où l’état du système varie au cours du temps, on divise l’estimation du filtre de Kalman en 2 étapes : la phase de prédiction : on utilise les informations passées et le modèle dynamique pour prédire l’état prochain du système. On prédit également la covariance de l’erreur. Elle modélise l’état du système. la phase de correction ou mise à jour : on combine la prédiction faite avec une nouvelle mesure pour affiner le modèle et l’estimation de l’état du système ainsi que la covariance de l’erreur. Elle modélise la mesure du système. Par exemple, si on mesure la position d’une voiture au temps $t-1$ puis au temps $t$. Si la voiture a une vitesse $v$ alors on n’intègre pas directement la nouvelle mesure directement. D’abord, on fast-forward notre modèle basé sur ce qu’on savait au temps $t-1$ pour avoir une prédiction de l’état au temps $t$. De cette manière, la nouvelle mesure acquise au temps $t$ est fusionnée non pas avec l’ancien modèle du système mais avec l’ancien modèle du système projeté vers l’avant au temps $t$ En partant de l’hypothèse que la dynamique du système modélisé est linéaire, la phase de prédiction s’écrit alors : \[\left. \begin{aligned} \hat x_t &amp;amp;= a \ \hat x_{t-1} \\ \hat \sigma_t^2 &amp;amp;= a^2 \ \hat \sigma_{t-1}^2 \\ &amp;amp;\scriptsize \color{blue} \text{car $var(ax+b) = a^2 var(x)$} \end{aligned} \right.\] Et la phase de correction calculée dans la section précédente : \[\left. \begin{aligned} \hat x_t &amp;amp;= \hat x_{t-1} + K (z_t - \hat x_{t-1}) \\ \hat \sigma_t^2 &amp;amp;= (1 - K) \hat \sigma_{t-1}^2 \\ \end{aligned} \right.\] Généralisation On peut étendre les équations précédentes au cas multidimensionnel où l’état de notre système est défini par plusieurs grandeurs. On cherche alors à estimer l’état du système $\hat X \in \mathbb{R}^d$ à l’instant $t$ ainsi que sa matrice de covariance associée $\hat P \in \mathbb{R}^{d \times d}$ (avec $d$ la dimension du système). Les équations deviennent : phase de prédiction \[\left. \begin{aligned} \hat X_t &amp;amp;= A \hat X_{t-1} \\ \hat P_t &amp;amp;= A P_{t-1} A^T + Q \end{aligned} \right.\] où $A \in \mathbb{R}^{d \times d}$ est la matrice de transition d’état modélisant la dynamique du système et $Q \in \mathbb{R}^{d \times d}$ la matrice de covariance du bruit de processus capturant les erreurs non modélisées par $A$ (plus elle est grande, plus on fait confiance aux mesures plutôt qu’aux prédictions du modèle dynamique). phase de correction \[\left. \begin{aligned} \hat X_t &amp;amp;= \hat X_t + K(Z_t - \hat X_t) \\ \hat P_t &amp;amp;= \hat P_t - K \hat P_t \end{aligned} \right.\] avec le gain de Kalman $ K = \hat P_t (\hat P_t + R)^{-1} $ où $Z \in \mathbb{R}^d$ est la mesure du système (ou observation) et $R \in \mathbb{R}^{d \times d}$ la matrice de covariance de la mesure qui modélise l’erreur des mesures. Il existe des versions plus élaborées du filtre de Kalman qui peuvent prendre en entrée une commande $U$ envoyée au système. On trouve également fréquemment la matrice d’observation $H \in \mathbb{R}^{d \times m}$ reliant l’état réel du système au variables observées, en effet on peut modéliser un système à $d$ dimensions mais seulement observer $m$ de ses variables ($m&amp;lt;d$). La phase de prédiction reste la même mais la phase de correction est alors : \[\begin{aligned} \hat X_t &amp;amp;= \hat X_t + K(\textcolor{blue}{H} Z_t - \hat X_t) \\ \hat P_t &amp;amp;= \hat P_t - K \textcolor{blue}{H} \hat P_t \end{aligned}\] avec le gain de Kalman $ K = \hat P_t \textcolor{blue}{H^t} (\textcolor{blue}{H} \hat P_t \textcolor{blue}{H^T} + R)^{-1} $ Exemples Le filtre de Kalman est un outil générique et ces équations peuvent facilement s’implémenter en quelques lignes : def kalman_estimation(X_est, P_est, Z_obs): # state prediction X_est = A @ X_est P_est = A @ P_est @ A.T + Q # observation Z_pred = H @ X_est # kalman gain K = P_est @ H.T @ np.linalg.inv(H @ P_est @ H.T + R) # correction phase if Z_obs: X_est = X_est + K @ (Z_obs - Z_pred) P_est = P_est - K @ H @ P_est # return final estimation return X_est, P_est Note: Si l’observation n’est pas disponible à l’instant $t$, on execute seulement la phase de prédiction du filtre de Kalman pour avoir une estimation grâce au modèle dynamique. Cependant, la partie cruciale du problème réside la plupart du temps dans la définition des paramètres du système $A$, $Q$, $H$ et $R$ afin que le filtre fonctionne correctement. De plus, il peut devenir puissant lorsqu’il permet d’estimer une grandeur qui n’est pas mesurée comme dans les exemples ci-dessous avec la vitesse d’un objet et le biais d’un gyroscope. Suivi d’objet (Tracking) On veut implémenter un filtre de Kalman appliqué à un problème de suivi d’objet sur une image. L’objet est repéré par un détecteur d’objet basique en regardant un intervalle de couleur dans l’espace HSV qui retourne la position $(x,y) \in \mathbb{N}^2$ en pixel dans l’image. def simple_detector(image): # go to HSV space image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) # look into interval mask = cv2.inRange(image, np.array([155, 50, 50]) , np.array([170, 255, 255])) # sort by contour obj = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2] obj = sorted(obj, key=lambda x:cv2.contourArea(x), reverse=True) # keep center point if obj: (cX, cY), _ = cv2.minEnclosingCircle(obj[0]) Z = [cX, cY] # return empty if no detection else: Z = [] return Z À chaque instant $t$, les déplacements de l’objet sont modélisés par l’équation de mouvement suivante : \[x_{t+1} = x_t + \Delta t \ \dot x_t\] où $\Delta t \in \mathbb{R}$ est le pas de temps, $x_t$ la position et $\dot x_t$ la vitesse de l’objet à $t$. L’état du système est décrit par la position et la vitesse en 2 dimensions : $ X_t = \begin{bmatrix} x_t &amp;amp; y_t &amp;amp; \dot x_t &amp;amp; \dot y_t \end{bmatrix}^T $. Pour obtenir la matrice de transition d’état $A$, on écrit la dynamique du système sous forme matricielle : \[\begin{array}{cc} &amp;amp;\Rightarrow&amp;amp; \left\{ \begin{array}{cc} x_{t+1} &amp;amp;=&amp;amp; x_t &amp;amp;+&amp;amp; 0 y_t &amp;amp;+&amp;amp; \Delta t \ \dot x_t &amp;amp;+&amp;amp; 0 \dot y_t \\ y_{t+1} &amp;amp;=&amp;amp; 0 x_t &amp;amp;+&amp;amp; y_t &amp;amp;+&amp;amp; 0 \dot x_t &amp;amp;+&amp;amp; \Delta t \ \dot y_t \\ \dot x_{t+1} &amp;amp;=&amp;amp; 0 x_t &amp;amp;+&amp;amp; 0 y_t &amp;amp;+&amp;amp; \dot x_t &amp;amp;+&amp;amp; 0 \dot y_t \\ \dot y_{t+1} &amp;amp;=&amp;amp; 0 x_t &amp;amp;+&amp;amp; 0 y_t &amp;amp;+&amp;amp; 0 \dot x_t &amp;amp;+&amp;amp; 1 \dot y_t \end{array} \right. \\ \\ \\ &amp;amp;\Rightarrow&amp;amp; \begin{array}{cc} X_{t+1} &amp;amp;=&amp;amp; \underbrace{ \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \Delta t &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; \Delta t \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\ \end{bmatrix}}_A X_t \end{array} \end{array}\] De plus, le détecteur permet d’obtenir seulement la position $(x,y)$ mais pas la vitesse $(\dot x, \dot y)$, la matrice d’observation est donc $H = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} $ pour relier seulement les 2 premières variables du système à notre observation. Les paramètres $Q$ et $R$ peuvent être affiner également en fonction du bruit de notre détecteur et du modèle. En python, on a : # Initial state X = np.array([500, 500, 0, 0]) # can be adjusted with 1st observation P = np.eye(4) # Kalman parameters dt = 0.1 A = np.array([[1, 0, dt, 0], [0, 1, 0, dt], [0, 0, 1, 0], [0, 0, 0, 1]]) H = np.array([[1, 0, 0, 0], [0, 1, 0, 0]]) Q = np.eye(4) R = np.eye(2) On ouvre la vidéo et pour chaque frame, on détecte la position de l’objet puis on applique le filtre de Kalman. Il nous permet d’avoir accès à la vitesse du système qui n’est pas observé. La vitesse est représentée dans l’exemple ci-dessous par la flèche. De plus, si la détection échoue et que la position de l’objet n’est pas disponible à l’instant $t$, on exécute seulement la phase de prédiction du filtre de Kalman pour avoir malgré tout une estimation grâce au modèle dynamique. # open video cap = cv2.VideoCapture(r&apos;path\to\my\video.mp4&apos;) # loop on video frame while True: # get frame ret, frame = cap.read() if not ret: break # simple object detection Z_obs = simple_detector(frame) # kalman filter X, P = kalman_estimation(X, P, Z_obs) # displaying plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) plt.plot(Z_obs[0], Z_obs[1], &apos;r.&apos;, label=&quot;detected&quot;) plt.plot(X[0], X[1], &apos;g.&apos;, label=&quot;kalman&quot;) plt.arrow(X[0], X[1], X[2], X[3], color=&apos;g&apos;, head_length=1.5) plt.legend(loc=&quot;upper right&quot;) plt.axis(&quot;off&quot;) # close video cap.release() Votre navigateur ne supporte pas la vidéo. Central inertiel (IMU) Les Inertial Measurement Unit sont des capteurs électroniques qu’on trouve de nos jours un peu partout (téléphones, drones, avion, nintendo wii …), ils permettent de mesurer l’accélération et la vitesse angulaire d’un objet pour ensuite estimer l’orientation et la position par intégration. L’accélération est mesurée par un accéléromètre et la vitesse angulaire par un gyroscope mais la difficulté vient du fait que le gyroscope possède un biais évoluant avec le temps. Si on ne corrige pas cette dérive, on aura l’impression que l’objet s’incline lentement alors qu’en réalité, celui-ci ne bouge pas ! La dynamique du système se modélise comme dans l’exemple précédent avec l’équation de mouvement angulaire : \[\alpha_{t+1} = \alpha_t + \Delta t \ \dot \alpha_t\] où $\Delta t \in \mathbb{R}$ est le pas de temps, $\alpha_t$ l’angle et $\dot \alpha_t$ la vitesse angulaire. L’état du système est, au final, décrit par l’angle, la vitesse angulaire et le biais $b$ (qu’on n’observe pas) : $X_t = \begin{bmatrix} \alpha_t &amp;amp; \dot \alpha_t &amp;amp; b_t \end{bmatrix}^T$. Ne connaissant pas le modèle d’évolution du biais $b$ du gyroscope, on le considére fixe ici. On a alors sous forme matricielle : \[X_{t+1} = \underbrace{\begin{bmatrix} 1 &amp;amp; \Delta t &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}}_A X_t\] Remarque: On modélise ici un cas simpliste où l’accéléromètre nous donne directement $\alpha$, l’angle d’inclinaison par rapport à la force de gravitation (qui n’est rien d’autre qu’une accélération !), l’objet tourne mais ne se déplace pas. En l’orientant vers le sol (selon l’axe Z), on obtient une accélération de 9.8 m/s², la constante $g$. Pour obtenir une mesure de l’angle d’orientation, il suffit donc de prendre $-\arcsin(a_{mesure}/g)$. Comme énoncé précédemment, le biais $b$ n’est pas une grandeur observée par notre IMU. La matrice d’observation apparait clairement en écrivant la relation reliant l’observation biaisé de la vitesse angulaire $\dot \alpha_{observé} = \dot \alpha_{vraie} + b$. C’est-à-dire : \[\underbrace{\begin{bmatrix} \alpha \\ \dot \alpha + b \end{bmatrix}}_{observation} = \underbrace{\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 1 &amp;amp; 1 \end{bmatrix}}_H \quad \underbrace{\begin{bmatrix} \alpha \\ \dot{\alpha} \\ b \end{bmatrix}}_X\] Enfin, il faut déterminer comment remplir $R$, le bruit de mesure et $Q$, le bruit du modèle. La matrice $R$ est simplement composé du bruit des capteurs sur la diagonale (pas de covariance car capteurs décorrélés entre eux) soit $R = \begin{bmatrix} \sigma_1^2 &amp;amp; 0 \\ 0 &amp;amp; \sigma_2^2\end{bmatrix}$. La matrice $Q$ représente les erreurs de modélisation de $A$ : par exemple on a modélisé que le biais $b$ et que la vitesse angulaire $\dot \alpha$ étaient constants, ce qui est faux, on mettra des termes assez élevés à ces endroits et on les affinera empiriquement en fonction des données du problème. Par contre, l’erreur du modèle sur l’angle $\alpha$ peut être fixée à $0$ puisque l’équation d’état détermine parfaitement sa valeur en fonction de $\dot \alpha$. Ici, on a $Q = \begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; \epsilon_{\dot \alpha} &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; \epsilon_b\end{bmatrix} = \scriptsize \textcolor{blue}{\begin{bmatrix}0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 3 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 5 \end{bmatrix} \leftarrow \textit{déterminé empiriquement}}$ En python, je commence par générer synthétiquement les données du problèmes. L’angle est une fonction sinusoïdale avec un biais qui évolue au cours du temps et la vitesse angulaire est sa dérivée, on ajoute un bruit gaussien à ces données pour avoir notre vecteur d’observation, la mesure qui sort des capteurs. dt = 0.05 # time step T = 10 # total time N = int(T/dt) # number of data times = np.arange(0, T, dt) # time array # Define state vector to be estimated (normally unknown) X_true = np.zeros((N, 3)) X_true[:, 0] = -1 * np.pi * np.cos(np.pi * times) + np.pi X_true[:, 1] = np.diff(X_true[:,0], prepend=-np.pi*dt)/dt # velocity as derivative of position X_true[:, 2] = 10 * times + 20 * np.sin(2 * np.pi * 0.1 * times) # Noise sensors noise = np.zeros((N,2)) s = np.array([np.pi**2 * 0.06, np.pi * 0.2]) noise[:,0] = s[0] * np.random.randn(N) noise[:,1] = s[1] * np.random.randn(N) # Generate observation X_obs = np.zeros((N, 2)) X_obs[:, 0] = X_true[:, 0] + noise[:,0] X_obs[:, 1] = X_true[:, 1] + X_true[:, 2] + noise[:,1] On peut ensuite déclarer les paramètres de Kalman définis précédement et initialiser le variable d’état et sa covariance estimés par le filtre (ici, X_est et P_est). De plus, je sauvegarde l’historique des valeurs dans X_kalman. # Kalman filter parameter H = np.array([[1,0,0],[0,1,1]]) R = np.array([[s[0]**2,0],[0,s[1]**2]]) A = np.array([[1,dt,0],[0,1,0],[0,0,1]]) Q = np.array([[0,0,0],[0,3,0],[0,0,5]]) # initial state X_est = np.zeros(3) P_est = np.zeros((3,3)) X_kalman = np.zeros((N, 3)) # loop over time for t in range(2,N): X_est, P_est = kalman_estimation(X_est, P_est, X_obs[t, :].T) # save history X_kalman[t, :] = X_est Ayant généré synthétiquement les données, l’état réel du système est disponible (ce qui n’est pas le cas en pratique). On peut donc comparer l’erreur du filtre à l’erreur si on prenait directement la mesure, on calcule les résidus $RSS = \sum_i^N (y_i - \hat y_i)^2$ et on a, pour le filtre de kalman, $RSS_{kalman} = [38; 2007]$ et, sans filtre, $RSS_{observation} = [80; 575873]$. La différence est flagrante pour la 2e variable $\dot \alpha$ où le biais est pris en compte par Kalman. Pour aller plus loin Une limite importante du filtre présenté ici est qu’il modélise une dépendance linéaire avec le temps ce qui assez rare en pratique. On peut malgré tout l’utiliser et obtenir de bon résultat comme montré dans les exemples ci-dessu mais il existe une version non linéaire appelé filtre de Kalman étendu où $A$ est remplacé par une fonction $f$ non linéaire et différentiable dans l’équation de prédiction d’état et sa jacobienne $F=\frac{\partial f}{\partial x}$ dans l’équation de prédiction de la covariance. Cette solution est généralement utilisée dans les système de navigation et les GPS mais on note qu’elle peut parfois être instable (divergence) selon l’initialisation de l’état initial contrairement à la version linéaire. Un autre avantage du filtre de Kalman est d’être capable de faire de la fusion de capteurs. Un exemple très simple serait d’avoir un système où l’on a accès à 2 capteurs bruités mesurant la même grandeur, par exemple un radar et un GPS qui mesurent la position $x$ on aurait la matrice d’observation $H$ via $\begin{bmatrix} x_{radar} \\ x_{GPS} \end{bmatrix} = \underbrace{\begin{bmatrix} 1 &amp;amp; 0 \\ 1 &amp;amp; 0 \end{bmatrix}}_H \begin{bmatrix} x \\ \dot x \end{bmatrix}$, et on profiterait ainsi des 2 informations des 2 capteurs différents dans nos prédictions.</summary></entry><entry xml:lang="fr"><title type="html">CNN : convolution, Pytorch, Deep Dream</title><link href="http://localhost:4000/posts/fr/2022-01-01-convolution_deepdream/" rel="alternate" type="text/html" title="CNN : convolution, Pytorch, Deep Dream" /><published>2022-01-01T20:10:10+01:00</published><updated>2022-01-01T20:10:10+01:00</updated><id>http://localhost:4000/posts/fr/convolution_deepdream</id><content type="html" xml:base="http://localhost:4000/posts/fr/2022-01-01-convolution_deepdream/">Les réseaux de neurones convolutionnels (CNN) sont les modèles qui ont permis de faire un bon en avant dans les problèmes de reconnaissance d&apos;image. Ils sont au coeur de nombreuses applications allant des systèmes de sécurité par identification faciale à la classification de vos photos de vacances en passant par la génération synthétique de visage et les filtres snapchat. L&apos;un des fondateurs de ce modèle est Yann Le Cun (un français !) qui, en 1989, applique la backpropagation du gradient pour apprendre des filtres de convolution et permet à un réseau de neurone à reconnaître des chiffres manuscrits. Cependant, c&apos;est seulement en 2012 que les CNN se répandent largement dans la communauté scientifique de la vision par ordinateur avec Alex Krizhevsky qui conçoit l&apos;architecture *AlexNet* et remporte la compétition *ImageNet Large Scale Visual Recognition Challenge* (1 million d&apos;images de 1000 classes différentes) en implémentant son algorithme sur des GPUs ce qui permet au modèle d&apos;apprendre rapidement d&apos;une grande quantité d&apos;image. Ce modèle atteint des performances 10% plus élevées que tous les autres à cette époque et il est désormais l&apos;un des papiers publiés les plus influents en Computer Vision (en 2021, plus de 80 000 citations selon Google Scholar).

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/cnn_header.png&quot; width=&quot;80%&quot;/&gt;
&lt;/p&gt;

## Convolutions et Réseaux de neurones

Les modèles de réseaux de neurones complètements connectés (cf [post précédent](https://julienguegan.github.io/posts/2021-09-10-reseau_de_neurone/)) ne sont pas adaptés pour résoudre des problèmes de traitement d&apos;image. En effet, les MLP ont chaque neurone d&apos;une couche connecté à chaque unité d&apos;entrée : le nombre de paramètre à apprendre devient vite élevé et une forte redondance dans les poids du réseau peut exister. De plus, pour utiliser une image dans un tel réseau, tous les pixels devrait être transformée en vecteur et aucune information sur la structure locale des pixels serait alors prise en compte. 

Le produit de convolution, noté $\ast$, est un opérateur qui généralise l&apos;idée de moyenne glissante. Il s&apos;applique aussi bien à des données temporelles (en traitement du signal par exemple) qu&apos;à des données spatiales (en traitement d&apos;image). Pour le cas des images, c&apos;est-à-dire discret et en 2 dimensions, la convolution entre une image $I$ et un noyau  $w$ (ou kernel) peut se calculer comme suit :

$$I(i,j) * \omega =\sum_{x=-a}^a{\sum_{y=-b}^b{ I(i+x,j+y)} \ \omega(x,y)}$$

L&apos;idée est de faire glisser le noyau spatialement sur toute l&apos;image et à chaque fois de faire une moyenne pondérée des pixels de l&apos;image se retrouvant dans la fenêtre concernée par les éléments du noyau. Selon la valeur des éléments du noyau de convolution $w$, l&apos;opération peut mettre en avant des caractéristiques particulières se trouvant dans l&apos;image comme des contours, des textures, des formes.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/image_convolution.gif&quot; width=&quot;40%&quot;/&gt;
&lt;/p&gt;

**Remarque:** Il existe plusieurs paramètres associés à l&apos;opération de convolution comme la taille du noyau utilisé, la taille du pas lorsqu&apos;on fait glisser la fenêtre sur l&apos;image, la façon dont on gère les bords de l&apos;image, le taux de dilatation du noyau ... [plus d&apos;infos ici](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)
{: .notice--warning}

On peut par exemple mettre en avant les pixels d&apos;une image correspondants aux contours horizontaux en appliquant une convolution avec un noyau de taille $3 \times 3$ avec des $-1$ dans la 1ère ligne, des $0$ dans la 2ème ligne et des $+1$ dans la 3ème ligne de la matrice.

```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.signal import convolve2d
# read image
image = np.array(Image.open(&quot;path/to/file.jpg&quot;).convert(&apos;L&apos;))
# apply convolution
kernel = np.array([[-1, -1, -1],
                   [ 0,  0,  0],
                   [+1, +1, +1]])
conv_output = convolve2d(image, kernel, mode=&apos;same&apos;)
# display
plt.figure(figsize=(15,5))
plt.subplot(121), plt.imshow(image, cmap=&apos;gray&apos;), plt.axis(&apos;off&apos;)
plt.subplot(122), plt.imshow(np.abs(conv_output), cmap=&apos;gray&apos;), plt.axis(&apos;off&apos;)
plt.tight_layout()
plt.show()
```

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/convolution_exemple.png&quot; width=&quot;80%&quot;/&gt;
&lt;/p&gt;

L&apos;idée de l&apos;architecture des modèles CNN est de garder des couches complètement connectées pour la classification. Cependant, en entrées de ces couches, l&apos;image n&apos;est pas directement utilisée, mais la sortie de plusieurs opérations de convolution qui ont pour but de mettre en avant les différentes caractéristiques d&apos;une image en encodant d&apos;une certaine façon les objets qui sont présents ou non. On utilise notamment des convolutions multi-canaux qui consistent à appliquer une convolution standard à chaque canaux de l&apos;entrée puis sommer chaque produits de convolution obtenus pour obtenir une unique matrice 2D. Par exemple pour une image couleur les canaux sont le rouge, vert et bleu, on a alors 3 kernels à convoluer avec les canaux associés puis les 3 produits obtenus sont sommés.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/multichannel_convolution.png&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

**Note:** En 2D (1 seul canal), on utilise le terme *kernel* pour parler du noyau. En 3D (plus d&apos;un canal), on utilise le terme *filtre* qui est constitué d&apos;autant de kernel que le nombre de canaux du volume d&apos;entrée. 
{: .notice--info}

Plus précisément dans les CNN, une couche convolutionnelle est composée un ensemble de $N_f$ filtres de taille $N_W$ x $N_H$ x $N_C$ plus un biais par filtre suivi d&apos;une fonction d&apos;activation non linéaire. Ici, $N_W$ et $N_H$ désigne les tailles spatiales du filtre alors que $N_C$ est le nombre de canaux (parfois appelé *feature map*). Chaque filtres réalisent une convolution multi-canaux, on obtient alors $N_f$ produits de convolution qui sont concaténés dans un volume de sortie. Ces $N_f$ produits deviennent alors les canaux du prochain volume qui passera dans la prochaine couche convolutionnelle. Notez que la profondeur des filtres doit nécessairement correspondre au nombre de canaux du volume d&apos;entrée de chaque couche mais le nombre de filtres est un hyperparamètre d&apos;architecture du modèle. Au final, l&apos;enchaînement de ces convolutions multicanaux crée en sortie un volume de caractéristiques (*features*) de l&apos;image d&apos;entrée, ces features sont alors passées au réseau complètement connecté pour la classification.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/architecture_cnn.png&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

**Important:** Une couche convolutionnelle est généralement composée (en plus de la convolution) d&apos;une fonction d&apos;activation non linéaire et parfois d&apos;autres types d&apos;opérations (pooling, batch-normalization, dropout ...).
{: .notice--success}

Dans le post précédent, on a défini un MLP et son entraînement de zéro. Ici, la librairie **PyTorch** est utilisée. Elle permet de facilement construire des réseaux de neurones en profitant de son [moteur de différentiation automatique](https://pytorch.org/blog/overview-of-pytorch-autograd-engine/#what-is-autograd) pour l&apos;entraînement ainsi que ses nombreuses fonctions spécialisées (comme la [convolution](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)).

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
```

```python
class My_Custom_Model(nn.Module):

    def __init__(self):
        &apos;&apos;&apos; define some layers &apos;&apos;&apos;
        super().__init__()
        # feature learning
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool  = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # classification
        self.fc1 = nn.Linear(16*5* 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        &apos;&apos;&apos; create model architecture - how operations are linked &apos;&apos;&apos;
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

Comme vous l&apos;aurez peut être compris, ce qui est intéressant avec ces opérations de convolutions est que le poids des filtres peuvent être appris lors de l&apos;optimisation par rétropropogation du gradient puisqu&apos;il est possible de calculer de façon exacte la valeur de $\frac{\partial\mathcal{L}}{\partial W}$ par dérivation en chaîne. 


```python
# define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(my_cnn_model.parameters(), lr=0.001)
# training loop
num_epochs = 100
for epoch in range(num_epochs):
    for i, data in enumerate(train_loader):
        images, labels = data
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        # optimization step
        optimizer.step()
```

**Note:** Pour des données d&apos;entrées volumineuses, on utilise souvent comme algorithme d&apos;optimisation une *descente de gradient stochastique* où la loss est approchée en utilisant un batch de quelques données (par exemple, 8, 16 ou 32 images).  
{: .notice--info}


## Deep Dream

L&apos;un des challenges des réseaux de neurones est de comprendre ce qu&apos;il se passe exactement à chaque couche. En effet, leur architecture en cascade ainsi que leurs nombreuses interconnexions font qu&apos;il n&apos;est pas évident d&apos;interpréter le rôle de chaque filtre. La visualisation des *features* est un axe de recherches s&apos;étant développé ces dernières années qui consiste à trouver des méthodes pour comprendre comment les CNNs voient un image.

DeepDream est le nom d&apos;une de ces techniques créée en 2015 par une équipe d&apos;ingénieur de Google, l&apos;idée est d&apos;utiliser un réseau déjà entraîné à reconnaître des formes pour modifier une image afin qu&apos;un neurone donné renvoie une sortie plus élevée que les autres. L&apos;algorithme ressemble à la backpropagation classique mais au lieu de modifier les poids du réseau on ajuste les pixels de l&apos;image d&apos;entrée. De plus, le critère d&apos;optimisation n&apos;est pas une cross entropie mais directement la norme de la sortie du neurone à visualiser (ça peut être la couche entière ou un filtre) qu&apos;on va chercher à maximiser, on fait alors une montée de gradient (on pourrait également minimiser l&apos;opposée).

```python
# Parameters
iterations   = 25   # number of gradient ascent steps per octave
at_layer     = 26   # layer at which we modify image to maximize outputs
lr           = 0.02 # learning rate
octave_scale = 2    # image scale between octaves
num_octaves  = 4    # number of octaves
```

```python
# Load Model pretrained
network = models.vgg19(pretrained=True)
# Use only needed layers
layers = list(network.features.children())
model = nn.Sequential(*layers[: (at_layer + 1)])
# Use GPU is available
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = model.to(device)
```

Une astuce supplémentaire pour obtenir une visualisation intéressante est d&apos;opérer à des résolutions spatiales différentes, ici on parle d&apos;*octave*. De plus, la loss est normalisée à toutes les couches pour que la contribution des grandes couches ne l&apos;emporte pas sur celle des petites couches.

```python
# loop on different resolution scale
detail = np.zeros_like(octaves[-1])
for k, octave_base in enumerate(tqdm(octaves[::-1], desc=&quot;Octaves : &quot;)):
    # Upsample detail to new octave dimension
    if k &gt; 0: 
      detail = nd.zoom(detail, np.array(octave_base.shape)/np.array(detail.shape), order=1) 
    # Add detail from previous octave to new base
    input_image = octave_base + detail
    # Updates the image to maximize outputs for n iterations
    input_image = Variable(torch.FloatTensor(input_image).to(device), requires_grad=True)
    for i in trange(iterations, desc=&quot;Iterations : &quot;, leave=False):
        model.zero_grad()
        out  = model(input_image)
        loss = out.norm()
        loss.backward()
        # gradient ascent
        avg_grad = np.abs(input_image.grad.data.cpu().numpy()).mean()
        norm_lr  = lr/avg_grad
        input_image.data = input_image.data + norm_lr * input_image.grad.data
        input_image.data = clip(input_image.data)
        input_image.grad.data.zero_()
        # Extract deep dream details
        detail = input_image.cpu().data.numpy() - octave_base
```

On obtient, selon le nombre d&apos;itération, des images de plus en plus abstraites avec des formes psychédéliques qui apparaissent au fur et à mesure d&apos;où le nom de *DeepDream*. En fait, ces formes abstraites sont présentes surtout pour les couches les plus profondes, les premières couches accentuent généralement des *features* simples comme des arêtes, des coins, des textures ...

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/deepdream_exemple.gif&quot; width=&quot;80%&quot;/&gt;
&lt;/p&gt;

Avec cet outil, on peut créer des effets artistiques très avancées comme sur [l&apos;instagram de DeepDreamGenerator](https://www.instagram.com/deepdreamgenerator/). Mais on peut également accentuer l&apos;effet pscychédélique en faisant beaucoup d&apos;itérations ou en alimentant plusieurs fois la sortie de l&apos;algorithme en entrée. Et avec un peu d&apos;effort, on peut parvenir à visualiser à quoi ça ressemble d&apos;aller au supermarché dans ces rêves à partir d&apos;images bien réelles. 

{% include video id=&quot;DgPaCWJL7XI&quot; provider=&quot;youtube&quot; %}

Tel que présenté ci-dessus, Deep Dream présente un inconvénient si on veut le lancer sur une image de bruit blanc en entrée pour visualiser ce qui pourrait en émerger et ainsi avoir une représentation plus exact des *features* du CNN. En effet, on voit que l&apos;image reste dominée par des motifs hautes-fréquences. 

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/deepdream_noise.png&quot; width=&quot;80%&quot;/&gt;
&lt;/p&gt;

Généralement, pour contrer cet effet, ce qui marche le mieux est d&apos;introduire une régularisation d&apos;une façon ou d&apos;une autre dans le modèle. Par exemple, la robustesse à la transformation essaie de trouver des exemples qui activent toujours fortement la fonction d&apos;optimisation lorsqu&apos;on les transforment très faiblement. Concrètement, cela signifie qu&apos;on tremble, tourne, diminue ou augmente l&apos;image de façon aléatoire avant d&apos;appliquer l&apos;étape d&apos;optimisation. Les librairies [lucid](https://github.com/tensorflow/lucid) (tensorflow) et [lucent](https://github.com/greentfrapp/lucent) (pytorch) sont des packages open-source qui implémentent toutes sortes de méthodes de visualisation. 

```python
# load librairies
from lucent.optvis import render
from lucent.modelzoo import vgg19
# load model
model = vgg19(pretrained=True)
model = model.to(device)
model.eval()
# run optimisation
image = render.render_vis(model, &quot;features:30&quot;,thresholds=[100],show_inline=True)
```

Un article bien plus complète sur les techniques de visualisation de features est disponible [ici](https://distill.pub/2017/feature-visualization/)

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/lucid_viz.png&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

---

[![Generic badge](https://img.shields.io/badge/écrit_avec-Jupyter_notebook-orange.svg?style=plastic&amp;logo=Jupyter)](https://jupyter.org/try) [![Generic badge](https://img.shields.io/badge/License-MIT-blue.svg?style=plastic)](https://lbesson.mit-license.org/) [![Generic badge](https://img.shields.io/badge/acces_au_code-github-black.svg?style=plastic&amp;logo=github)](https://github.com/julienguegan/notebooks_blog/blob/main/visualisation_CNN.ipynb) [![Generic badge](https://img.shields.io/badge/execute_le_code-binder-ff69b4.svg?style=plastic&amp;logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAMAAAC%2BRQ9vAAACOlBMVEX%2F%2F%2F9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olJXmsq%2FdJX1olLVa4pXmsrmZYH1olL1olJXmspXmsrmZYH1olJXmsr1olJXmspXmsr1olJXmsr1olJXmsrmZYH1olL1olL1olJXmspXmsrmZYH1olL1olL1olJXmsrmZYH1olL1olL1olJXmsrmZYHqdnT1olJXmsq6dZf1olJXmsrKk3rmZYH1olJXmsrCc5RXmsr0n1TtgWz1olJXmspXmsrmZYH1olJXmsqNhq%2Fzmlj1olJXmspZmshXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olL1olJXmsr1olJXmsrtgGz1olL1olJXmsr1olJXmsrmZYH1olJXmsrbaYf1olJXmsr1olJXmsr1olLIcJFXmsr1olJXmsr1olJXmsr1olJXmsr1olL1olJXmspZmshZmsldmsZemsVfl8Zgl8Zom71pk8Frm7tvm7dxkL1ykLx0m7R4m7F6jbh7jbh8nK6CnKmDirOEibOGnKaInKWNhq%2BNnKGSnZ2Vg6qegKaff6WfnZSnfKGnno6ofKGvnoeweZyxeZy3noG5dpjCcpPDcpPGn3bLb4%2FPoG%2FVa4rXoGnYoGjdaIbeaIXhoWHmZYHnaX7obXvpcHjqdHXreHLroVrtgGzuhGnuh2bxk17yl1vzm1j0nlX1olIgJPdZAAAAfnRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hYWFtgYGBkZnBwcHFxdHx8fn6AgICHiIuQkJCSnKCgoKavsLCwsLO4uMDAwMDBwcTFxsjO0NDQ09TW1tjY3Nzd4ODg4uLl5%2Bjo6uvr7O3v8PDw8%2FPz9vb39%2Fj5%2Bfv7%2FPz9%2Ff5K%2BfZ5AAAI4ElEQVR42uzWAWfDQBjG8Yc4qoihEApBIIoOOpaiFAUBBB3EjFDKRImZy0d7vtuYYWN36Zq4u5v7fYO%2FB%2B%2BLwENBEARBEAR32Zc0gpcWRXmS%2FO7SHPI5PDIvaip01TrypKGlXr2B6%2FKaV%2BirGA67v%2FBa9dKrCLWXGA5anvhXlYBjopI36DdwStrxNo2AO%2Fa8WZ%2FBEaLhGHs4YdFxnGME%2B5KeY7UCtq160v%2BOFUn%2FOxLyH3QkPafSwhrxzukcYcsrp7SFHSWnlcGGnEOaQ57i0ywrqo4DpIB5QlLruI7w07w4U%2BsZ5j1R420n8Ju46qmxhmkZ1WQBJVHq6gUM66hUCujEJ3e%2B3YIqMsWQLZVmMCmSVDgLDEskFR5h0m7kLRatC3NEckSFosPCHA%2FqitEdMxjzwbxZN7eRNGG8tcpr%2BS2vA3KFmZODoFLlDaOS4%2FXxleVj9OqYacLMzMzYR%2BHsZwtz5hnvSNOSf%2F97Vc%2F0NI%2B%2FBwM0q%2FQJMsjoynXfYFr%2BPxe9SgtVijdiLT3Jjrmxlu5UIf5wlLq%2BraqTD9dfqbSjFrhY1T5jLNkzMdbRUMVy6nsqgdpYx4TKbMViHXA2bm%2BOJqoEY7QlNpVEfayDKoD3eqzhBSqNpqo4R7dcyJdjDX%2BHuW7Ouq%2BhshqCiG9yTfPDV%2FgmUWCvpLbCmSMzqsC3%2BSvWcInvEOUyZEeL5mtzxUQEfI9%2FYw3%2F8X2mZsuOVUVxEUDGP%2FwQeZ%2BSM7pSocrL8cNciDXwowQeJaWhQjK6RfwIFzU%2Fe5UfIxpiI0M%2B4npTmduWcZmfIJ%2FU1yshIxtxiTI46tZuZAxhTipDQ659yPACLksG5712IMMLuUwZHHriMuxVYBlXGBD50pHKXgWWEbNJh72MtKgKnMX%2Fxjq8KmZxrALXVNb%2BIV9TBQyAFS4mrFqFO4oNxMDHIUGV%2Bo0sGwDdHxvoT5ChcmNcL2ITl2INF9hAlKlGLz6VjXwSgxoXE%2BI7JRZvu7GJwO8Y63jRaMJRpGcCnlNJXqkgg6aGX3ij7K9Vuig2NQwYkvcNe4GhlMkzZCrOfSKbgQxDhpjGhvH7RNQfWzKLPUMi%2BeUTVEd%2Fwgc4fggtifc0Alkjm6SmeEd%2FivWgikHmGCC3bQoSqKCBsZamtKbXwuaoL4rdqQxUATYcmusQJjNHuikW227kWEvBS7YXH22qjgOQvwX24iDS%2BI%2FHe%2FQqasBtk4KveNoCXcDB%2B6NIC2IMsEc3%2FBl4o%2B7RIFZN5eETAw0T0%2FA74YOEAVW4aDU81pKx%2Bo%2BNpvp7BQ38UPdijKgXKQpxWfdZjCiOJhpluFXp6TFkolg5FXlgooFpafAiWFiNLsaQopMSvWAzwpweG5g7je9y5sgtztw5EUoPbRF%2FUOyhCw2LbMw1PrJnx9qV6gEr1%2B48MAf%2FDfZvJ66RJ0T3GHJi21KlZ%2Fn2U%2FhK1crNQ%2FoTZEKs5dia%2BcrEos2n5GpCFO0zdrv589sWqrZZtPu83FOREKaspO5xeo1KyPz156S2yDZxSldrn16tbHhUSFNaQAZ0Dezm5zcoS%2BZvPw8zRulkEzQJuIPbP1%2FZs%2BjYg85RVIZHiXScX6FKY%2FN5tyqADDJyr847tECVysITcdxUS5WTgf18iyqHvRbeLSgj9ZYqj%2BepHcjo8Lkql5dTVZfR4RtVPp%2Bn5GXIq8A6xPMGUFF9HR5r6Gb27i%2BVK94mV6BGHPOuskY%2BXhVA1wSZp1wyjtyQt%2FTxkcotncgJOTvnSP2o2mDxxp2Hjxxn5uNHDu%2FcuFi1wXdu3Ly%2F3W5%2BijKycs9xfpTjO5YoI6%2BSC3y2qXH7mQPoD6yhd6M5tA0iF0Ro1Kch1aowH%2Fbqz8DRRpiE%2FJwSmykUSEuj4Y4PIwrxsKjxVwWZIeUcwBx1CjIv1cY0uKZZIT4mB2SSP%2ByarQC%2FD4NjVPbbNuWzAiMePB3pogA%2FdnpkcIeu59MK0JoSeXcL6kNkjG866EKe5jg6%2FSpoDi%2Fhe8E6qMK0w8xQAh3Ngg9G8snC1O%2F%2Ft%2FjICKWnn0DPoc%2FlKaWnh0kF9092FrMln4wECRL4OBC1Uf55U2mpEUgdWh2vGI4xSP7gMKV3j%2FESTYfm3XwNPkUv4MTGQGG3WfbVZ%2BFe9hoMI6UfWr3%2BBHG7RsA7NMXEFJS3Rtk8msRZdLCbigRTuH2mrXpjZMF9BBkUm2OKuxUgFgKOsG%2BeDQQ2TUurw%2BUZFvLcKvU4y3Z9xRj4RABZtk6gC9Rw8uDWdeoeq7buO8lmDA39eIFEDipEwNFbnOUE5AjSBQU9qTawdEIy0CpVj%2BAa1R6zY6BY9Qo5IhO5U%2BGTiWeVBnKF70yHT0a6CsgQ0NGfMNDH6yR1CKgAvUsXalc6oiy1ibQM8kMx7xaQgfHyXA6hRy5lCJSJVrm7%2BjJw9Y2x%2B6%2F3morIIC%2FHpTDVo2R0Een%2FNGTtPb2gi1AWHQeJ0N%2FuZkVDKDnjgYxqC4lGeWTBbJEKFwvJcxLC%2FmRFCjTjcmRyBTYT5XyypCtom0TxR4XYDrksWYEHuV1JHC878%2BjJx3vzo7te86gUUq2Vibdg7bdq3aZdd9i0blUZP90PTj%2Fl0Z5gI5VCM%2FyUPI3OJq%2F9xBY1Jf94oytjCLkGiPUO6rlnlY5XSBjzo5fmlH2ssB%2Boi98q22uVekVpSVGlaLVfouJIIV%2BJWJWlloOZwcrCxWSoUXputGuHuLKEQBSGDwaDQmAxrVFtyuDaswB2UIs4a395ueKKCcyd7g4wSX%2B%2BxJ8cWequDpMVA8nVjsiGiIEsGzReWiUrhrr0SmQOtkQMZZUtxaIvdG4xWGJbMmizmW0eo1W2aTPECjsEw3n2qDi8Cpk9ajDezr66B4NfNoqyL2CGwrf0kPRfPpRv7ZjCKe9UMEngjdRilo23UYd5hHeJmEkGVIwgwyrW74iYL%2FEi9VhBVF5RHdbgKs%2FLBqswmWdtWElQnlEc1mKEH9MN63EHPyMGS%2FKfhIjFsnzmn6hYLM2myndKNFif2yvbymbxLWyUwlfHHgy%2BjfMp5eOHpOQtHo%2FH4%2FEY7x8MZ7AAyatDDgAAAABJRU5ErkJggg%3D%3D)](https://hub.gke2.mybinder.org/user/julienguegan-notebooks_blog-z8qd9bd5/notebooks/visualisation_CNN.ipynb)</content><author><name>Julien Guégan</name></author><category term="blog" /><category term="machine learning" /><category term="deep learning" /><category term="convolution" /><category term="deep dream" /><summary type="html">Les réseaux de neurones convolutionnels (CNN) sont les modèles qui ont permis de faire un bon en avant dans les problèmes de reconnaissance d’image. Ils sont au coeur de nombreuses applications allant des systèmes de sécurité par identification faciale à la classification de vos photos de vacances en passant par la génération synthétique de visage et les filtres snapchat. L’un des fondateurs de ce modèle est Yann Le Cun (un français !) qui, en 1989, applique la backpropagation du gradient pour apprendre des filtres de convolution et permet à un réseau de neurone à reconnaître des chiffres manuscrits. Cependant, c’est seulement en 2012 que les CNN se répandent largement dans la communauté scientifique de la vision par ordinateur avec Alex Krizhevsky qui conçoit l’architecture AlexNet et remporte la compétition ImageNet Large Scale Visual Recognition Challenge (1 million d’images de 1000 classes différentes) en implémentant son algorithme sur des GPUs ce qui permet au modèle d’apprendre rapidement d’une grande quantité d’image. Ce modèle atteint des performances 10% plus élevées que tous les autres à cette époque et il est désormais l’un des papiers publiés les plus influents en Computer Vision (en 2021, plus de 80 000 citations selon Google Scholar). Convolutions et Réseaux de neurones Les modèles de réseaux de neurones complètements connectés (cf post précédent) ne sont pas adaptés pour résoudre des problèmes de traitement d’image. En effet, les MLP ont chaque neurone d’une couche connecté à chaque unité d’entrée : le nombre de paramètre à apprendre devient vite élevé et une forte redondance dans les poids du réseau peut exister. De plus, pour utiliser une image dans un tel réseau, tous les pixels devrait être transformée en vecteur et aucune information sur la structure locale des pixels serait alors prise en compte. Le produit de convolution, noté $\ast$, est un opérateur qui généralise l’idée de moyenne glissante. Il s’applique aussi bien à des données temporelles (en traitement du signal par exemple) qu’à des données spatiales (en traitement d’image). Pour le cas des images, c’est-à-dire discret et en 2 dimensions, la convolution entre une image $I$ et un noyau $w$ (ou kernel) peut se calculer comme suit : \[I(i,j) * \omega =\sum_{x=-a}^a{\sum_{y=-b}^b{ I(i+x,j+y)} \ \omega(x,y)}\] L’idée est de faire glisser le noyau spatialement sur toute l’image et à chaque fois de faire une moyenne pondérée des pixels de l’image se retrouvant dans la fenêtre concernée par les éléments du noyau. Selon la valeur des éléments du noyau de convolution $w$, l’opération peut mettre en avant des caractéristiques particulières se trouvant dans l’image comme des contours, des textures, des formes. Remarque: Il existe plusieurs paramètres associés à l’opération de convolution comme la taille du noyau utilisé, la taille du pas lorsqu’on fait glisser la fenêtre sur l’image, la façon dont on gère les bords de l’image, le taux de dilatation du noyau … plus d’infos ici On peut par exemple mettre en avant les pixels d’une image correspondants aux contours horizontaux en appliquant une convolution avec un noyau de taille $3 \times 3$ avec des $-1$ dans la 1ère ligne, des $0$ dans la 2ème ligne et des $+1$ dans la 3ème ligne de la matrice. import matplotlib.pyplot as plt import numpy as np from scipy.signal import convolve2d # read image image = np.array(Image.open(&quot;path/to/file.jpg&quot;).convert(&apos;L&apos;)) # apply convolution kernel = np.array([[-1, -1, -1], [ 0, 0, 0], [+1, +1, +1]]) conv_output = convolve2d(image, kernel, mode=&apos;same&apos;) # display plt.figure(figsize=(15,5)) plt.subplot(121), plt.imshow(image, cmap=&apos;gray&apos;), plt.axis(&apos;off&apos;) plt.subplot(122), plt.imshow(np.abs(conv_output), cmap=&apos;gray&apos;), plt.axis(&apos;off&apos;) plt.tight_layout() plt.show() L’idée de l’architecture des modèles CNN est de garder des couches complètement connectées pour la classification. Cependant, en entrées de ces couches, l’image n’est pas directement utilisée, mais la sortie de plusieurs opérations de convolution qui ont pour but de mettre en avant les différentes caractéristiques d’une image en encodant d’une certaine façon les objets qui sont présents ou non. On utilise notamment des convolutions multi-canaux qui consistent à appliquer une convolution standard à chaque canaux de l’entrée puis sommer chaque produits de convolution obtenus pour obtenir une unique matrice 2D. Par exemple pour une image couleur les canaux sont le rouge, vert et bleu, on a alors 3 kernels à convoluer avec les canaux associés puis les 3 produits obtenus sont sommés. Note: En 2D (1 seul canal), on utilise le terme kernel pour parler du noyau. En 3D (plus d’un canal), on utilise le terme filtre qui est constitué d’autant de kernel que le nombre de canaux du volume d’entrée. Plus précisément dans les CNN, une couche convolutionnelle est composée un ensemble de $N_f$ filtres de taille $N_W$ x $N_H$ x $N_C$ plus un biais par filtre suivi d’une fonction d’activation non linéaire. Ici, $N_W$ et $N_H$ désigne les tailles spatiales du filtre alors que $N_C$ est le nombre de canaux (parfois appelé feature map). Chaque filtres réalisent une convolution multi-canaux, on obtient alors $N_f$ produits de convolution qui sont concaténés dans un volume de sortie. Ces $N_f$ produits deviennent alors les canaux du prochain volume qui passera dans la prochaine couche convolutionnelle. Notez que la profondeur des filtres doit nécessairement correspondre au nombre de canaux du volume d’entrée de chaque couche mais le nombre de filtres est un hyperparamètre d’architecture du modèle. Au final, l’enchaînement de ces convolutions multicanaux crée en sortie un volume de caractéristiques (features) de l’image d’entrée, ces features sont alors passées au réseau complètement connecté pour la classification. Important: Une couche convolutionnelle est généralement composée (en plus de la convolution) d’une fonction d’activation non linéaire et parfois d’autres types d’opérations (pooling, batch-normalization, dropout …). Dans le post précédent, on a défini un MLP et son entraînement de zéro. Ici, la librairie PyTorch est utilisée. Elle permet de facilement construire des réseaux de neurones en profitant de son moteur de différentiation automatique pour l’entraînement ainsi que ses nombreuses fonctions spécialisées (comme la convolution). import torch import torch.nn as nn import torch.nn.functional as F from torch import optim class My_Custom_Model(nn.Module): def __init__(self): &apos;&apos;&apos; define some layers &apos;&apos;&apos; super().__init__() # feature learning self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) # classification self.fc1 = nn.Linear(16*5* 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): &apos;&apos;&apos; create model architecture - how operations are linked &apos;&apos;&apos; x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x Comme vous l’aurez peut être compris, ce qui est intéressant avec ces opérations de convolutions est que le poids des filtres peuvent être appris lors de l’optimisation par rétropropogation du gradient puisqu’il est possible de calculer de façon exacte la valeur de $\frac{\partial\mathcal{L}}{\partial W}$ par dérivation en chaîne. # define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(my_cnn_model.parameters(), lr=0.001) # training loop num_epochs = 100 for epoch in range(num_epochs): for i, data in enumerate(train_loader): images, labels = data # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward pass optimizer.zero_grad() loss.backward() # optimization step optimizer.step() Note: Pour des données d’entrées volumineuses, on utilise souvent comme algorithme d’optimisation une descente de gradient stochastique où la loss est approchée en utilisant un batch de quelques données (par exemple, 8, 16 ou 32 images). Deep Dream L’un des challenges des réseaux de neurones est de comprendre ce qu’il se passe exactement à chaque couche. En effet, leur architecture en cascade ainsi que leurs nombreuses interconnexions font qu’il n’est pas évident d’interpréter le rôle de chaque filtre. La visualisation des features est un axe de recherches s’étant développé ces dernières années qui consiste à trouver des méthodes pour comprendre comment les CNNs voient un image. DeepDream est le nom d’une de ces techniques créée en 2015 par une équipe d’ingénieur de Google, l’idée est d’utiliser un réseau déjà entraîné à reconnaître des formes pour modifier une image afin qu’un neurone donné renvoie une sortie plus élevée que les autres. L’algorithme ressemble à la backpropagation classique mais au lieu de modifier les poids du réseau on ajuste les pixels de l’image d’entrée. De plus, le critère d’optimisation n’est pas une cross entropie mais directement la norme de la sortie du neurone à visualiser (ça peut être la couche entière ou un filtre) qu’on va chercher à maximiser, on fait alors une montée de gradient (on pourrait également minimiser l’opposée). # Parameters iterations = 25 # number of gradient ascent steps per octave at_layer = 26 # layer at which we modify image to maximize outputs lr = 0.02 # learning rate octave_scale = 2 # image scale between octaves num_octaves = 4 # number of octaves # Load Model pretrained network = models.vgg19(pretrained=True) # Use only needed layers layers = list(network.features.children()) model = nn.Sequential(*layers[: (at_layer + 1)]) # Use GPU is available device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = model.to(device) Une astuce supplémentaire pour obtenir une visualisation intéressante est d’opérer à des résolutions spatiales différentes, ici on parle d’octave. De plus, la loss est normalisée à toutes les couches pour que la contribution des grandes couches ne l’emporte pas sur celle des petites couches. # loop on different resolution scale detail = np.zeros_like(octaves[-1]) for k, octave_base in enumerate(tqdm(octaves[::-1], desc=&quot;Octaves : &quot;)): # Upsample detail to new octave dimension if k &amp;gt; 0: detail = nd.zoom(detail, np.array(octave_base.shape)/np.array(detail.shape), order=1) # Add detail from previous octave to new base input_image = octave_base + detail # Updates the image to maximize outputs for n iterations input_image = Variable(torch.FloatTensor(input_image).to(device), requires_grad=True) for i in trange(iterations, desc=&quot;Iterations : &quot;, leave=False): model.zero_grad() out = model(input_image) loss = out.norm() loss.backward() # gradient ascent avg_grad = np.abs(input_image.grad.data.cpu().numpy()).mean() norm_lr = lr/avg_grad input_image.data = input_image.data + norm_lr * input_image.grad.data input_image.data = clip(input_image.data) input_image.grad.data.zero_() # Extract deep dream details detail = input_image.cpu().data.numpy() - octave_base On obtient, selon le nombre d’itération, des images de plus en plus abstraites avec des formes psychédéliques qui apparaissent au fur et à mesure d’où le nom de DeepDream. En fait, ces formes abstraites sont présentes surtout pour les couches les plus profondes, les premières couches accentuent généralement des features simples comme des arêtes, des coins, des textures … Avec cet outil, on peut créer des effets artistiques très avancées comme sur l’instagram de DeepDreamGenerator. Mais on peut également accentuer l’effet pscychédélique en faisant beaucoup d’itérations ou en alimentant plusieurs fois la sortie de l’algorithme en entrée. Et avec un peu d’effort, on peut parvenir à visualiser à quoi ça ressemble d’aller au supermarché dans ces rêves à partir d’images bien réelles. Tel que présenté ci-dessus, Deep Dream présente un inconvénient si on veut le lancer sur une image de bruit blanc en entrée pour visualiser ce qui pourrait en émerger et ainsi avoir une représentation plus exact des features du CNN. En effet, on voit que l’image reste dominée par des motifs hautes-fréquences. Généralement, pour contrer cet effet, ce qui marche le mieux est d’introduire une régularisation d’une façon ou d’une autre dans le modèle. Par exemple, la robustesse à la transformation essaie de trouver des exemples qui activent toujours fortement la fonction d’optimisation lorsqu’on les transforment très faiblement. Concrètement, cela signifie qu’on tremble, tourne, diminue ou augmente l’image de façon aléatoire avant d’appliquer l’étape d’optimisation. Les librairies lucid (tensorflow) et lucent (pytorch) sont des packages open-source qui implémentent toutes sortes de méthodes de visualisation. # load librairies from lucent.optvis import render from lucent.modelzoo import vgg19 # load model model = vgg19(pretrained=True) model = model.to(device) model.eval() # run optimisation image = render.render_vis(model, &quot;features:30&quot;,thresholds=[100],show_inline=True) Un article bien plus complète sur les techniques de visualisation de features est disponible ici</summary></entry><entry xml:lang="fr"><title type="html">Réseau de Neurone : statistique, gradient, perceptron</title><link href="http://localhost:4000/posts/fr/2021-09-10-reseau_de_neurone/" rel="alternate" type="text/html" title="Réseau de Neurone : statistique, gradient, perceptron" /><published>2021-09-11T01:25:30+02:00</published><updated>2021-09-11T01:25:30+02:00</updated><id>http://localhost:4000/posts/fr/reseau_de_neurone</id><content type="html" xml:base="http://localhost:4000/posts/fr/2021-09-10-reseau_de_neurone/">Ces dernières années, on entend de plus en plus dans les médias les mots : *intelligence artificielle*, *réseau de neurone*, *Deep Learning* ... En effet, de nombreuses innovations ont emmergées grâce à ces technologies mais que ce cache-t-il vraiment derrière cette terminologie ? Depuis que les premiers ordinateurs programmables ont été conçus, les gens ont été étonnés de voir ces ordinateurs résoudre des tâches impossibles pour tout être humain. Cependant, ces problèmes étaient en fait faciles à décrire par une liste formelle de règles mathématiques. Le vrai challenge pour les ordinateurs est d&apos;effectuer des tâches que les humains réalisent facilement et intuitivement mais qu&apos;ils ont beaucoup plus de mal à décrire formellement comme, par exemple, reconnaître un langage ou des visages. De nos jours, on appelle intelligence artificielle (IA) toute technique permettant de résoudre un problème plus ou moins complexe par le biais d’une machine. Le Machine Learning et le Deep Learning sont des champs d&apos;étude de l&apos;IA fondés sur des théories statistiques.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/IA_classification.png&quot; width=&quot;70%&quot;/&gt;
&lt;/p&gt;

## Apprentissage Statistique

L&apos;apprentissage statistique (ou *Machine Learning* en anglais) se concentre sur l&apos;étude des algorithmes permettant aux ordinateurs d&apos;*apprendre* à partir des données et d&apos;améliorer leur performances à résoudre des tâches sans avoir explicitement programmée chacune d&apos;entre elles. Soit un ensemble $(x_i,y_i)$ de $n$ données d&apos;apprentissage (avec $x_i$ une donnée et $y_i$ sa classe). Le but de l&apos;apprentissage supervisé est de déterminer une estimation $h$ de $f$ en utilisant les données $(x_i,y_i)$ à disposition. On distingue alors deux types de prédictions selon la nature de $Y$ : si $Y \subset \mathbb{R}$, on parle de problème de **régression** et si $Y = \{1,...,I\}$ on parle de problème de **classification**. Pour estimer la loi $f$, une approche classique est la minimisation du risque empirique. Étant donné une fonction loss $L(\hat{y},y)$ (aussi appelé fonction de coût, de perte, objectif) qui mesure à quel point la prédiction $\hat{y}$ d&apos;un modèle $h$ est différente de la vraie classe $y$ alors le risque empirique est : 

$$ R_{emp}(h) = \frac{1}{n} \sum_{i=1}^n L\left(h(x_i),y_i\right) $$

Le principe de minimisation du risque empirique dit que l&apos;algorithme d&apos;apprentissage doit choisir un modèle $\hat{h}$ qui minimise ce risque empirique : $ \hat{h} = \arg \min_{h \in \mathcal{H}} R_{emp}(h) $. L&apos;algorithme d&apos;apprentissage consiste donc à résoudre un problème d&apos;optimisation. La fonction à minimiser est une approximation d&apos;une probabilité inconnue, elle est faite en moyennant les données à disposition et plus ces données seront nombreuses plus elle l&apos;approximation sera juste. À propos de la fonction *loss* $L(\hat{y},y)$, on peut en théorie utiliser la fonction *0-1 loss* pour pénaliser les erreurs et ne rien faire sinon : 

$$ L(\hat{y},y) = \left\{
    \begin{array}{ll}
        1 &amp; \text{si } \hat{y} \neq y \\
        0 &amp; \text{si } \hat{y} = y
    \end{array}
\right.$$

Mais, en pratique, il est préférable d&apos;avoir une fonction loss continue et différentiable pour l&apos;algorithme d&apos;optimisation. Par exemple, la *mean squared error* (MSE) qui est souvent choisie pour des problèmes de regression, par exemple pour un cas linéaire où l&apos;on veut trouver la meilleure droite passant par des points. Cependant, la MSE a le désavantage d&apos;être dominée par les *outliers* qui peuvent faire tendre la somme vers une valeur élevée. Une loss fréquemment utilisée en classification et plus adaptée est la *cross-entropie* (CE) :

$$ MSE =  (y - \hat{y})^2 \quad \quad \quad CE = - y \log(\hat{y}) $$

```python
def compute_loss(y_true, y_pred, name):
    if name == &apos;MSE&apos;:
        loss = (y_true - y_pred)**2
    elif name == &apos;CE&apos;:
        loss = -(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))
    return loss.mean()
```

**Note:** Pour résoudre ce problème de minimisation, le plus efficace est d&apos;utiliser un algorithme de descente de gradient (cf [post précédent](https://julienguegan.github.io/posts/2021-08-21-optimisation_profil_aile/#descente-de-gradient)) mais nécessite de connaître la dérivée exacte de la fonction Loss. Les librairies récente de Deep Learning (Pytorch, Tensorflow, Caffe ...) implémentent ces algorithmes d&apos;optimisation ainsi que des frameworks d&apos;[auto-différentation](https://fr.wikipedia.org/wiki/D%C3%A9rivation_automatique)
{: .notice--info}

Maintenant que la notion d&apos;apprentissage a été définie, l&apos;élément le plus important est de construire un modèle $h$ capable de classifier ou régresser des données. L&apos;un des modèles parmis les plus connues de nos jours est celui des réseaux de neurones.

## Réseaux de Neurones

### Perceptron 

Le premier modèle à l’origine des réseaux de neurones est le Perceptron (F. Rosenblatt,1957), il peut être vu comme un unique et simple neurone qui résout un problème de classification linéaire. Le Perceptron transforme un vecteur d&apos;entrée $X=(x_1,...,x_d) \in \mathbb{R}^d$ en une sortie $Y \in [0,1]$, l’étiquette de classification. L&apos;idée est qu&apos;on cherche à séparer notre espace d’entrée en 2 régions par un hyperplan $\mathcal{H}$ définit simplement par : 

$$\mathcal{H} : w^T X + b = 0 \iff \mathcal{H} : \sum_{i=1}^d w_i x_i + b = 0 $$

où $w=(w_1,...,w_d) \in \mathbb{R}^d$ et $b \in \mathbb{R}$ sont les paramètres de l&apos;hyperplan $\mathcal{H}$ qu&apos;on appelle communément les poids du neurone.

```python
class Perceptron:
    
    def __init__(self, n_inputs, n_iter=30, alpha=2e-4):
        self.n_iter = n_iter # number of iterations during training
        self.alpha  = alpha # learning rate
        self.w      = np.zeros((n_iter, n_inputs+1))
        self.w[0,:] = np.random.randn(n_inputs+1) # weights and bias parameters

    def predict(self, X, i=-1): 
        return np.sign(X @ self.w[i,1:] + self.w[i,0]) 
```

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/perceptron.png&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

Cet hyperplan sépare donc l&apos;espace en 2 et est donc capable de classifier un donnée $X$ en créant une règle comme $ f(X) = 1 \ \text{si } w^T X + b&lt;0  \ ; 0 \text{ sinon} $. Notre modèle de neurone se résume donc à des paramètres modélisant un hyperplan et une règle de classification. Dans notre paradigme du machine learning, on suppose qu&apos;on a, à notre disposition, un ensemble de $n$ données étiquetées $(X,Y) \in \mathbb{R}^{n\times d} \times [0,1]^n$. Comme expliqué dans la section précédentes, la phase d&apos;apprentissage consiste à minimiser l&apos;erreur de classification que fait le modèle sur ces données et ajuster les paramètres $w$ et $b$ qui permettent au modèle du Perceptron de séparer correctement ces données. 

```python
    def gradient_descent(self, i, y, y_pred):
        # construct input X and 1 to have bias
        inputs = np.hstack((np.ones((X.shape[0], 1)), X))
        # compute gradient of the MSE loss
        gradient_loss = -2 * np.dot(y_true - y_pred, inputs)
        # apply a step of gradient descent
        self.w[i+1,:] = self.w[i, :] - self.alpha * gradient_loss
```

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/perceptron_training.jpg&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

 &lt;details&gt; &lt;summary&gt; Ci-dessus, la loss MSE a été utilisée. Sa dérivée exacte en fonction de $w$ peut se calculer facilement (cf ci-dessous). &lt;/summary&gt;

 $$
    L = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 =  \frac{1}{n}\sum_{i=1}^n (y_i - (w_i x_i + b))^2 \\[5pt]
    \left\{
        \begin{array}{ccc}
            \dfrac{\partial L}{\partial w} &amp;=&amp; \dfrac{\partial}{\partial w} \dfrac{1}{n} \sum_{i=1}^n (y_i - (w_i x_i + b))^2  \\[10pt]
            \dfrac{\partial L}{\partial b} &amp;=&amp; \dfrac{\partial}{\partial b} \dfrac{1}{n} \sum_{i=1}^n (y_i - (w_i x_i + b))^2  \\
        \end{array}
    \right. \\[10pt]
    \left\{
        \begin{array}{cc}
            \dfrac{\partial L}{\partial w} &amp;=&amp; -\dfrac{2}{m}\sum_{i=1}^{m}(y_i - w_i x_i - b)x_i \\[10pt]
            \dfrac{\partial L}{\partial b} &amp;=&amp; -\dfrac{2}{m}\sum_{i=1}^{m}(y_i - w_i x_i - b)
        \end{array}
    \right. \\[10pt]
    \left\{
        \begin{array}{cc}
            \dfrac{\partial L}{\partial w} &amp;=&amp; -\dfrac{2}{m}\sum_{i=1}^{m}(y_i - \hat{y}_i)x_i \\[10pt]
            \dfrac{\partial L}{\partial b} &amp;=&amp; -\dfrac{2}{m}\sum_{i=1}^{m}(y_i - \hat{y}_i)
        \end{array}
    \right. \\
 $$
 En python, on a rajouté 1 à l&apos;entrée X pour prendre en compte l&apos;équation du biais. Le gradient s&apos;écrit bien `gradient_loss = -2 * np.dot(y_true - y_pred, inputs)`
&lt;/details&gt; {: .notice--primary}

Notez que ce modèle est très proche d&apos;une simple régression linéaire mais présenté ici pour un problème de classification, on peut facilement l&apos;adapter à une régression en supprimant la fonction `np.sign()` dans le code python. D&apos;autres part, le problème résolu ici est binaire (on a 2 classes) mais on peut facilement étendre le modèle pour qu&apos;il prédise plusieurs classes en remplaçant le vecteur de poids $w \in \mathbb{R}^d$ par une matrice $W \in \mathbb{R}^{d \times c}$ représentant donc plusieurs hyperplans séparateurs où $c$ est le nombre de classes possible, la sortie du modèle $Y$ n&apos;est plus un scalaire mais alors un vecteur de $\mathbb{R}^c$. Enfin, on notera qu&apos;en pratique, la fonction de classification $f$ est remplacée par une fonction différentiable $\sigma : \mathbb{R} \rightarrow [0,1]$. Pour un problème binaire, la fonction sigmoïde $\sigma(z)=\frac{1}{1+e^{-z}}$ est utilisée puis la classe est choisie selon un seuil (ex : $\sigma(z) &gt; 0.5$). Pour un cas multi-classe, la fonction de classification est une fonction softmax $\sigma(z_j)=\frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}}$ qui retourne un vecteur de *pseudo-probabilité* (somme à 1) et la classe retenue est choisie en gardant la position de la valeur maximum du vecteur.

### Perceptron Multi-Couches 

Comme décrit ci-dessus, un neurone unique peut résoudre un problème linéairement séparable mais échoue lorsque les données ne sont pas linéairement séparable. Pour approximer des comportements non linéaires, l&apos;idée est d&apos;ajouter notamment des fonctions non linéaires dans le modèle, elles sont appelées *fonctions d&apos;activation* (cf exemples ci-dessous). 

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/activation_function.png&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return x * (x &gt; 0)

def tanh(x):
    return np.tanh(x)
```

Le modèle du réseau de neurones ou Perceptron Multi-Couches consiste à enchaîner successivement plusieurs transformations linéaires effectuées par des neurones simples et des transformations non linéaires réalisées par ces fonctions d&apos;activations jusqu&apos;à la dernière opération qui retournera la classe prédite par le modèle. Une couche $l$ du réseau de neurone est alors composée de $m$ neurones modélisés par une matrice de poids $W^l \in \mathbb{R}^{(d+1)\times m}$ (par simplicité on intègre le biais $b$ dans $W$) ainsi que d&apos;une fonction d&apos;activation $A^l$. Au final, le réseau de neurone complet peut être décrit par une combinaison de **composition de fonctions et multiplications matricielles** en allant de la 1ère couche à la dernière : 

$$ h(X) = \hat{Y} = A^l(W^l A^{l-1}(W^{l-1} \cdots A^0(W^0 X)\cdots)) $$

```python
class MultiLayerPerceptron:
    &apos;&apos;&apos; MLP model with 2 layers &apos;&apos;&apos;

    def __init__(self, n_0, n_1, n_2):
        # initialize weights of 1st layer (hidden) and 2nd layer (output)
        self.W1 = np.random.randn(n_0, n_1)
        self.b1 = np.zeros(shape=(1, n_1))
        self.W2 = np.random.randn(n_1, n_2)
        self.b2 = np.zeros(shape=(1, n_2))        

    def forward(self, X):
        # input
        self.A0 = X
        # first layer
        self.Z1 = np.dot(self.A0, self.W1) + self.b1
        self.A1 = relu(self.Z1)
        # second layer
        self.Z2 = np.dot(self.A1, self.W2) + self.b2
        self.A2 = sigmoid(self.Z2)
        # output
        y_pred = self.A2
        return y_pred
```

**Note:** Notez qu&apos;on peut construire un réseau avec autant de couches cachées que l&apos;on veut et que chacunes de ces couches peuvent être constituées également d&apos;un nombre arbitraire de neurones et ce peu importe la dimension d&apos;entrée et de sortie du problème.
{: .notice--danger}

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/multi_layer_perceptron.png&quot; width=&quot;70%&quot;/&gt;
&lt;/p&gt;

Cet enchaînement de couches de neurones pose problème pour la phase d&apos;entraînement : le calcul de $\frac{\partial\mathcal{L}}{\partial W}$ est moins trivial que pour le modèle du neurone formel puisqu&apos;il faut prendre en compte les poids $W^l$ de chaque couche $l$. La technique de la **rétropropagation du gradient** permet d&apos;entraîner des réseaux de neurones multi-couches en se basant sur la règle de dérivation en chaîne. Le gradient de la loss est calculé en utilisant les dérivées des poids des neurones et leur fonction d&apos;activation en partant de la dernière couche jusqu&apos;à la première couche. Il faut donc parcourir le réseau vers l&apos;avant (*&lt;span style=&quot;color:green&quot;&gt;forward pass&lt;/span&gt;*) pour obtenir la valeur de la loss puis vers l&apos;arrière (*&lt;span style=&quot;color:red&quot;&gt;backward pass&lt;/span&gt;*) pour obtenir la valeur de la dérivée de la loss nécessaire à l&apos;algorithme d&apos;optimisation. Si on s&apos;intéresse au neurone $j$ de la couche $l$ vers le neurone $i$ de la couche $l+1$, on note $a$ la valeur du produit vectoriel, $o$ la sortie du neurone (après activation) et qu&apos;on garde le reste des notations précédentes, le calcul du gradient de $L$ en fonction de $W$ est :

$$ 
\begin{align*}
    \dfrac{\partial L}{\partial w_{ij}^l} &amp;= \underbrace{\quad \dfrac{\partial L}{\partial a_{j}^l} \ \ } \ \ \underbrace{\quad \dfrac{\partial a_j^l}{\partial w_{ij}^l} \quad} \\ 
                                      &amp; \qquad \ \ \ \delta_j^l \quad \ \ \ \dfrac{\partial}{\partial w_{ij}^l} \sum_{n=0}^{N_{l-1}} w_{nj}^l o_n^{l-1} = o_i^{l-1}
\end{align*} 
$$

On a donc que la dérivée $\dfrac{\partial L}{\partial w_{ij}^l}$ dépend du terme $\delta_j^l$ de la couche $l$ et de la sortie $o_i^{l-1}$ de la couche $l-1$. Ça fait sens puisque le poids $w_{ij}^l$ connecte la sortie du neurone $i$ dans la couche $l-1$ à l&apos;entrée du neurone $j$ dans la couche $l$. On développe maintenant le terme $\delta_j^l$ : 

$$ \delta_j^l = \dfrac{\partial L}{\partial a_{j}^l} =  \sum_{n=1}^{N_{l+1}} \dfrac{\partial L}{\partial a_n^{l+1}}\dfrac{\partial  a_n^{l+1}}{\partial  a_j^l} = \sum_{n=1}^{N_{l+1}} \delta_n^{l+1} \dfrac{\partial  a_n^{l+1}}{\partial  a_j^l} $$

or, on a : 

$$ 
\begin{align*}
    &amp; a_n^{l+1} &amp;=&amp; \sum_{n=1}^{N_{l}} w_{jn}^{l+1}A(a_j^l) \\
    \Rightarrow &amp; \dfrac{\partial a_n^{l+1}}{\partial  a_j^l} &amp;=&amp; \ w_{jn}^{l+1}A&apos;(a_j^l)
\end{align*}
$$

et donc :

$$ \dfrac{\partial L}{\partial w_{ij}^l} = \delta_j^l o_i^{l-1} = A&apos;(a_j^l) o_i^{l-1} \sum_{n=1}^{N_{l+1}} w_{jn}^{l+1} \delta_n^{l+1} $$ 

On obtient donc que la dérivée partielle de $L$ par rapport à $w_{ij}$ à la couche $l$ dépend également de la dérivée à la couche $l+1$. Pour calculer la dérivée de tout le réseau en utilisant la règle de la dérivation en chaîne, il est donc nécessaire de commencer par la dernière couche pour finir par la première d&apos;où le terme de *backpropagation de l&apos;erreur*.

**Attention:** Comme les calculs de la phase de backpropagation dépendent également de $a_j^l$ et $o_i^{l-1}$, il faut faire d&apos;abord une *pass forward* avant la *pass backward* pour stocker ces valeurs en mémoires.
{: .notice--warning}

```python
&apos;&apos;&apos; training methods for 2 layer MLP and cross-entropy loss &apos;&apos;&apos;
    def backward(self, X, y):
        m = y.shape[0]
        self.dZ2 = self.A2 - y
        self.dW2 = 1/m * np.dot(self.A1.T, self.dZ2)
        self.db2 = 1/m * np.sum(self.dZ2, axis=0, keepdims=True)
        self.dA1 = np.dot(self.dZ2, self.W2.T)
        self.dZ1 = np.multiply(self.dA1, d_relu(self.Z1))
        self.dW1 = 1/m * np.dot(X.T, self.dZ1)
        self.db1 = 1/m * np.sum(self.dZ1, axis=0, keepdims=True)

    def gradient_descent(self, alpha):
        self.W1 = self.W1 - alpha * self.dW1
        self.b1 = self.b1 - alpha * self.db1
        self.W2 = self.W2 - alpha * self.dW2
        self.b2 = self.b2 - alpha * self.db2
```

{% include mlp_training.html %}

**Note:** Lors de l&apos;apprentissage, on optimise seulement les poids $W$ du modèle. Le nombre de couches cachées ainsi que le nombre de neurones par couche sont fixes et ne changent pas. On parle d&apos;*hyperparamètres*, il faut les choisir lors de la conception du modèle. Des techniques de recherches d&apos;hyperparamètres optimaux existent mais sont complexes et gourmandes en temps de calcul.
{: .notice--danger}

### Aller plus loin

La popularité des réseaux de neurones ces dernières années n&apos;est en réalité pas due au modèle du MLP  présenté jusqu&apos;à présent. En effet, l&apos;inconvénient principal du MLP est le grand nombre de connexion existant entre chaque neurone qui entraîne une forte redondance et une difficulté à l&apos;entrainement lorsque le nombre de neurone et de dimension d&apos;entrée sont élevés. 

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/advanced_neural_network.png&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

Sur des problèmes complexes comme l&apos;analyse d&apos;image, le traitement de texte ou le traitement de la parole, l&apos;efficacité des réseaux de neurones actuels est, en majorité, due à des opérations et des connexions plus avancées qui permettent de modéliser et représenter efficacement ces problèmes. Par exemple, pour les images, des opérateurs de convolution sont exploités, ils tirent parti de la structure locale des pixels pour comprendre les informations présentes, allant de formes simples (lignes, cercles, couleur ...) à plus complexes (animaux, bâtiment, paysages ...). Pour des données séquentielles, les connexions LSTM (*long short-term memory*) sont capables de mémoriser des données importantes passées en évitant les problèmes de disparition de gradient. D&apos;autres parts, de nombreuses techniques existent pour améliorer la phase d&apos;apprentissage et avoir des performances qui généralisent le modèle à des données dites de test jamais vues par le modèle lors de l&apos;entraînement (augmentation de données, dropout, early stopping ...).

---

[![Generic badge](https://img.shields.io/badge/écrit_avec-Jupyter_notebook-orange.svg?style=plastic&amp;logo=Jupyter)](https://jupyter.org/try) [![Generic badge](https://img.shields.io/badge/License-MIT-blue.svg?style=plastic)](https://lbesson.mit-license.org/) [![Generic badge](https://img.shields.io/badge/acces_au_code-github-black.svg?style=plastic&amp;logo=github)](https://github.com/julienguegan/notebooks_blog/blob/main/reseau_de_neurone.ipynb) [![Generic badge](https://img.shields.io/badge/execute_le_code-binder-ff69b4.svg?style=plastic&amp;logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAMAAAC%2BRQ9vAAACOlBMVEX%2F%2F%2F9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olJXmsq%2FdJX1olLVa4pXmsrmZYH1olL1olJXmspXmsrmZYH1olJXmsr1olJXmspXmsr1olJXmsr1olJXmsrmZYH1olL1olL1olJXmspXmsrmZYH1olL1olL1olJXmsrmZYH1olL1olL1olJXmsrmZYHqdnT1olJXmsq6dZf1olJXmsrKk3rmZYH1olJXmsrCc5RXmsr0n1TtgWz1olJXmspXmsrmZYH1olJXmsqNhq%2Fzmlj1olJXmspZmshXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olL1olJXmsr1olJXmsrtgGz1olL1olJXmsr1olJXmsrmZYH1olJXmsrbaYf1olJXmsr1olJXmsr1olLIcJFXmsr1olJXmsr1olJXmsr1olJXmsr1olL1olJXmspZmshZmsldmsZemsVfl8Zgl8Zom71pk8Frm7tvm7dxkL1ykLx0m7R4m7F6jbh7jbh8nK6CnKmDirOEibOGnKaInKWNhq%2BNnKGSnZ2Vg6qegKaff6WfnZSnfKGnno6ofKGvnoeweZyxeZy3noG5dpjCcpPDcpPGn3bLb4%2FPoG%2FVa4rXoGnYoGjdaIbeaIXhoWHmZYHnaX7obXvpcHjqdHXreHLroVrtgGzuhGnuh2bxk17yl1vzm1j0nlX1olIgJPdZAAAAfnRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hYWFtgYGBkZnBwcHFxdHx8fn6AgICHiIuQkJCSnKCgoKavsLCwsLO4uMDAwMDBwcTFxsjO0NDQ09TW1tjY3Nzd4ODg4uLl5%2Bjo6uvr7O3v8PDw8%2FPz9vb39%2Fj5%2Bfv7%2FPz9%2Ff5K%2BfZ5AAAI4ElEQVR42uzWAWfDQBjG8Yc4qoihEApBIIoOOpaiFAUBBB3EjFDKRImZy0d7vtuYYWN36Zq4u5v7fYO%2FB%2B%2BLwENBEARBEAR32Zc0gpcWRXmS%2FO7SHPI5PDIvaip01TrypKGlXr2B6%2FKaV%2BirGA67v%2FBa9dKrCLWXGA5anvhXlYBjopI36DdwStrxNo2AO%2Fa8WZ%2FBEaLhGHs4YdFxnGME%2B5KeY7UCtq160v%2BOFUn%2FOxLyH3QkPafSwhrxzukcYcsrp7SFHSWnlcGGnEOaQ57i0ywrqo4DpIB5QlLruI7w07w4U%2BsZ5j1R420n8Ju46qmxhmkZ1WQBJVHq6gUM66hUCujEJ3e%2B3YIqMsWQLZVmMCmSVDgLDEskFR5h0m7kLRatC3NEckSFosPCHA%2FqitEdMxjzwbxZN7eRNGG8tcpr%2BS2vA3KFmZODoFLlDaOS4%2FXxleVj9OqYacLMzMzYR%2BHsZwtz5hnvSNOSf%2F97Vc%2F0NI%2B%2FBwM0q%2FQJMsjoynXfYFr%2BPxe9SgtVijdiLT3Jjrmxlu5UIf5wlLq%2BraqTD9dfqbSjFrhY1T5jLNkzMdbRUMVy6nsqgdpYx4TKbMViHXA2bm%2BOJqoEY7QlNpVEfayDKoD3eqzhBSqNpqo4R7dcyJdjDX%2BHuW7Ouq%2BhshqCiG9yTfPDV%2FgmUWCvpLbCmSMzqsC3%2BSvWcInvEOUyZEeL5mtzxUQEfI9%2FYw3%2F8X2mZsuOVUVxEUDGP%2FwQeZ%2BSM7pSocrL8cNciDXwowQeJaWhQjK6RfwIFzU%2Fe5UfIxpiI0M%2B4npTmduWcZmfIJ%2FU1yshIxtxiTI46tZuZAxhTipDQ659yPACLksG5712IMMLuUwZHHriMuxVYBlXGBD50pHKXgWWEbNJh72MtKgKnMX%2Fxjq8KmZxrALXVNb%2BIV9TBQyAFS4mrFqFO4oNxMDHIUGV%2Bo0sGwDdHxvoT5ChcmNcL2ITl2INF9hAlKlGLz6VjXwSgxoXE%2BI7JRZvu7GJwO8Y63jRaMJRpGcCnlNJXqkgg6aGX3ij7K9Vuig2NQwYkvcNe4GhlMkzZCrOfSKbgQxDhpjGhvH7RNQfWzKLPUMi%2BeUTVEd%2Fwgc4fggtifc0Alkjm6SmeEd%2FivWgikHmGCC3bQoSqKCBsZamtKbXwuaoL4rdqQxUATYcmusQJjNHuikW227kWEvBS7YXH22qjgOQvwX24iDS%2BI%2FHe%2FQqasBtk4KveNoCXcDB%2B6NIC2IMsEc3%2FBl4o%2B7RIFZN5eETAw0T0%2FA74YOEAVW4aDU81pKx%2Bo%2BNpvp7BQ38UPdijKgXKQpxWfdZjCiOJhpluFXp6TFkolg5FXlgooFpafAiWFiNLsaQopMSvWAzwpweG5g7je9y5sgtztw5EUoPbRF%2FUOyhCw2LbMw1PrJnx9qV6gEr1%2B48MAf%2FDfZvJ66RJ0T3GHJi21KlZ%2Fn2U%2FhK1crNQ%2FoTZEKs5dia%2BcrEos2n5GpCFO0zdrv589sWqrZZtPu83FOREKaspO5xeo1KyPz156S2yDZxSldrn16tbHhUSFNaQAZ0Dezm5zcoS%2BZvPw8zRulkEzQJuIPbP1%2FZs%2BjYg85RVIZHiXScX6FKY%2FN5tyqADDJyr847tECVysITcdxUS5WTgf18iyqHvRbeLSgj9ZYqj%2BepHcjo8Lkql5dTVZfR4RtVPp%2Bn5GXIq8A6xPMGUFF9HR5r6Gb27i%2BVK94mV6BGHPOuskY%2BXhVA1wSZp1wyjtyQt%2FTxkcotncgJOTvnSP2o2mDxxp2Hjxxn5uNHDu%2FcuFi1wXdu3Ly%2F3W5%2BijKycs9xfpTjO5YoI6%2BSC3y2qXH7mQPoD6yhd6M5tA0iF0Ro1Kch1aowH%2Fbqz8DRRpiE%2FJwSmykUSEuj4Y4PIwrxsKjxVwWZIeUcwBx1CjIv1cY0uKZZIT4mB2SSP%2ByarQC%2FD4NjVPbbNuWzAiMePB3pogA%2FdnpkcIeu59MK0JoSeXcL6kNkjG866EKe5jg6%2FSpoDi%2Fhe8E6qMK0w8xQAh3Ngg9G8snC1O%2F%2Ft%2FjICKWnn0DPoc%2FlKaWnh0kF9092FrMln4wECRL4OBC1Uf55U2mpEUgdWh2vGI4xSP7gMKV3j%2FESTYfm3XwNPkUv4MTGQGG3WfbVZ%2BFe9hoMI6UfWr3%2BBHG7RsA7NMXEFJS3Rtk8msRZdLCbigRTuH2mrXpjZMF9BBkUm2OKuxUgFgKOsG%2BeDQQ2TUurw%2BUZFvLcKvU4y3Z9xRj4RABZtk6gC9Rw8uDWdeoeq7buO8lmDA39eIFEDipEwNFbnOUE5AjSBQU9qTawdEIy0CpVj%2BAa1R6zY6BY9Qo5IhO5U%2BGTiWeVBnKF70yHT0a6CsgQ0NGfMNDH6yR1CKgAvUsXalc6oiy1ibQM8kMx7xaQgfHyXA6hRy5lCJSJVrm7%2BjJw9Y2x%2B6%2F3morIIC%2FHpTDVo2R0Een%2FNGTtPb2gi1AWHQeJ0N%2FuZkVDKDnjgYxqC4lGeWTBbJEKFwvJcxLC%2FmRFCjTjcmRyBTYT5XyypCtom0TxR4XYDrksWYEHuV1JHC878%2BjJx3vzo7te86gUUq2Vibdg7bdq3aZdd9i0blUZP90PTj%2Fl0Z5gI5VCM%2FyUPI3OJq%2F9xBY1Jf94oytjCLkGiPUO6rlnlY5XSBjzo5fmlH2ssB%2Boi98q22uVekVpSVGlaLVfouJIIV%2BJWJWlloOZwcrCxWSoUXputGuHuLKEQBSGDwaDQmAxrVFtyuDaswB2UIs4a395ueKKCcyd7g4wSX%2B%2BxJ8cWequDpMVA8nVjsiGiIEsGzReWiUrhrr0SmQOtkQMZZUtxaIvdG4xWGJbMmizmW0eo1W2aTPECjsEw3n2qDi8Cpk9ajDezr66B4NfNoqyL2CGwrf0kPRfPpRv7ZjCKe9UMEngjdRilo23UYd5hHeJmEkGVIwgwyrW74iYL%2FEi9VhBVF5RHdbgKs%2FLBqswmWdtWElQnlEc1mKEH9MN63EHPyMGS%2FKfhIjFsnzmn6hYLM2myndKNFif2yvbymbxLWyUwlfHHgy%2BjfMp5eOHpOQtHo%2FH4%2FEY7x8MZ7AAyatDDgAAAABJRU5ErkJggg%3D%3D)](https://hub.gke2.mybinder.org/user/julienguegan-notebooks_blog-z8qd9bd5/notebooks/reseau_de_neurone.ipynb)</content><author><name>Julien Guégan</name></author><category term="blog" /><category term="machine learning" /><category term="deep learning" /><summary type="html">Ces dernières années, on entend de plus en plus dans les médias les mots : intelligence artificielle, réseau de neurone, Deep Learning … En effet, de nombreuses innovations ont emmergées grâce à ces technologies mais que ce cache-t-il vraiment derrière cette terminologie ? Depuis que les premiers ordinateurs programmables ont été conçus, les gens ont été étonnés de voir ces ordinateurs résoudre des tâches impossibles pour tout être humain. Cependant, ces problèmes étaient en fait faciles à décrire par une liste formelle de règles mathématiques. Le vrai challenge pour les ordinateurs est d’effectuer des tâches que les humains réalisent facilement et intuitivement mais qu’ils ont beaucoup plus de mal à décrire formellement comme, par exemple, reconnaître un langage ou des visages. De nos jours, on appelle intelligence artificielle (IA) toute technique permettant de résoudre un problème plus ou moins complexe par le biais d’une machine. Le Machine Learning et le Deep Learning sont des champs d’étude de l’IA fondés sur des théories statistiques. Apprentissage Statistique L’apprentissage statistique (ou Machine Learning en anglais) se concentre sur l’étude des algorithmes permettant aux ordinateurs d’apprendre à partir des données et d’améliorer leur performances à résoudre des tâches sans avoir explicitement programmée chacune d’entre elles. Soit un ensemble $(x_i,y_i)$ de $n$ données d’apprentissage (avec $x_i$ une donnée et $y_i$ sa classe). Le but de l’apprentissage supervisé est de déterminer une estimation $h$ de $f$ en utilisant les données $(x_i,y_i)$ à disposition. On distingue alors deux types de prédictions selon la nature de $Y$ : si $Y \subset \mathbb{R}$, on parle de problème de régression et si $Y = {1,…,I}$ on parle de problème de classification. Pour estimer la loi $f$, une approche classique est la minimisation du risque empirique. Étant donné une fonction loss $L(\hat{y},y)$ (aussi appelé fonction de coût, de perte, objectif) qui mesure à quel point la prédiction $\hat{y}$ d’un modèle $h$ est différente de la vraie classe $y$ alors le risque empirique est : \[R_{emp}(h) = \frac{1}{n} \sum_{i=1}^n L\left(h(x_i),y_i\right)\] Le principe de minimisation du risque empirique dit que l’algorithme d’apprentissage doit choisir un modèle $\hat{h}$ qui minimise ce risque empirique : $ \hat{h} = \arg \min_{h \in \mathcal{H}} R_{emp}(h) $. L’algorithme d’apprentissage consiste donc à résoudre un problème d’optimisation. La fonction à minimiser est une approximation d’une probabilité inconnue, elle est faite en moyennant les données à disposition et plus ces données seront nombreuses plus elle l’approximation sera juste. À propos de la fonction loss $L(\hat{y},y)$, on peut en théorie utiliser la fonction 0-1 loss pour pénaliser les erreurs et ne rien faire sinon : \[L(\hat{y},y) = \left\{ \begin{array}{ll} 1 &amp;amp; \text{si } \hat{y} \neq y \\ 0 &amp;amp; \text{si } \hat{y} = y \end{array} \right.\] Mais, en pratique, il est préférable d’avoir une fonction loss continue et différentiable pour l’algorithme d’optimisation. Par exemple, la mean squared error (MSE) qui est souvent choisie pour des problèmes de regression, par exemple pour un cas linéaire où l’on veut trouver la meilleure droite passant par des points. Cependant, la MSE a le désavantage d’être dominée par les outliers qui peuvent faire tendre la somme vers une valeur élevée. Une loss fréquemment utilisée en classification et plus adaptée est la cross-entropie (CE) : \[MSE = (y - \hat{y})^2 \quad \quad \quad CE = - y \log(\hat{y})\] def compute_loss(y_true, y_pred, name): if name == &apos;MSE&apos;: loss = (y_true - y_pred)**2 elif name == &apos;CE&apos;: loss = -(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred)) return loss.mean() Note: Pour résoudre ce problème de minimisation, le plus efficace est d’utiliser un algorithme de descente de gradient (cf post précédent) mais nécessite de connaître la dérivée exacte de la fonction Loss. Les librairies récente de Deep Learning (Pytorch, Tensorflow, Caffe …) implémentent ces algorithmes d’optimisation ainsi que des frameworks d’auto-différentation Maintenant que la notion d’apprentissage a été définie, l’élément le plus important est de construire un modèle $h$ capable de classifier ou régresser des données. L’un des modèles parmis les plus connues de nos jours est celui des réseaux de neurones. Réseaux de Neurones Perceptron Le premier modèle à l’origine des réseaux de neurones est le Perceptron (F. Rosenblatt,1957), il peut être vu comme un unique et simple neurone qui résout un problème de classification linéaire. Le Perceptron transforme un vecteur d’entrée $X=(x_1,…,x_d) \in \mathbb{R}^d$ en une sortie $Y \in [0,1]$, l’étiquette de classification. L’idée est qu’on cherche à séparer notre espace d’entrée en 2 régions par un hyperplan $\mathcal{H}$ définit simplement par : \[\mathcal{H} : w^T X + b = 0 \iff \mathcal{H} : \sum_{i=1}^d w_i x_i + b = 0\] où $w=(w_1,…,w_d) \in \mathbb{R}^d$ et $b \in \mathbb{R}$ sont les paramètres de l’hyperplan $\mathcal{H}$ qu’on appelle communément les poids du neurone. class Perceptron: def __init__(self, n_inputs, n_iter=30, alpha=2e-4): self.n_iter = n_iter # number of iterations during training self.alpha = alpha # learning rate self.w = np.zeros((n_iter, n_inputs+1)) self.w[0,:] = np.random.randn(n_inputs+1) # weights and bias parameters def predict(self, X, i=-1): return np.sign(X @ self.w[i,1:] + self.w[i,0]) Cet hyperplan sépare donc l’espace en 2 et est donc capable de classifier un donnée $X$ en créant une règle comme $ f(X) = 1 \ \text{si } w^T X + b&amp;lt;0 \ ; 0 \text{ sinon} $. Notre modèle de neurone se résume donc à des paramètres modélisant un hyperplan et une règle de classification. Dans notre paradigme du machine learning, on suppose qu’on a, à notre disposition, un ensemble de $n$ données étiquetées $(X,Y) \in \mathbb{R}^{n\times d} \times [0,1]^n$. Comme expliqué dans la section précédentes, la phase d’apprentissage consiste à minimiser l’erreur de classification que fait le modèle sur ces données et ajuster les paramètres $w$ et $b$ qui permettent au modèle du Perceptron de séparer correctement ces données. def gradient_descent(self, i, y, y_pred): # construct input X and 1 to have bias inputs = np.hstack((np.ones((X.shape[0], 1)), X)) # compute gradient of the MSE loss gradient_loss = -2 * np.dot(y_true - y_pred, inputs) # apply a step of gradient descent self.w[i+1,:] = self.w[i, :] - self.alpha * gradient_loss Ci-dessus, la loss MSE a été utilisée. Sa dérivée exacte en fonction de $w$ peut se calculer facilement (cf ci-dessous). $$ L = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n}\sum_{i=1}^n (y_i - (w_i x_i + b))^2 \\[5pt] \left\{ \begin{array}{ccc} \dfrac{\partial L}{\partial w} &amp;amp;=&amp;amp; \dfrac{\partial}{\partial w} \dfrac{1}{n} \sum_{i=1}^n (y_i - (w_i x_i + b))^2 \\[10pt] \dfrac{\partial L}{\partial b} &amp;amp;=&amp;amp; \dfrac{\partial}{\partial b} \dfrac{1}{n} \sum_{i=1}^n (y_i - (w_i x_i + b))^2 \\ \end{array} \right. \\[10pt] \left\{ \begin{array}{cc} \dfrac{\partial L}{\partial w} &amp;amp;=&amp;amp; -\dfrac{2}{m}\sum_{i=1}^{m}(y_i - w_i x_i - b)x_i \\[10pt] \dfrac{\partial L}{\partial b} &amp;amp;=&amp;amp; -\dfrac{2}{m}\sum_{i=1}^{m}(y_i - w_i x_i - b) \end{array} \right. \\[10pt] \left\{ \begin{array}{cc} \dfrac{\partial L}{\partial w} &amp;amp;=&amp;amp; -\dfrac{2}{m}\sum_{i=1}^{m}(y_i - \hat{y}_i)x_i \\[10pt] \dfrac{\partial L}{\partial b} &amp;amp;=&amp;amp; -\dfrac{2}{m}\sum_{i=1}^{m}(y_i - \hat{y}_i) \end{array} \right. \\ $$ En python, on a rajouté 1 à l&apos;entrée X pour prendre en compte l&apos;équation du biais. Le gradient s&apos;écrit bien `gradient_loss = -2 * np.dot(y_true - y_pred, inputs)` Notez que ce modèle est très proche d’une simple régression linéaire mais présenté ici pour un problème de classification, on peut facilement l’adapter à une régression en supprimant la fonction np.sign() dans le code python. D’autres part, le problème résolu ici est binaire (on a 2 classes) mais on peut facilement étendre le modèle pour qu’il prédise plusieurs classes en remplaçant le vecteur de poids $w \in \mathbb{R}^d$ par une matrice $W \in \mathbb{R}^{d \times c}$ représentant donc plusieurs hyperplans séparateurs où $c$ est le nombre de classes possible, la sortie du modèle $Y$ n’est plus un scalaire mais alors un vecteur de $\mathbb{R}^c$. Enfin, on notera qu’en pratique, la fonction de classification $f$ est remplacée par une fonction différentiable $\sigma : \mathbb{R} \rightarrow [0,1]$. Pour un problème binaire, la fonction sigmoïde $\sigma(z)=\frac{1}{1+e^{-z}}$ est utilisée puis la classe est choisie selon un seuil (ex : $\sigma(z) &amp;gt; 0.5$). Pour un cas multi-classe, la fonction de classification est une fonction softmax $\sigma(z_j)=\frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}}$ qui retourne un vecteur de pseudo-probabilité (somme à 1) et la classe retenue est choisie en gardant la position de la valeur maximum du vecteur. Perceptron Multi-Couches Comme décrit ci-dessus, un neurone unique peut résoudre un problème linéairement séparable mais échoue lorsque les données ne sont pas linéairement séparable. Pour approximer des comportements non linéaires, l’idée est d’ajouter notamment des fonctions non linéaires dans le modèle, elles sont appelées fonctions d’activation (cf exemples ci-dessous). def sigmoid(x): return 1 / (1 + np.exp(-x)) def relu(x): return x * (x &amp;gt; 0) def tanh(x): return np.tanh(x) Le modèle du réseau de neurones ou Perceptron Multi-Couches consiste à enchaîner successivement plusieurs transformations linéaires effectuées par des neurones simples et des transformations non linéaires réalisées par ces fonctions d’activations jusqu’à la dernière opération qui retournera la classe prédite par le modèle. Une couche $l$ du réseau de neurone est alors composée de $m$ neurones modélisés par une matrice de poids $W^l \in \mathbb{R}^{(d+1)\times m}$ (par simplicité on intègre le biais $b$ dans $W$) ainsi que d’une fonction d’activation $A^l$. Au final, le réseau de neurone complet peut être décrit par une combinaison de composition de fonctions et multiplications matricielles en allant de la 1ère couche à la dernière : \[h(X) = \hat{Y} = A^l(W^l A^{l-1}(W^{l-1} \cdots A^0(W^0 X)\cdots))\] class MultiLayerPerceptron: &apos;&apos;&apos; MLP model with 2 layers &apos;&apos;&apos; def __init__(self, n_0, n_1, n_2): # initialize weights of 1st layer (hidden) and 2nd layer (output) self.W1 = np.random.randn(n_0, n_1) self.b1 = np.zeros(shape=(1, n_1)) self.W2 = np.random.randn(n_1, n_2) self.b2 = np.zeros(shape=(1, n_2)) def forward(self, X): # input self.A0 = X # first layer self.Z1 = np.dot(self.A0, self.W1) + self.b1 self.A1 = relu(self.Z1) # second layer self.Z2 = np.dot(self.A1, self.W2) + self.b2 self.A2 = sigmoid(self.Z2) # output y_pred = self.A2 return y_pred Note: Notez qu’on peut construire un réseau avec autant de couches cachées que l’on veut et que chacunes de ces couches peuvent être constituées également d’un nombre arbitraire de neurones et ce peu importe la dimension d’entrée et de sortie du problème. Cet enchaînement de couches de neurones pose problème pour la phase d’entraînement : le calcul de $\frac{\partial\mathcal{L}}{\partial W}$ est moins trivial que pour le modèle du neurone formel puisqu’il faut prendre en compte les poids $W^l$ de chaque couche $l$. La technique de la rétropropagation du gradient permet d’entraîner des réseaux de neurones multi-couches en se basant sur la règle de dérivation en chaîne. Le gradient de la loss est calculé en utilisant les dérivées des poids des neurones et leur fonction d’activation en partant de la dernière couche jusqu’à la première couche. Il faut donc parcourir le réseau vers l’avant (forward pass) pour obtenir la valeur de la loss puis vers l’arrière (backward pass) pour obtenir la valeur de la dérivée de la loss nécessaire à l’algorithme d’optimisation. Si on s’intéresse au neurone $j$ de la couche $l$ vers le neurone $i$ de la couche $l+1$, on note $a$ la valeur du produit vectoriel, $o$ la sortie du neurone (après activation) et qu’on garde le reste des notations précédentes, le calcul du gradient de $L$ en fonction de $W$ est : \[\begin{align*} \dfrac{\partial L}{\partial w_{ij}^l} &amp;amp;= \underbrace{\quad \dfrac{\partial L}{\partial a_{j}^l} \ \ } \ \ \underbrace{\quad \dfrac{\partial a_j^l}{\partial w_{ij}^l} \quad} \\ &amp;amp; \qquad \ \ \ \delta_j^l \quad \ \ \ \dfrac{\partial}{\partial w_{ij}^l} \sum_{n=0}^{N_{l-1}} w_{nj}^l o_n^{l-1} = o_i^{l-1} \end{align*}\] On a donc que la dérivée $\dfrac{\partial L}{\partial w_{ij}^l}$ dépend du terme $\delta_j^l$ de la couche $l$ et de la sortie $o_i^{l-1}$ de la couche $l-1$. Ça fait sens puisque le poids $w_{ij}^l$ connecte la sortie du neurone $i$ dans la couche $l-1$ à l’entrée du neurone $j$ dans la couche $l$. On développe maintenant le terme $\delta_j^l$ : \[\delta_j^l = \dfrac{\partial L}{\partial a_{j}^l} = \sum_{n=1}^{N_{l+1}} \dfrac{\partial L}{\partial a_n^{l+1}}\dfrac{\partial a_n^{l+1}}{\partial a_j^l} = \sum_{n=1}^{N_{l+1}} \delta_n^{l+1} \dfrac{\partial a_n^{l+1}}{\partial a_j^l}\] or, on a : \[\begin{align*} &amp;amp; a_n^{l+1} &amp;amp;=&amp;amp; \sum_{n=1}^{N_{l}} w_{jn}^{l+1}A(a_j^l) \\ \Rightarrow &amp;amp; \dfrac{\partial a_n^{l+1}}{\partial a_j^l} &amp;amp;=&amp;amp; \ w_{jn}^{l+1}A&apos;(a_j^l) \end{align*}\] et donc : \[\dfrac{\partial L}{\partial w_{ij}^l} = \delta_j^l o_i^{l-1} = A&apos;(a_j^l) o_i^{l-1} \sum_{n=1}^{N_{l+1}} w_{jn}^{l+1} \delta_n^{l+1}\] On obtient donc que la dérivée partielle de $L$ par rapport à $w_{ij}$ à la couche $l$ dépend également de la dérivée à la couche $l+1$. Pour calculer la dérivée de tout le réseau en utilisant la règle de la dérivation en chaîne, il est donc nécessaire de commencer par la dernière couche pour finir par la première d’où le terme de backpropagation de l’erreur. Attention: Comme les calculs de la phase de backpropagation dépendent également de $a_j^l$ et $o_i^{l-1}$, il faut faire d’abord une pass forward avant la pass backward pour stocker ces valeurs en mémoires. &apos;&apos;&apos; training methods for 2 layer MLP and cross-entropy loss &apos;&apos;&apos; def backward(self, X, y): m = y.shape[0] self.dZ2 = self.A2 - y self.dW2 = 1/m * np.dot(self.A1.T, self.dZ2) self.db2 = 1/m * np.sum(self.dZ2, axis=0, keepdims=True) self.dA1 = np.dot(self.dZ2, self.W2.T) self.dZ1 = np.multiply(self.dA1, d_relu(self.Z1)) self.dW1 = 1/m * np.dot(X.T, self.dZ1) self.db1 = 1/m * np.sum(self.dZ1, axis=0, keepdims=True) def gradient_descent(self, alpha): self.W1 = self.W1 - alpha * self.dW1 self.b1 = self.b1 - alpha * self.db1 self.W2 = self.W2 - alpha * self.dW2 self.b2 = self.b2 - alpha * self.db2 Note: Lors de l’apprentissage, on optimise seulement les poids $W$ du modèle. Le nombre de couches cachées ainsi que le nombre de neurones par couche sont fixes et ne changent pas. On parle d’hyperparamètres, il faut les choisir lors de la conception du modèle. Des techniques de recherches d’hyperparamètres optimaux existent mais sont complexes et gourmandes en temps de calcul. Aller plus loin La popularité des réseaux de neurones ces dernières années n’est en réalité pas due au modèle du MLP présenté jusqu’à présent. En effet, l’inconvénient principal du MLP est le grand nombre de connexion existant entre chaque neurone qui entraîne une forte redondance et une difficulté à l’entrainement lorsque le nombre de neurone et de dimension d’entrée sont élevés. Sur des problèmes complexes comme l’analyse d’image, le traitement de texte ou le traitement de la parole, l’efficacité des réseaux de neurones actuels est, en majorité, due à des opérations et des connexions plus avancées qui permettent de modéliser et représenter efficacement ces problèmes. Par exemple, pour les images, des opérateurs de convolution sont exploités, ils tirent parti de la structure locale des pixels pour comprendre les informations présentes, allant de formes simples (lignes, cercles, couleur …) à plus complexes (animaux, bâtiment, paysages …). Pour des données séquentielles, les connexions LSTM (long short-term memory) sont capables de mémoriser des données importantes passées en évitant les problèmes de disparition de gradient. D’autres parts, de nombreuses techniques existent pour améliorer la phase d’apprentissage et avoir des performances qui généralisent le modèle à des données dites de test jamais vues par le modèle lors de l’entraînement (augmentation de données, dropout, early stopping …).</summary></entry><entry xml:lang="fr"><title type="html">Optimisation : algorithme, XFOIL, profil d’aile</title><link href="http://localhost:4000/posts/fr/2021-08-21-optimisation_profil_aile/" rel="alternate" type="text/html" title="Optimisation : algorithme, XFOIL, profil d’aile" /><published>2021-08-22T04:25:30+02:00</published><updated>2021-08-22T04:25:30+02:00</updated><id>http://localhost:4000/posts/fr/optimisation_profil_aile</id><content type="html" xml:base="http://localhost:4000/posts/fr/2021-08-21-optimisation_profil_aile/">Dans la vie de tous les jours, on cherche souvent à optimiser nos actions pour faire le moins d&apos;effort et bien, dans le monde de l&apos;ingénierie, c&apos;est la même chose. Les problèmes de minimisation sont omniprésents dans de nombreux systèmes que ce soit pour obtenir un gain de temps, d’argent, d’énergie, de matière première, ou encore de satisfaction. On peut par exemple chercher à optimiser un trajet, la forme d’un objet, un prix de vente, une réaction chimique, le contrôle aérien, le rendement d’un appareil, le fonctionnement d&apos;un moteur ... La complexité des problèmes et de leur modélisation fait de l&apos;optimisation une branche des mathématiques très vaste et variée, en pratique la qualité des résultats dépend de la pertinence du modèle, du bon choix des variables que l&apos;on cherche à optimiser, de l’efficacité de l’algorithme et des moyens pour le traitement numérique. Dans le domaine de l&apos;aérodynamisme, la forme des avions et des voitures de courses est souvent designer pour que l&apos;énergie dépensée soit minimum. Après avoir introduit quelques aspects algorithmiques du problème de minimisation, l&apos;article suivant présentera comment le profil d&apos;une aile d&apos;avion peut être optimisé pour maximiser ses performances.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/optimization_problems.png&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

## Algorithmes d&apos;optimisation

Face à la résolution d&apos;un problème d&apos;optimisation, une 1&lt;sup&gt;ère&lt;/sup&gt; étape est d&apos;identifier à quelle catégorie il appartient. En effet, les algorithmes sont plus ou moins adaptés pour des catégories données puisque le problème peut être continue ou discret, avec ou sans contraintes, différentiable ou non, convexe ou non ... On écrit un problème d&apos;optimisation sans contraintes simplement :

$$ \min_{x \in X} f(x) $$

où $f$ peut être appelée fonction objectif ou fonction coût. En théorie, pour des problèmes non contraints, on peut trouver le(s) minimum(s) en regardant quand $ \nabla f(x) = 0 $ ([condition du premier ordre](https://fr.wikipedia.org/wiki/Conditions_d%27optimalit%C3%A9#Conditions_du_premier_ordre_sans_contrainte)) et la positivité de la hessienne $H(x)$ ([condition du second ordre](https://fr.wikipedia.org/wiki/Conditions_d%27optimalit%C3%A9#Conditions_du_deuxi%C3%A8me_ordre_sans_contrainte)). Pour un problème avec contraintes, les [conditions de Kuhn-Tucker](https://fr.wikipedia.org/wiki/Conditions_de_Karush-Kuhn-Tucker) appliquées à la fonction [Lagrangienne](https://fr.wikipedia.org/wiki/Multiplicateur_de_Lagrange) permettent de transformer le problème en un nouveau sans contraintes mais avec des inconnues supplémentaires.

**Note:** Un problème de maximisation peut être facilement transposer en un problème de minimisation : 
$$\max_{x \in X} f(x) \Leftrightarrow \min_{x \in X} - f(x)$$ 
{: .notice--primary}

Pour des cas simples, on peut donc résoudre analytiquement le problème. Par exemple, lorsque $f$ a une forme quadratique, linéaire et sans contrainte, annuler le gradient revient à résoudre un système linéaire. Mais, en pratique, le gradient peut avoir une forme trop compliqué ou même la fonction $f$ peut ne pas avoir de forme analytique connue (elle peut être le résultat d&apos;une d&apos;EDP résolu numériquement par exemple). Il existe donc une grande variété d&apos;[algorithmes d&apos;optimisation](https://fr.wikipedia.org/wiki/Cat%C3%A9gorie:Algorithme_d%27optimisation) itératifs pour essayer de trouver le minimum, certains étant plus ou moins adaptés à certains type de problème. D&apos;autre part, il est courant de valider ces algorithmes en les testant sur des fonctions connues pour lesquelles on connaît analytiquement la valeur du vraie mimimum, elles permettent d&apos;évaluer les caractéristiques des approches comme la vitesse de convergence, la robustesses, la précision ou le comportement général. Une liste assez complète de ces fonctions test est disponible sur [wikipedia](https://en.wikipedia.org/wiki/Test_functions_for_optimization).

Par soucis de simplicité, les 3 approches ci-dessous sont des approches simples qui permettent d&apos;introduire les notions basiques des algorithmes d&apos;optimisation, elles seront testées sur la fonction test de Himmelblau. Dans la réalité, on fait le plus souvent appelle a des librairies ou logiciels spécialisés qui implémentent des approches bien plus sophistiquées.

&lt;center&gt;
{% include himmelblau.html %}
&lt;/center&gt;

### Descente de gradient

![image-right](/assets/images/gradient_descent.gif){: .align-right width=&quot;45%&quot;} L&apos;algorithme de descente de gradient permet de minimiser des fonctions réelles différentiables. Cette approche itérative améliore successivement le point recherché en se déplacement dans la direction opposé au gradient de façon à faire décroître la fonction objectif. Dans sa version simple, l&apos;algorithme trouve un minimum local (et pas global) et peut présenter certains inconvénients comme par exemple la difficulté à converger si le paramètre $\alpha$ (le pas de la descente) est mal réglé. Il existe tout une famille de méthodes dites *à directions de descente* qui exploitent le gradient pour converger le minimum de $f$ de façon plus efficace. 

```python 
def gradient_descent(f, x0, gradient, alpha=0.01, itermax=1000):
    # initialization
    x, fx = np.zeros((itermax+1, len(x0))), np.zeros((itermax+1, 1))
    x[0,:] = x0
    # iterative loop
    k = 0
    while (k &lt; itermax):
        grad_fxk = gradient(f, x[k,:]) # use analytical expression or numerical approximation
        x[k+1,:] = x[k,:] - alpha * grad_fxk
        k = k+1
    return x
```

**Note:** Si le pas de descente $\alpha$ est trop petit, l&apos;algorithme risque de converger trop lentement (voir jamais). Si $\alpha$ est trop grand, l&apos;algorithme peut diverger (notamment en zigzaguant dans les vallées étroites)
{: .notice--info}

### Nelder-Mead

![image-right](/assets/images/nelder_mead.gif){: .align-right width=&quot;45%&quot;} Un problème majeur des algorithmes à directions de descente est qu&apos;elles sont surtout efficaces pour des fonctions différentiables et lorsqu&apos;on connaît l&apos;expression exacte du gradient de $f$. On peut néanmoins approximer le gradient par schéma numérique mais l&apos;approximation faite rend souvent cette approche inefficace. La méthode de Nelder-Mead est une méthode qui exploite le concept de [simplexe](https://fr.wikipedia.org/wiki/Simplexe) : une figure de $N+1$ sommets pour un espace à $N$ dimensions. L&apos;idée consiste, à chaque itération, d&apos;évaluer la valeur de la fonction $f$ à chaque point du simplexe et, selon ses valeurs, effectuer des transformations géométriques du simplexe (réflexion, expansion, contraction). Par exemple, dans une vallée, le simplexe sera étiré dans la direction où $f$ diminue. Bien que simple, cet algorithme permet de trouver un minimum sans calcul de gradient cependant il est moins efficace quand la dimension d&apos;entrée $N$ est grande.

```python 
def nelder_mead(f, x0, params=2, itermax=1000):
    c = params   
    # initialization
    x1, x2, x3 = np.array([[x0[0]-0.5,x0[1]],[x0[0],x0[1]],[x0[0],x0[1]+0.5]])
    x  = np.array([x1, x2, x3])
    xm = np.zeros((itermax+1, len(x0)))
    # iterative loop
    k = 0
    while (k &lt; itermax):
        # SORT SIMPLEX
        A = f(x.T)
        index = np.argsort(A) 
        x_min, x_max, x_bar = x[index[0],:], x[index[2],:], (x[index[0],:] + x[index[1],:])/2
        # REFLECTION
        x_refl = x_bar + (x_bar - x_max)
        # EXPANSION
        if f(x_refl) &lt; f(x_min): 
            x_exp = x_bar + 2*(x_bar - x_max)
            if f(x_exp) &lt; f(x_refl):
                x_max = x_exp
            else:
                x_max = x_refl
        elif (f(x_min) &lt; f(x_refl)) and (f(x_refl) &lt; f(x_max)):
            x_max = x_refl 
        # CONTRACTION
        else: 
            x_con = x_bar - (x_bar - x_max)/2
            if f(x_con) &lt; f(x_min):
                x_max = x_con
            else:
                x[index[1],:] = x_max + (x[index[1],:] - x_min)/2
        # UPDATE DATAs
        x = np.array([x_max, x[index[1],:], x_min])
        xm[k+1,:] = x_bar
        k = k+1
    return xm[:k+1,:]
```

**Attention:** Comme la descente de gradient, la méthode de Nelder-Mead converge vers un minimum local de la fonction $f$ et non global. Il est toutefois possible de redémarrer l&apos;algorithme avec une valeur d&apos;initialisation $x_0$ différente pour espérer converger vers un nouveau minimum plus petit.
{: .notice--warning}

### Stratégie d&apos;évolution

![image-right](/assets/images/evolution_strategy.gif){: .align-right width=&quot;45%&quot;} Les méthodes présentées précédemment sont capables de trouver des minima mais des minima locaux et non globaux. Les techniques dites stratégies d&apos;évolution sont des [métaheuristiques](https://fr.wikipedia.org/wiki/M%C3%A9taheuristique) inspirées de la théorie de l&apos;évolution qui converge statistiquement vers un minimum global. L&apos;idée est de partir d&apos;une population de $\mu$ *parents* qui vont produire $\lambda$ *enfants*. Parmis ces $\lambda$ enfants, seuls ceux les mieux sont sélectionné pour faire partie de la prochaine *génération*. Le vocabulaire utilisé est celui de l&apos;évolution mais, en pratique, on fait des tirages aléatoires de points et on garde ceux pour lesquelles la fonction $f$ est minimale. This algorithm can find a global minimum but the main drawback is that it requires a large number of evaluations of the function $f$ which is generally computationally expensive.

```python 
def evolution_strategy(f, x0, params=[5,3,1], imax=1000):
    # parameters
    dim = len(x0)
    lambd, mu, tau = params
    # initialization
    x, xp, s = np.zeros((imax+1, dim)), np.zeros((imax+1, lambd, dim)), np.zeros((imax+1, dim))
    x[0,:] = x0
    s[0,:] = [0.1,0.1]
    # ITERATIVE LOOP
    k = 0
    while (k &lt; imax):
        # GENERATION
        sp = s[k,:] * np.exp(tau * randn(lambd, dim))
        xp[k,:,:] = x[k,:] + sp * randn(lambd, dim)
        Zp = [f(xi) for xi in xp[k,:,:]]
        # SELECTION
        mins = np.argsort(Zp)[:mu]
        xc   = xp[k,mins,:]
        sc   = sp[mins,:]
        # UPDATE
        x[k+1,:] = np.mean(xc, 0)
        s[k+1,:] = np.mean(sc, 0)
        k = k+1
    return x[:k+1,:]
```

## Problème d&apos;aérodynamisme

Imaginons qu&apos;on veuille créer un avion d&apos;un poids de $P=6 kg$ et qui aura une vitesse moyenne de vol de $V=12 m/s$, le problème est de designer les ailes de l&apos;avion de telle façon que l&apos;énergie qui sera dépensée soit minimum. Si on considère un vol stationnaire, il existe 4 forces principales qui s&apos;opposent : la poussée (produit par les moteurs), la trainée ( due à la résistance de l&apos;air, le profil de l&apos;aile, la compressibilité ...), le poids (gravité terrestre), et la portance (plus d&apos;infos chez [science étonnante](https://www.youtube.com/watch?v=r-ESaj_4ujc)). Le but de notre problème d&apos;optimisation du profil d&apos;aile est donc de trouver une forme d&apos;aile qui minimisera la trainée et maximisera la portance. La portance verticale $F_y$ d&apos;une aile et la trainée horizontale $F_x$ sont calculées grâce aux formules suivantes issues de la mécanique des fluides : 

$$ F_y = \frac{1}{2}\, \rho\, S\, V^2\, C_y  \quad \text{et} \quad  F_x = \frac{1}{2}\, \rho\, S\, V^2\, C_x $$

avec 
- $\rho$ la masse volumique de l&apos;air ($kg/m^3$)
- $S$ la surface de l&apos;aile ($m^2$)
- $V$ la vitesse ($m/s$)
- $C_y$ le coefficient de portance
- $C_x$ le coefficient de trainée

Finalement, la fonction à minimiser s&apos;écrit : 

$$ f(x) = F_x + \max(0, P - F_y) $$

avec $$x = \left[ \text{NACA}_{M} \text{, NACA}_{P} \text{, NACA}_{XX} \text{, L, }\alpha \right]$$
{: .text-center}

```python 
# constantes
poids = 6
Ro    = 1
V     = 12
# function to minimize
def cost_function(x):
    # call xfoil
    write_xfoil(x)
    os.system(r&apos;xfoil.exe &lt; input.dat&apos;)
    CL, CD = read_xfoil()
    # compute COST function
    L  = x[3]
    c  = (1/10)*L
    S  = L*c
    Fx = 0.5*Ro*S*V**2*CD
    Fy = 0.5*Ro*S*V**2*CL
    y  = Fx + max(0, poids-Fy)
    return y
```

Les paramètres à trouver définissant la forme de l&apos;aile sont la géométrie du profil, l&apos;envergure $L$ de l&apos;aile et l&apos;[angle d&apos;attaque](https://fr.wikipedia.org/wiki/Incidence_(a%C3%A9rodynamique)) $\alpha$. La géométrie du profil peut être définie par le code [NACA](https://fr.wikipedia.org/wiki/Profil_NACA) MPXX où M est la cambrure maximale, P le point de cambrure maximal par rapport au bord d&apos;attaque de la corde, et XX l&apos;épaisseur maximale du profil. Par exemple, le profil aérodynamique NACA 2412 possède une cambrure maximale de $2%$ à $40%$ à partir du bord d&apos;attaque avec une épaisseur maximale de 12%. D&apos;autres part, pour simplifier, on supposera que la corde est 10 fois plus petite que l&apos;envergure de l&apos;aile. Ensuite, pour pouvoir évaluer les forces $F_x$ et $F_y$, il est nécessaire de connaître les coefficients $C_y$ et $C_x$. Ces coefficients dépendent de la forme de l&apos;aile ainsi que de grandeurs physiques comme le nombre de Mach $Ma = \frac{v}{a} = \frac{12}{340}$ et le nombre de Reynolds $Re = \frac{\rho v L}{\mu} = \frac{12L}{1.8 10^{-5}}$, l&apos;estimation de $C_x$ et $C_y$ n&apos;est pas un problème évident. Mais des solveurs aérodynamiques comme [XFOIL](https://web.mit.edu/drela/Public/web/xfoil/) implémentent des outils pour calculer ces coefficients (cf [page 16](http://acversailles.free.fr/documentation/08~Documentation_Generale_M_Suire/Aerodynamique/Profils/Programmes/X%20Foil/xfoil_doc.pdf)). L&apos;idée est donc d&apos;executer des commandes XFOIL et de récupérer sa sortie à chaque fois que la fonction coût $f$ doit être évaluée.

```python 
def read_xfoil():
    with open(&quot;results.dat&quot;, &quot;r&quot;) as file:
        coeffs = file.readlines()[-1]
    CL = float(coeffs.split()[1])
    CD = float(coeffs.split()[2])
    return CL, CD

def write_xfoil(x):
    NACAx, NACAy  = int(x[0]), int(x[1])
    NACAep, alpha = int(x[2]), x[4]
    corde    = (1/10)*x[3]
    mach     = 12/340
    reynolds = corde*12./(1.8*10e-5)
    # write command in file
    file = open(&quot;input.dat&quot;, &quot;w&quot;)
    file.write(&quot;plop\ng\n\n&quot;)
    file.write(&quot;naca &quot;+str(NACAx)+str(NACAy)+str(NACAep)+&quot;\n\noper\n&quot;)
    file.write(&quot;mach &quot;+str(mach)+&quot;\n&quot;)
    file.write(&quot;visc &quot;+str(reynolds)+&quot;\n&quot;)
    file.write(&quot;pacc\nresults.dat\ny\n\n&quot;)
    file.write(&quot;alfa &quot;+str(alpha)+&quot;\n\nquit&quot;)
    file.close()
```

Maintenant qu&apos;on est capable de calculer la fonction $f$ à minimiser, on peut appliquer un algorithme d&apos;optimisation. Étant que les méthodes présentées dans la section précédente sont basiques, il n&apos;y a pas de honte à utiliser directement des librairies comme [Scipy](https://docs.scipy.org/doc/scipy/reference/optimize.html) qui implémente la métaheuristique du [recuit simulé](https://fr.wikipedia.org/wiki/Recuit_simul%C3%A9) (s&apos;inspire de processus métallurgique). Scipy possède plusieurs autres algorithmes d&apos;optimisation mais, après expériences, c&apos;est celui-ci qui semblait être le plus efficace.


```python
from scipy import optimize
x0 = np.array([2, 4, 12, 5, 5])
bounds = [(0,4),(2,8),(10,20),(2,6),(0,10)]
optimize.dual_annealing(cost_function, bounds, x0=x0, maxiter=10)
```

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/optimization_airfoil.gif&quot; width=&quot;200%&quot;/&gt;
&lt;/p&gt;


&lt;center&gt;
{% include wing_plotly.html %}
&lt;/center&gt;


---

[![Generic badge](https://img.shields.io/badge/écrit_avec-Jupyter_notebook-orange.svg?style=plastic&amp;logo=Jupyter)](https://jupyter.org/try) [![Generic badge](https://img.shields.io/badge/License-MIT-blue.svg?style=plastic)](https://lbesson.mit-license.org/) [![Generic badge](https://img.shields.io/badge/acces_au_code-github-black.svg?style=plastic&amp;logo=github)](https://github.com/julienguegan/notebooks_blog/blob/main/optimisation.ipynb) [![Generic badge](https://img.shields.io/badge/execute_le_code-binder-ff69b4.svg?style=plastic&amp;logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAMAAAC%2BRQ9vAAACOlBMVEX%2F%2F%2F9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olJXmsq%2FdJX1olLVa4pXmsrmZYH1olL1olJXmspXmsrmZYH1olJXmsr1olJXmspXmsr1olJXmsr1olJXmsrmZYH1olL1olL1olJXmspXmsrmZYH1olL1olL1olJXmsrmZYH1olL1olL1olJXmsrmZYHqdnT1olJXmsq6dZf1olJXmsrKk3rmZYH1olJXmsrCc5RXmsr0n1TtgWz1olJXmspXmsrmZYH1olJXmsqNhq%2Fzmlj1olJXmspZmshXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olL1olJXmsr1olJXmsrtgGz1olL1olJXmsr1olJXmsrmZYH1olJXmsrbaYf1olJXmsr1olJXmsr1olLIcJFXmsr1olJXmsr1olJXmsr1olJXmsr1olL1olJXmspZmshZmsldmsZemsVfl8Zgl8Zom71pk8Frm7tvm7dxkL1ykLx0m7R4m7F6jbh7jbh8nK6CnKmDirOEibOGnKaInKWNhq%2BNnKGSnZ2Vg6qegKaff6WfnZSnfKGnno6ofKGvnoeweZyxeZy3noG5dpjCcpPDcpPGn3bLb4%2FPoG%2FVa4rXoGnYoGjdaIbeaIXhoWHmZYHnaX7obXvpcHjqdHXreHLroVrtgGzuhGnuh2bxk17yl1vzm1j0nlX1olIgJPdZAAAAfnRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hYWFtgYGBkZnBwcHFxdHx8fn6AgICHiIuQkJCSnKCgoKavsLCwsLO4uMDAwMDBwcTFxsjO0NDQ09TW1tjY3Nzd4ODg4uLl5%2Bjo6uvr7O3v8PDw8%2FPz9vb39%2Fj5%2Bfv7%2FPz9%2Ff5K%2BfZ5AAAI4ElEQVR42uzWAWfDQBjG8Yc4qoihEApBIIoOOpaiFAUBBB3EjFDKRImZy0d7vtuYYWN36Zq4u5v7fYO%2FB%2B%2BLwENBEARBEAR32Zc0gpcWRXmS%2FO7SHPI5PDIvaip01TrypKGlXr2B6%2FKaV%2BirGA67v%2FBa9dKrCLWXGA5anvhXlYBjopI36DdwStrxNo2AO%2Fa8WZ%2FBEaLhGHs4YdFxnGME%2B5KeY7UCtq160v%2BOFUn%2FOxLyH3QkPafSwhrxzukcYcsrp7SFHSWnlcGGnEOaQ57i0ywrqo4DpIB5QlLruI7w07w4U%2BsZ5j1R420n8Ju46qmxhmkZ1WQBJVHq6gUM66hUCujEJ3e%2B3YIqMsWQLZVmMCmSVDgLDEskFR5h0m7kLRatC3NEckSFosPCHA%2FqitEdMxjzwbxZN7eRNGG8tcpr%2BS2vA3KFmZODoFLlDaOS4%2FXxleVj9OqYacLMzMzYR%2BHsZwtz5hnvSNOSf%2F97Vc%2F0NI%2B%2FBwM0q%2FQJMsjoynXfYFr%2BPxe9SgtVijdiLT3Jjrmxlu5UIf5wlLq%2BraqTD9dfqbSjFrhY1T5jLNkzMdbRUMVy6nsqgdpYx4TKbMViHXA2bm%2BOJqoEY7QlNpVEfayDKoD3eqzhBSqNpqo4R7dcyJdjDX%2BHuW7Ouq%2BhshqCiG9yTfPDV%2FgmUWCvpLbCmSMzqsC3%2BSvWcInvEOUyZEeL5mtzxUQEfI9%2FYw3%2F8X2mZsuOVUVxEUDGP%2FwQeZ%2BSM7pSocrL8cNciDXwowQeJaWhQjK6RfwIFzU%2Fe5UfIxpiI0M%2B4npTmduWcZmfIJ%2FU1yshIxtxiTI46tZuZAxhTipDQ659yPACLksG5712IMMLuUwZHHriMuxVYBlXGBD50pHKXgWWEbNJh72MtKgKnMX%2Fxjq8KmZxrALXVNb%2BIV9TBQyAFS4mrFqFO4oNxMDHIUGV%2Bo0sGwDdHxvoT5ChcmNcL2ITl2INF9hAlKlGLz6VjXwSgxoXE%2BI7JRZvu7GJwO8Y63jRaMJRpGcCnlNJXqkgg6aGX3ij7K9Vuig2NQwYkvcNe4GhlMkzZCrOfSKbgQxDhpjGhvH7RNQfWzKLPUMi%2BeUTVEd%2Fwgc4fggtifc0Alkjm6SmeEd%2FivWgikHmGCC3bQoSqKCBsZamtKbXwuaoL4rdqQxUATYcmusQJjNHuikW227kWEvBS7YXH22qjgOQvwX24iDS%2BI%2FHe%2FQqasBtk4KveNoCXcDB%2B6NIC2IMsEc3%2FBl4o%2B7RIFZN5eETAw0T0%2FA74YOEAVW4aDU81pKx%2Bo%2BNpvp7BQ38UPdijKgXKQpxWfdZjCiOJhpluFXp6TFkolg5FXlgooFpafAiWFiNLsaQopMSvWAzwpweG5g7je9y5sgtztw5EUoPbRF%2FUOyhCw2LbMw1PrJnx9qV6gEr1%2B48MAf%2FDfZvJ66RJ0T3GHJi21KlZ%2Fn2U%2FhK1crNQ%2FoTZEKs5dia%2BcrEos2n5GpCFO0zdrv589sWqrZZtPu83FOREKaspO5xeo1KyPz156S2yDZxSldrn16tbHhUSFNaQAZ0Dezm5zcoS%2BZvPw8zRulkEzQJuIPbP1%2FZs%2BjYg85RVIZHiXScX6FKY%2FN5tyqADDJyr847tECVysITcdxUS5WTgf18iyqHvRbeLSgj9ZYqj%2BepHcjo8Lkql5dTVZfR4RtVPp%2Bn5GXIq8A6xPMGUFF9HR5r6Gb27i%2BVK94mV6BGHPOuskY%2BXhVA1wSZp1wyjtyQt%2FTxkcotncgJOTvnSP2o2mDxxp2Hjxxn5uNHDu%2FcuFi1wXdu3Ly%2F3W5%2BijKycs9xfpTjO5YoI6%2BSC3y2qXH7mQPoD6yhd6M5tA0iF0Ro1Kch1aowH%2Fbqz8DRRpiE%2FJwSmykUSEuj4Y4PIwrxsKjxVwWZIeUcwBx1CjIv1cY0uKZZIT4mB2SSP%2ByarQC%2FD4NjVPbbNuWzAiMePB3pogA%2FdnpkcIeu59MK0JoSeXcL6kNkjG866EKe5jg6%2FSpoDi%2Fhe8E6qMK0w8xQAh3Ngg9G8snC1O%2F%2Ft%2FjICKWnn0DPoc%2FlKaWnh0kF9092FrMln4wECRL4OBC1Uf55U2mpEUgdWh2vGI4xSP7gMKV3j%2FESTYfm3XwNPkUv4MTGQGG3WfbVZ%2BFe9hoMI6UfWr3%2BBHG7RsA7NMXEFJS3Rtk8msRZdLCbigRTuH2mrXpjZMF9BBkUm2OKuxUgFgKOsG%2BeDQQ2TUurw%2BUZFvLcKvU4y3Z9xRj4RABZtk6gC9Rw8uDWdeoeq7buO8lmDA39eIFEDipEwNFbnOUE5AjSBQU9qTawdEIy0CpVj%2BAa1R6zY6BY9Qo5IhO5U%2BGTiWeVBnKF70yHT0a6CsgQ0NGfMNDH6yR1CKgAvUsXalc6oiy1ibQM8kMx7xaQgfHyXA6hRy5lCJSJVrm7%2BjJw9Y2x%2B6%2F3morIIC%2FHpTDVo2R0Een%2FNGTtPb2gi1AWHQeJ0N%2FuZkVDKDnjgYxqC4lGeWTBbJEKFwvJcxLC%2FmRFCjTjcmRyBTYT5XyypCtom0TxR4XYDrksWYEHuV1JHC878%2BjJx3vzo7te86gUUq2Vibdg7bdq3aZdd9i0blUZP90PTj%2Fl0Z5gI5VCM%2FyUPI3OJq%2F9xBY1Jf94oytjCLkGiPUO6rlnlY5XSBjzo5fmlH2ssB%2Boi98q22uVekVpSVGlaLVfouJIIV%2BJWJWlloOZwcrCxWSoUXputGuHuLKEQBSGDwaDQmAxrVFtyuDaswB2UIs4a395ueKKCcyd7g4wSX%2B%2BxJ8cWequDpMVA8nVjsiGiIEsGzReWiUrhrr0SmQOtkQMZZUtxaIvdG4xWGJbMmizmW0eo1W2aTPECjsEw3n2qDi8Cpk9ajDezr66B4NfNoqyL2CGwrf0kPRfPpRv7ZjCKe9UMEngjdRilo23UYd5hHeJmEkGVIwgwyrW74iYL%2FEi9VhBVF5RHdbgKs%2FLBqswmWdtWElQnlEc1mKEH9MN63EHPyMGS%2FKfhIjFsnzmn6hYLM2myndKNFif2yvbymbxLWyUwlfHHgy%2BjfMp5eOHpOQtHo%2FH4%2FEY7x8MZ7AAyatDDgAAAABJRU5ErkJggg%3D%3D)](https://hub.gke2.mybinder.org/user/julienguegan-notebooks_blog-z8qd9bd5/notebooks/optimisation.ipynb)</content><author><name>Julien Guégan</name></author><category term="blog" /><category term="optimisation" /><category term="gradient" /><category term="heuristique" /><category term="aérodynamisme" /><category term="modélisation" /><summary type="html">Dans la vie de tous les jours, on cherche souvent à optimiser nos actions pour faire le moins d’effort et bien, dans le monde de l’ingénierie, c’est la même chose. Les problèmes de minimisation sont omniprésents dans de nombreux systèmes que ce soit pour obtenir un gain de temps, d’argent, d’énergie, de matière première, ou encore de satisfaction. On peut par exemple chercher à optimiser un trajet, la forme d’un objet, un prix de vente, une réaction chimique, le contrôle aérien, le rendement d’un appareil, le fonctionnement d’un moteur … La complexité des problèmes et de leur modélisation fait de l’optimisation une branche des mathématiques très vaste et variée, en pratique la qualité des résultats dépend de la pertinence du modèle, du bon choix des variables que l’on cherche à optimiser, de l’efficacité de l’algorithme et des moyens pour le traitement numérique. Dans le domaine de l’aérodynamisme, la forme des avions et des voitures de courses est souvent designer pour que l’énergie dépensée soit minimum. Après avoir introduit quelques aspects algorithmiques du problème de minimisation, l’article suivant présentera comment le profil d’une aile d’avion peut être optimisé pour maximiser ses performances. Algorithmes d’optimisation Face à la résolution d’un problème d’optimisation, une 1ère étape est d’identifier à quelle catégorie il appartient. En effet, les algorithmes sont plus ou moins adaptés pour des catégories données puisque le problème peut être continue ou discret, avec ou sans contraintes, différentiable ou non, convexe ou non … On écrit un problème d’optimisation sans contraintes simplement : \[\min_{x \in X} f(x)\] où $f$ peut être appelée fonction objectif ou fonction coût. En théorie, pour des problèmes non contraints, on peut trouver le(s) minimum(s) en regardant quand $ \nabla f(x) = 0 $ (condition du premier ordre) et la positivité de la hessienne $H(x)$ (condition du second ordre). Pour un problème avec contraintes, les conditions de Kuhn-Tucker appliquées à la fonction Lagrangienne permettent de transformer le problème en un nouveau sans contraintes mais avec des inconnues supplémentaires. Note: Un problème de maximisation peut être facilement transposer en un problème de minimisation : \(\max_{x \in X} f(x) \Leftrightarrow \min_{x \in X} - f(x)\) Pour des cas simples, on peut donc résoudre analytiquement le problème. Par exemple, lorsque $f$ a une forme quadratique, linéaire et sans contrainte, annuler le gradient revient à résoudre un système linéaire. Mais, en pratique, le gradient peut avoir une forme trop compliqué ou même la fonction $f$ peut ne pas avoir de forme analytique connue (elle peut être le résultat d’une d’EDP résolu numériquement par exemple). Il existe donc une grande variété d’algorithmes d’optimisation itératifs pour essayer de trouver le minimum, certains étant plus ou moins adaptés à certains type de problème. D’autre part, il est courant de valider ces algorithmes en les testant sur des fonctions connues pour lesquelles on connaît analytiquement la valeur du vraie mimimum, elles permettent d’évaluer les caractéristiques des approches comme la vitesse de convergence, la robustesses, la précision ou le comportement général. Une liste assez complète de ces fonctions test est disponible sur wikipedia. Par soucis de simplicité, les 3 approches ci-dessous sont des approches simples qui permettent d’introduire les notions basiques des algorithmes d’optimisation, elles seront testées sur la fonction test de Himmelblau. Dans la réalité, on fait le plus souvent appelle a des librairies ou logiciels spécialisés qui implémentent des approches bien plus sophistiquées. Descente de gradient L’algorithme de descente de gradient permet de minimiser des fonctions réelles différentiables. Cette approche itérative améliore successivement le point recherché en se déplacement dans la direction opposé au gradient de façon à faire décroître la fonction objectif. Dans sa version simple, l’algorithme trouve un minimum local (et pas global) et peut présenter certains inconvénients comme par exemple la difficulté à converger si le paramètre $\alpha$ (le pas de la descente) est mal réglé. Il existe tout une famille de méthodes dites à directions de descente qui exploitent le gradient pour converger le minimum de $f$ de façon plus efficace. def gradient_descent(f, x0, gradient, alpha=0.01, itermax=1000): # initialization x, fx = np.zeros((itermax+1, len(x0))), np.zeros((itermax+1, 1)) x[0,:] = x0 # iterative loop k = 0 while (k &amp;lt; itermax): grad_fxk = gradient(f, x[k,:]) # use analytical expression or numerical approximation x[k+1,:] = x[k,:] - alpha * grad_fxk k = k+1 return x Note: Si le pas de descente $\alpha$ est trop petit, l’algorithme risque de converger trop lentement (voir jamais). Si $\alpha$ est trop grand, l’algorithme peut diverger (notamment en zigzaguant dans les vallées étroites) Nelder-Mead Un problème majeur des algorithmes à directions de descente est qu’elles sont surtout efficaces pour des fonctions différentiables et lorsqu’on connaît l’expression exacte du gradient de $f$. On peut néanmoins approximer le gradient par schéma numérique mais l’approximation faite rend souvent cette approche inefficace. La méthode de Nelder-Mead est une méthode qui exploite le concept de simplexe : une figure de $N+1$ sommets pour un espace à $N$ dimensions. L’idée consiste, à chaque itération, d’évaluer la valeur de la fonction $f$ à chaque point du simplexe et, selon ses valeurs, effectuer des transformations géométriques du simplexe (réflexion, expansion, contraction). Par exemple, dans une vallée, le simplexe sera étiré dans la direction où $f$ diminue. Bien que simple, cet algorithme permet de trouver un minimum sans calcul de gradient cependant il est moins efficace quand la dimension d’entrée $N$ est grande. def nelder_mead(f, x0, params=2, itermax=1000): c = params # initialization x1, x2, x3 = np.array([[x0[0]-0.5,x0[1]],[x0[0],x0[1]],[x0[0],x0[1]+0.5]]) x = np.array([x1, x2, x3]) xm = np.zeros((itermax+1, len(x0))) # iterative loop k = 0 while (k &amp;lt; itermax): # SORT SIMPLEX A = f(x.T) index = np.argsort(A) x_min, x_max, x_bar = x[index[0],:], x[index[2],:], (x[index[0],:] + x[index[1],:])/2 # REFLECTION x_refl = x_bar + (x_bar - x_max) # EXPANSION if f(x_refl) &amp;lt; f(x_min): x_exp = x_bar + 2*(x_bar - x_max) if f(x_exp) &amp;lt; f(x_refl): x_max = x_exp else: x_max = x_refl elif (f(x_min) &amp;lt; f(x_refl)) and (f(x_refl) &amp;lt; f(x_max)): x_max = x_refl # CONTRACTION else: x_con = x_bar - (x_bar - x_max)/2 if f(x_con) &amp;lt; f(x_min): x_max = x_con else: x[index[1],:] = x_max + (x[index[1],:] - x_min)/2 # UPDATE DATAs x = np.array([x_max, x[index[1],:], x_min]) xm[k+1,:] = x_bar k = k+1 return xm[:k+1,:] Attention: Comme la descente de gradient, la méthode de Nelder-Mead converge vers un minimum local de la fonction $f$ et non global. Il est toutefois possible de redémarrer l’algorithme avec une valeur d’initialisation $x_0$ différente pour espérer converger vers un nouveau minimum plus petit. Stratégie d’évolution Les méthodes présentées précédemment sont capables de trouver des minima mais des minima locaux et non globaux. Les techniques dites stratégies d’évolution sont des métaheuristiques inspirées de la théorie de l’évolution qui converge statistiquement vers un minimum global. L’idée est de partir d’une population de $\mu$ parents qui vont produire $\lambda$ enfants. Parmis ces $\lambda$ enfants, seuls ceux les mieux sont sélectionné pour faire partie de la prochaine génération. Le vocabulaire utilisé est celui de l’évolution mais, en pratique, on fait des tirages aléatoires de points et on garde ceux pour lesquelles la fonction $f$ est minimale. This algorithm can find a global minimum but the main drawback is that it requires a large number of evaluations of the function $f$ which is generally computationally expensive. def evolution_strategy(f, x0, params=[5,3,1], imax=1000): # parameters dim = len(x0) lambd, mu, tau = params # initialization x, xp, s = np.zeros((imax+1, dim)), np.zeros((imax+1, lambd, dim)), np.zeros((imax+1, dim)) x[0,:] = x0 s[0,:] = [0.1,0.1] # ITERATIVE LOOP k = 0 while (k &amp;lt; imax): # GENERATION sp = s[k,:] * np.exp(tau * randn(lambd, dim)) xp[k,:,:] = x[k,:] + sp * randn(lambd, dim) Zp = [f(xi) for xi in xp[k,:,:]] # SELECTION mins = np.argsort(Zp)[:mu] xc = xp[k,mins,:] sc = sp[mins,:] # UPDATE x[k+1,:] = np.mean(xc, 0) s[k+1,:] = np.mean(sc, 0) k = k+1 return x[:k+1,:] Problème d’aérodynamisme Imaginons qu’on veuille créer un avion d’un poids de $P=6 kg$ et qui aura une vitesse moyenne de vol de $V=12 m/s$, le problème est de designer les ailes de l’avion de telle façon que l’énergie qui sera dépensée soit minimum. Si on considère un vol stationnaire, il existe 4 forces principales qui s’opposent : la poussée (produit par les moteurs), la trainée ( due à la résistance de l’air, le profil de l’aile, la compressibilité …), le poids (gravité terrestre), et la portance (plus d’infos chez science étonnante). Le but de notre problème d’optimisation du profil d’aile est donc de trouver une forme d’aile qui minimisera la trainée et maximisera la portance. La portance verticale $F_y$ d’une aile et la trainée horizontale $F_x$ sont calculées grâce aux formules suivantes issues de la mécanique des fluides : \[F_y = \frac{1}{2}\, \rho\, S\, V^2\, C_y \quad \text{et} \quad F_x = \frac{1}{2}\, \rho\, S\, V^2\, C_x\] avec $\rho$ la masse volumique de l’air ($kg/m^3$) $S$ la surface de l’aile ($m^2$) $V$ la vitesse ($m/s$) $C_y$ le coefficient de portance $C_x$ le coefficient de trainée Finalement, la fonction à minimiser s’écrit : \[f(x) = F_x + \max(0, P - F_y)\] avec \(x = \left[ \text{NACA}_{M} \text{, NACA}_{P} \text{, NACA}_{XX} \text{, L, }\alpha \right]\) # constantes poids = 6 Ro = 1 V = 12 # function to minimize def cost_function(x): # call xfoil write_xfoil(x) os.system(r&apos;xfoil.exe &amp;lt; input.dat&apos;) CL, CD = read_xfoil() # compute COST function L = x[3] c = (1/10)*L S = L*c Fx = 0.5*Ro*S*V**2*CD Fy = 0.5*Ro*S*V**2*CL y = Fx + max(0, poids-Fy) return y Les paramètres à trouver définissant la forme de l’aile sont la géométrie du profil, l’envergure $L$ de l’aile et l’angle d’attaque $\alpha$. La géométrie du profil peut être définie par le code NACA MPXX où M est la cambrure maximale, P le point de cambrure maximal par rapport au bord d’attaque de la corde, et XX l’épaisseur maximale du profil. Par exemple, le profil aérodynamique NACA 2412 possède une cambrure maximale de $2%$ à $40%$ à partir du bord d’attaque avec une épaisseur maximale de 12%. D’autres part, pour simplifier, on supposera que la corde est 10 fois plus petite que l’envergure de l’aile. Ensuite, pour pouvoir évaluer les forces $F_x$ et $F_y$, il est nécessaire de connaître les coefficients $C_y$ et $C_x$. Ces coefficients dépendent de la forme de l’aile ainsi que de grandeurs physiques comme le nombre de Mach $Ma = \frac{v}{a} = \frac{12}{340}$ et le nombre de Reynolds $Re = \frac{\rho v L}{\mu} = \frac{12L}{1.8 10^{-5}}$, l’estimation de $C_x$ et $C_y$ n’est pas un problème évident. Mais des solveurs aérodynamiques comme XFOIL implémentent des outils pour calculer ces coefficients (cf page 16). L’idée est donc d’executer des commandes XFOIL et de récupérer sa sortie à chaque fois que la fonction coût $f$ doit être évaluée. def read_xfoil(): with open(&quot;results.dat&quot;, &quot;r&quot;) as file: coeffs = file.readlines()[-1] CL = float(coeffs.split()[1]) CD = float(coeffs.split()[2]) return CL, CD def write_xfoil(x): NACAx, NACAy = int(x[0]), int(x[1]) NACAep, alpha = int(x[2]), x[4] corde = (1/10)*x[3] mach = 12/340 reynolds = corde*12./(1.8*10e-5) # write command in file file = open(&quot;input.dat&quot;, &quot;w&quot;) file.write(&quot;plop\ng\n\n&quot;) file.write(&quot;naca &quot;+str(NACAx)+str(NACAy)+str(NACAep)+&quot;\n\noper\n&quot;) file.write(&quot;mach &quot;+str(mach)+&quot;\n&quot;) file.write(&quot;visc &quot;+str(reynolds)+&quot;\n&quot;) file.write(&quot;pacc\nresults.dat\ny\n\n&quot;) file.write(&quot;alfa &quot;+str(alpha)+&quot;\n\nquit&quot;) file.close() Maintenant qu’on est capable de calculer la fonction $f$ à minimiser, on peut appliquer un algorithme d’optimisation. Étant que les méthodes présentées dans la section précédente sont basiques, il n’y a pas de honte à utiliser directement des librairies comme Scipy qui implémente la métaheuristique du recuit simulé (s’inspire de processus métallurgique). Scipy possède plusieurs autres algorithmes d’optimisation mais, après expériences, c’est celui-ci qui semblait être le plus efficace. from scipy import optimize x0 = np.array([2, 4, 12, 5, 5]) bounds = [(0,4),(2,8),(10,20),(2,6),(0,10)] optimize.dual_annealing(cost_function, bounds, x0=x0, maxiter=10)</summary></entry><entry xml:lang="fr"><title type="html">Dynamique des populations : EDO, écologie, logistique</title><link href="http://localhost:4000/posts/fr/2021-08-11-dynamique_des_populations/" rel="alternate" type="text/html" title="Dynamique des populations : EDO, écologie, logistique" /><published>2021-08-12T04:18:30+02:00</published><updated>2021-08-12T04:18:30+02:00</updated><id>http://localhost:4000/posts/fr/dynamique_des_populations</id><content type="html" xml:base="http://localhost:4000/posts/fr/2021-08-11-dynamique_des_populations/">Parmis les enjeux du 21&lt;sup&gt;ème&lt;/sup&gt; siècle, l&apos;écologie a un rôle majeure puisqu&apos;elle est la science qui étudie les interactions des êtres vivants entre eux et avec leur milieu. Pour modéliser ces interactions, la dynamique des populations est la branche qui s&apos;intéresse aux fluctuations démographiques des espèces. Ses applications sont nombreuses puisqu&apos;elle peut permettre de répondre à des problèmes variés comme la gestion d&apos;espèces menacées, la protection des cultures contre des nuisibles, le contrôle de bioréacteurs ou la prédiction des épidémies.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/ecologie.png&quot;/&gt;
&lt;/p&gt;

## Modèle de Verhulst

À la fin du 18&lt;sup&gt;ème&lt;/sup&gt; siècle, le modèle de **Malthus** décrit la variation d&apos;une taille de population $y$ au cours du temps $t$ par l&apos;équation différentielle ordinaire[^1] (EDO) :

[^1]: Le terme ordinaire est utilisé par opposition au terme équation différentielle partielle (ou équation aux dérivées partielles) où la ou les fonctions inconnues peuvent dépendre de plusieurs variables.

$$ y&apos;(t) = (n-m) y(t) = r y(t) $$

avec les constantes : $n$ le taux de natalité, $m$ le taux de mortalité et $r$ le taux de croissance. Ce modèle nous dit que, selon le taux de croissance $r$, la taille des populations peut soit diminuer, rester constante ou augmenter de manière exponentielle. Ce modèle ne reflète pas la réalité puisque une population n&apos;augmentera jamais à l&apos;infini.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/malthus_verlhust_photos.png&quot; width=&quot;50%&quot;/&gt;
&lt;/p&gt;

En 1840, **Verlhust** propose un modèle de croissance plus adapté en partant de l&apos;hypothèse que le taux de croissance $r$ n&apos;est pas une constante mais est fonction affine de la taille de population $y$ :

$$ y&apos;(t) = \big(n(y) - m(y)\big) y(t) $$

Verlhust part notamment de l&apos;hypothèse que plus la taille d&apos;une population augmente alors plus son taux de natalité $n$ diminue et plus son taux de mortalité $m$ augmente. En partant de cette hypothèse et en appliquant quelques manipulations algébriques astucieuses, on peut montrer que l&apos;équation différentielle précédente peut se réécrire sous la forme :

$$ y&apos;(t) = r y(t) \left(1 - \frac{y(t)}{K}\right) $$

avec $K$ une constante appelée *capacité d&apos;accueil*. On peut résoudre analytiquement cette équation avec la condition initiale $y(t=0)=y_0$, on obtient la **solution logistique** :

$$ y(t) = \frac{K}{1+\left(\frac{K}{y_0}-1\right)e^{-rt}} $$

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/verlhust_graph.png&quot; width=&quot;70%&quot;/&gt;
&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;Résolution détaillée de l&apos;équation différentielle logistique par séparation de variable&lt;/summary&gt;

  $$
  \begin{align*}
    \int_{y_0}^{y(t)} \frac{1}{y(1-y/K)}dy &amp;= \int_0^t r \ d\tau \\
    \int_{y_0}^{y(t)} \frac{K}{y(K-y)}dy &amp;= \int_0^t r \ d\tau \\
    \int_{y_0}^{y(t)} \frac{1}{y}dy +  \int_{y_0}^{y(t)} \frac{1}{K-1}dy &amp;= \int_0^t r \ d\tau \\
    \ln \left| \frac{y(t)}{y_0} \right| - \ln \left| \frac{K-y(t)}{K-y_0} \right| &amp;= r \ t \\
    \ln \left( \frac{y(t)\big(K-y_0\big)}{y_0\big(K-y(t)\big)} \right) &amp;= r \ t \\
    \frac{y(t)}{K-y(t)} &amp;= \frac{y_0}{K-y_0}e^{rt} \\
    y(t)\left(1+\frac{y_0}{K-y_0}e^{rt} \right) &amp;= \frac{K y_0 e^{rt}}{K-y_0} \\
    y(t) &amp;= \frac{Ky_0e^{rt}}{K-y_0+y_0e^{rt}} \\
    y(t) &amp;= \frac{K y_0}{(K-y_0)e^{-rt}+y_0} \\
  \end{align*} \\
  \square
  $$
&lt;/details&gt; {: .notice--primary}

On remarque que $ \lim\limits_{t\to\infty} y(t) = K $. Ce qui signifie que peut importe la taille de la population initiale $y_0$, la population finira toujours par tendre vers $K$ la capacité d&apos;accueil qu&apos;on qualifie souvent comme le nombre d’individus maximal que le milieu peut accueillir (selon l&apos;espace, les ressources ...). Cette [fonction dite logistique](https://fr.wikipedia.org/wiki/Fonction_logistique_(Verhulst)) introduite pour la première fois par Verlhust pour modéliser la croissance des populations trouvera par la suite plein d&apos;application dans des domaines variés comme l&apos;économie, la chimie, les statistiques et plus récemment les réseaux de neurones artificielles.

## Modèle de Lotka-Volterra

Les modèles de Lotka-Volterra sont des sytèmes d&apos;équations simples qui sont apparus au début du 20&lt;sup&gt;ème&lt;/sup&gt; siècle. Ils portent le nom de deux mathématiciens qui ont publié en même temps mais indépendamment sur le sujet : Volterra, en 1926, pour modéliser les populations de sardines et de leurs prédateurs et Lotka, en 1924, dans son livre _Elements of Physical Biology_. Contrairement au modèle de Verlhust qui s&apos;intéresse à une seule population, les modèles de Lotka-Volterra modélisent les interactions entre plusieurs espèces, chacune ayant un impact sur le développement de l&apos;autres.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/lotka_volterra_photos.png&quot; width=&quot;50%&quot;/&gt;
&lt;/p&gt;

### *Proie-prédateur*

Le modèle proie-prédateur de Lotka-Volterra a permis d&apos;expliquer des données collectées de certaines populations d&apos;animaux comme le lynx et lièvre ainsi que le loup et l&apos;élan aux Etats-Unis. On y représente l&apos;évolution du nombre proies $x$ et de prédateurs $y$ au cours du temps $t$ selon le modèle suivant :

$$
\left\{
  \begin{array}{ccc}
    x&apos;(t) = x(t)\ \big(\alpha - \beta y(t)\big) \\
    y&apos;(t) = y(t)\ \big( \delta x(t) - \gamma\big)
  \end{array}
\right.
$$

avec les paramètres $\alpha$ et $\delta$ sont les taux de reproduction respectivement des proies et des prédateurs et $\beta$ et $\gamma$ sont les taux de mortalité, respectivement, des proies et des prédateurs. 

**Note:** On parle de système autonome : le temps $t$ n&apos;apparaît pas explicitement dans les équations.
{: .notice--primary}

Si on développe chacune des équations, on peut plus facilement donner une interprétation. Pour les proies, on a d&apos;une part le terme $\alpha x(t)$ qui modélise la croissance exponentielle avec une source illimitée de nourriture et d&apos;autre part $- \beta x(t) y(t)$ qui représente la prédation proportionnelle à la fréquence de rencontre entre prédateurs et proies. L&apos;équation des prédateurs est très semblable à celle des proies, $\delta x(t)y(t)$ est la croissance des prédateurs proportionnelle à la quantité de nourriture disponible (les proies) et $- \gamma y(t)$ représente la mort naturelle des prédateurs.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/fox_rabbit.gif&quot; width=&quot;70%&quot;/&gt;
&lt;/p&gt;

On peut caculer les équilibres de ce système d&apos;équations différentielles et également en déduire un comportement mais les solutions n&apos;ont pas d&apos;expression analytique simple. Néanmoins, il est possible de calculer une solution approchée numériquement (plus de détails dans la [`section suivante`](#méthode-numérique-pour-les-edo)).

```python
# define ODE function to resolve
r, c, m, b = 3, 4, 1, 2
def prey_predator(XY, t=0):
    dX = r*XY[0] - c*XY[0]*XY[1]
    dY = b*XY[0]*XY[1] - m*XY[1]
    return [dX, dY]
```
```python
# discretization
T0   = 0
Tmax = 12
n    = 200
T    = np.linspace(T0, Tmax, n) 
```
On calcule ici l&apos;évolution des 2 populations en fonction du temps pour une condition initiale fixée, on voit qu&apos;elles ont un comportement périodique et en décalage de phase.
```python
# TEMPORAL DYNAMIC
X0 = [1,1]
solution = integrate.odeint(prey_predator, X0, T) # use scipy solver
```
&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/lotka_volterra_graph2.png&quot; width=&quot;70%&quot;/&gt;
&lt;/p&gt;

Ici, on calcule plusieurs solutions pour différentes conditions initiales qu&apos;on affiche dans l&apos;espace de phase (le temps n&apos;appararaît pas). On peut également afficher le champ de vecteur généré par le système d&apos;équation avec `plt.quiver()` pour une grille de valeur.

```python
# PHASES SPACE
# some trajectories
orbits = []
for i in range(5):
    X0    = [0.2+i*0.1, 0.2+i*0.1]
    orbit = integrate.odeint(prey_predator, X0, T)
    orbits.append(orbit) 
# vector field
x, y             = np.linspace(0, 2.5, 20), np.linspace(0, 2, 20)
X_grid, Y_grid   = np.meshgrid(x, y)                      
DX_grid, DY_grid = prey_predator([X_grid, Y_grid])
N                = np.sqrt(DX_grid ** 2 + DY_grid ** 2) 
N[N==0]          = 1
DX_grid, DY_grid = DX_grid/N, DY_grid/N
```

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/lotka_volterra_graph1.png&quot; width=&quot;70%&quot;/&gt;
&lt;/p&gt;

**Attention:** Les unités des simulations ne reflète pas la réalité, il faut des populations suffisamment grandes pour que la modélisation soit correcte.
{: .notice--danger}

Dans le modèle utilisé, les prédateurs prospèrent lorsque les proies sont nombreuses, mais finissent par épuiser leurs ressources et déclinent. Lorsque la population de prédateurs a suffisamment diminué, les proies profitant du répit se reproduisent et leur population augmente de nouveau. Cette dynamique se poursuit en un cycle de croissance et déclin. Il existe 2 équilibres : le point $(0,0)$ est un point de selle instable qui montre que l&apos;extinction des 2 espèce est en fait quasiment impossible à obtenir et le point $(\frac{\gamma}{\delta}, \frac{\alpha}{\beta})$ est un centre stable, les populations oscillent autour cet état.

**Note:** Cette modélisation reste assez simple, un grande nombre de variante existe. On peut rajouter des termes de disparition des 2 espèces (dus à la pêche, chasse, pesticide ...), tenir compte de la capacité d&apos;accueil du milieu en utilisant un terme logistique.
{: .notice--info}

### *Compétition*

Le modèle de compétition de Lotka-Volterra est une variante du modèle de prédation où les 2 espèces n&apos;ont pas une hierarchie de proies et prédateurs mais sont en compétition l&apos;une et l&apos;autre. De plus, la dynamique de base n&apos;est plus une simple croissance exponentielle mais logistique (avec les paramètres $r_i$ et $K_i$) : 

$$
\left\{
  \begin{array}{ccc}
    x_1&apos;(t) = r_1x_1(t)\left(1- \frac{x_1(t)+\alpha_{12}x_2(t)}{K_1}\right) \\
    x_2&apos;(t) = r_2x_2(t)\left(1- \frac{x_2(t)+\alpha_{21}x_1(t)}{K_2}\right)
  \end{array}
\right.
$$

avec $\alpha_{12}$ l&apos;effet de l&apos;espèce 2 sur la population de l&apos;espèce 1 et réciproquement $\alpha_{21}$ l&apos;effet de l&apos;espèce 2 sur l&apos;espèce 1. Par exemple, pour l&apos;équation de l&apos;espèce 1, le coefficient $\alpha_{12}$ est multiplié par la taille de la population $x_2$. Quand $\alpha_{12} &lt; 1$ alors l&apos;effet de l&apos;espèce 2 sur l&apos;espèce 1 est plus petit que l&apos;effet de l&apos;espèce 1 sur ces propres membres. Et inversement, quand $\alpha_{12} &gt; 1$, l&apos;effet de l&apos;espèce 2 sur l&apos;espèce 1 est supérieur à l&apos;effet de l&apos;espèce 1 sur ces propres membres.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/competition_interspecific.jfif&quot; width=&quot;60%&quot;/&gt;
&lt;/p&gt;

Pour comprendre plus en détails les prédictions du modèles, il est utile de tracer comme précédemment les diagrammes d&apos;espace de phase $(x_1,x_2)$. On peut distinguer 4 scénarios selon les valeurs des coefficients de compétition, j&apos;affiche ci-dessous les champs de vecteurs de ces scénarios avec `plt.streamplot()` ainsi que les isoclines, les courbes pour lesquelles $$x_1&apos;(t)=0$$ ou $$x_2&apos;(t)=0$$:

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/lotka_volterra_graph3.png&quot; width=&quot;70%&quot;/&gt;
&lt;/p&gt;

```python
# define ODE to resolve
r1, K1 = 3, 1
r2, K2 = 3, 1
def competition(X1X2, a1, a2):
    dX1 = r1*X1X2[0] * (1-(X1X2[0]+a1*X1X2[1])/K1)
    dX2 = r2*X1X2[1] * (1-(X1X2[1]+a2*X1X2[0])/K2)
    return [dX1, dX2]
```
```python
# compute derivatives for each scenario
N = 20
x, y = np.linspace(0, 2.5, N), np.linspace(0, 2, N)
X_grid, Y_grid = np.meshgrid(x, y)
DX_grid, DY_grid = np.zeros((4,N,N)), np.zeros((4,N,N))
coeffs = np.array([[1.5,1.5],[0.5,0.5],[1.5,0.5],[0.5,1.5]])
for k,(a1,a2) in enumerate(coeffs):
    DX_grid[k,:], DY_grid[k,:] = competition([X_grid, Y_grid], a1, a2)
```

Au final, les 4 comportements possibles en fonction de $\alpha_{12}$ et $\alpha_{21}$ sont les suivants :

1. Exclusion compétitive d&apos;une des deux espèces en fonction des conditions initiales.
2. Coexistence stable des deux espèces.
3. Exclusion compétitive de l&apos;espèce 1 par l&apos;espèce 2.
4. Exclusion compétitive de l&apos;espèce 2 par l&apos;espèce 1.

La coexistence stable des 2 espèces n&apos;est possible que si $\alpha_{12} &lt; 1$ et $\alpha_{21} &lt; 1$, c&apos;est-à-dire qu&apos;il faut que la *compétition interspécifique* soit plus faible que la *compétition intraspécifique*.

## Méthode numérique pour les EDO

Cette section est un petit peu à part du réel sujet de ce post puisque j&apos;y introduis les méthodes numériques pour résoudre les équations différentielles. En effet, il est possible de déduire de nombreuses propriétés d&apos;un système d&apos;EDO en se basant sur les théorèmes mathématiques pour la théorie des systèmes dynamiques ([méthode de Lyapunov](https://fr.wikipedia.org/wiki/Stabilit%C3%A9_de_Liapounov), [invariance de LaSalle](https://en.wikipedia.org/wiki/LaSalle%27s_invariance_principle), [théorème de Poincaré-Bendixon](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_Poincar%C3%A9-Bendixson) ...) mais seul un nombre restreint d&apos;équations différentielles admettent une solution analytique. En pratique, on préfère souvent avoir une méthode qui calcule une solution approximative du problème à tout temps $t$. On considère le problème $$y&apos;(t) = f\big(t,y(t)\big)$$ avec $y(t_0)=y_0$. L&apos;idée des méthodes numériques est de résoudre le problème sur un ensemble discret de points $(t_n,y_n)$ avec $h_n=t_{n+1}-t_n$, un pas de temps fixé.

**Euler**

![image-right](/assets/images/euler_method.png){: .align-right width=&quot;30%&quot;} La méthode d&apos;Euler est la plus basique des méthodes numériques pour EDO, elle utilise l&apos;équation différentielle pour calculer la pente de la tangente à n&apos;importe quel point de la courbe solution. La solution est approchée en partant du point initial $y_0$ connu pour lequel on calcule la tangente, on fait ensuite un pas de temps le long de cette tangente on obtient alors un nouveau point $y_1$. L&apos;idée est de répéter ce processus, pour un pas de temps de $t_n$ à $t_{n+1}$ on peut l&apos;écrire comme $y_{n+1} = y_n + h f(t_n,y_n)$.

Cette méthode est très simple à mettre en place, par exemple en python :
```python
def Euler_method(f, y0, t):
    y = np.zeros((len(t), len(y0)))
    y[0,:] = y0
    for i in range(len(t)-1):
        y[i+1] = y[i] + h*f(y[i], t[i])
    return y
```

**Runge-Kutta**

![image-right](/assets/images/rungekutta_method.png){: .align-right width=&quot;45%&quot;} Les méthodes de Runge-Kutta sont une famille de méthodes, la plus connue est celle d&apos;ordre 4. Elle est plus précise que la méthode d&apos;Euler en faisant une moyenne pondérée de 4 termes correspondant à différentes pentes de la courbe dans l&apos;intervalle de temps fixé. On la définit par :

$$ y_{n+1} = y_n + \frac{h}{6} (k_1+2k_2+2k_3+k_4) $$

avec :
 - $k_1=f(t_n,y_n)$
 - $k_2=f(t_n+h/2,y_n+hk_1/2)$
 - $k_3=f(t_n+h/2,y_n+hk_2/2)$
 - $k_4=f(t_n,y_n+hk_3)$

```python
def RungeKutta4_method(f, y0, t):
    y = np.zeros((len(t), len(y0)))
    y[0] = y0
    for i in range(len(t)-1):
        k1 = f(y[i], t[i])
        k2 = f(y[i]+k1*h/2, t[i]+h/2)
        k3 = f(y[i]+k2*h/2, t[i]+h/2)
        k4 = f(y[i]+k3*h, t[i]+h)
        y[i+1] = y[i] + (h/6) * (k1 + 2*k2 + 2*k3 + k4)
    return y
```

**Exemple**

Pour vérifier le comportement de ces méthodes, j&apos;ai choisi de les testées sur un modèle bien connu de la physique : l&apos;oscillateur harmonique. Dans sa forme la plus simple, on peut calculer exactement la solution du problème et donc comparer nos méthodes de résolution approchées. Le problème peut s&apos;écrire :

$$ 
\left\{
  \begin{array}{ccc}
    y&apos;&apos;(t) + y(t) = 0 \\
    y(0) = y_0
  \end{array}
\right.
$$

et admet pour solution $ y(t) = y_0 \cos(t) $.

```python
# initial condition
y0 = [2, 0]
# discretization
t = np.linspace(0, 5*pi, 100)
h = t[1] - t[0]
# ODE formulation
def problem(y, t):
    return np.array([y[1], -y[0]])
# analytic solution
def exact_solution(t):
    return y0[0]*np.cos(t)
y_exact = exact_solution(t)
y_euler = Euler_method(problem, y0, t)[:, 0]
y_rk4   = RungeKutta4_method(problem, y0, t)[:, 0]    
```

Comme attendu la méthode de Runge-Kutta est bien plus précise que la méthode d&apos;Euler (mais plus lente). L&apos;erreur des méthodes numériques diminue en fonction de la taille du pas de temps $h$ mais plus $h$ est petit et plus le temps de calcul est long. En théorie, pour analyser les méthodes numériques on se base sur 3 critères : 
- la convergence qui garantit que la solution approchée est proche de la solution réelle.
- l&apos;ordre qui quantifie la qualité de l&apos;approximation pour une itération. 
- la stabilité qui juge du comportement de l&apos;erreur.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/numerical_ODE.gif&quot; width=&quot;70%&quot;/&gt;
&lt;/p&gt;

**En pratique** : Dans les [`problèmes précédents`](#proie-prédateur) , j&apos;utilise la fonction `integrate.odeint(f, y0, t)` de [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html) qui est un solveur d&apos;EDO plus avancé qui utilise la méthode d&apos;[Adams–Bashforth](https://en.wikipedia.org/wiki/Linear_multistep_method#Adams–Bashforth_methods)
{: .notice--info}

---

[![Generic badge](https://img.shields.io/badge/écrit_avec-Jupyter_notebook-orange.svg?style=plastic&amp;logo=Jupyter)](https://jupyter.org/try) [![Generic badge](https://img.shields.io/badge/License-MIT-blue.svg?style=plastic)](https://lbesson.mit-license.org/) [![Generic badge](https://img.shields.io/badge/acces_au_code-github-black.svg?style=plastic&amp;logo=github)](https://github.com/julienguegan/notebooks_blog/blob/main/dynamique_population.ipynb) [![Generic badge](https://img.shields.io/badge/execute_le_code-binder-ff69b4.svg?style=plastic&amp;logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAMAAAC%2BRQ9vAAACOlBMVEX%2F%2F%2F9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olJXmsq%2FdJX1olLVa4pXmsrmZYH1olL1olJXmspXmsrmZYH1olJXmsr1olJXmspXmsr1olJXmsr1olJXmsrmZYH1olL1olL1olJXmspXmsrmZYH1olL1olL1olJXmsrmZYH1olL1olL1olJXmsrmZYHqdnT1olJXmsq6dZf1olJXmsrKk3rmZYH1olJXmsrCc5RXmsr0n1TtgWz1olJXmspXmsrmZYH1olJXmsqNhq%2Fzmlj1olJXmspZmshXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olL1olJXmsr1olJXmsrtgGz1olL1olJXmsr1olJXmsrmZYH1olJXmsrbaYf1olJXmsr1olJXmsr1olLIcJFXmsr1olJXmsr1olJXmsr1olJXmsr1olL1olJXmspZmshZmsldmsZemsVfl8Zgl8Zom71pk8Frm7tvm7dxkL1ykLx0m7R4m7F6jbh7jbh8nK6CnKmDirOEibOGnKaInKWNhq%2BNnKGSnZ2Vg6qegKaff6WfnZSnfKGnno6ofKGvnoeweZyxeZy3noG5dpjCcpPDcpPGn3bLb4%2FPoG%2FVa4rXoGnYoGjdaIbeaIXhoWHmZYHnaX7obXvpcHjqdHXreHLroVrtgGzuhGnuh2bxk17yl1vzm1j0nlX1olIgJPdZAAAAfnRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hYWFtgYGBkZnBwcHFxdHx8fn6AgICHiIuQkJCSnKCgoKavsLCwsLO4uMDAwMDBwcTFxsjO0NDQ09TW1tjY3Nzd4ODg4uLl5%2Bjo6uvr7O3v8PDw8%2FPz9vb39%2Fj5%2Bfv7%2FPz9%2Ff5K%2BfZ5AAAI4ElEQVR42uzWAWfDQBjG8Yc4qoihEApBIIoOOpaiFAUBBB3EjFDKRImZy0d7vtuYYWN36Zq4u5v7fYO%2FB%2B%2BLwENBEARBEAR32Zc0gpcWRXmS%2FO7SHPI5PDIvaip01TrypKGlXr2B6%2FKaV%2BirGA67v%2FBa9dKrCLWXGA5anvhXlYBjopI36DdwStrxNo2AO%2Fa8WZ%2FBEaLhGHs4YdFxnGME%2B5KeY7UCtq160v%2BOFUn%2FOxLyH3QkPafSwhrxzukcYcsrp7SFHSWnlcGGnEOaQ57i0ywrqo4DpIB5QlLruI7w07w4U%2BsZ5j1R420n8Ju46qmxhmkZ1WQBJVHq6gUM66hUCujEJ3e%2B3YIqMsWQLZVmMCmSVDgLDEskFR5h0m7kLRatC3NEckSFosPCHA%2FqitEdMxjzwbxZN7eRNGG8tcpr%2BS2vA3KFmZODoFLlDaOS4%2FXxleVj9OqYacLMzMzYR%2BHsZwtz5hnvSNOSf%2F97Vc%2F0NI%2B%2FBwM0q%2FQJMsjoynXfYFr%2BPxe9SgtVijdiLT3Jjrmxlu5UIf5wlLq%2BraqTD9dfqbSjFrhY1T5jLNkzMdbRUMVy6nsqgdpYx4TKbMViHXA2bm%2BOJqoEY7QlNpVEfayDKoD3eqzhBSqNpqo4R7dcyJdjDX%2BHuW7Ouq%2BhshqCiG9yTfPDV%2FgmUWCvpLbCmSMzqsC3%2BSvWcInvEOUyZEeL5mtzxUQEfI9%2FYw3%2F8X2mZsuOVUVxEUDGP%2FwQeZ%2BSM7pSocrL8cNciDXwowQeJaWhQjK6RfwIFzU%2Fe5UfIxpiI0M%2B4npTmduWcZmfIJ%2FU1yshIxtxiTI46tZuZAxhTipDQ659yPACLksG5712IMMLuUwZHHriMuxVYBlXGBD50pHKXgWWEbNJh72MtKgKnMX%2Fxjq8KmZxrALXVNb%2BIV9TBQyAFS4mrFqFO4oNxMDHIUGV%2Bo0sGwDdHxvoT5ChcmNcL2ITl2INF9hAlKlGLz6VjXwSgxoXE%2BI7JRZvu7GJwO8Y63jRaMJRpGcCnlNJXqkgg6aGX3ij7K9Vuig2NQwYkvcNe4GhlMkzZCrOfSKbgQxDhpjGhvH7RNQfWzKLPUMi%2BeUTVEd%2Fwgc4fggtifc0Alkjm6SmeEd%2FivWgikHmGCC3bQoSqKCBsZamtKbXwuaoL4rdqQxUATYcmusQJjNHuikW227kWEvBS7YXH22qjgOQvwX24iDS%2BI%2FHe%2FQqasBtk4KveNoCXcDB%2B6NIC2IMsEc3%2FBl4o%2B7RIFZN5eETAw0T0%2FA74YOEAVW4aDU81pKx%2Bo%2BNpvp7BQ38UPdijKgXKQpxWfdZjCiOJhpluFXp6TFkolg5FXlgooFpafAiWFiNLsaQopMSvWAzwpweG5g7je9y5sgtztw5EUoPbRF%2FUOyhCw2LbMw1PrJnx9qV6gEr1%2B48MAf%2FDfZvJ66RJ0T3GHJi21KlZ%2Fn2U%2FhK1crNQ%2FoTZEKs5dia%2BcrEos2n5GpCFO0zdrv589sWqrZZtPu83FOREKaspO5xeo1KyPz156S2yDZxSldrn16tbHhUSFNaQAZ0Dezm5zcoS%2BZvPw8zRulkEzQJuIPbP1%2FZs%2BjYg85RVIZHiXScX6FKY%2FN5tyqADDJyr847tECVysITcdxUS5WTgf18iyqHvRbeLSgj9ZYqj%2BepHcjo8Lkql5dTVZfR4RtVPp%2Bn5GXIq8A6xPMGUFF9HR5r6Gb27i%2BVK94mV6BGHPOuskY%2BXhVA1wSZp1wyjtyQt%2FTxkcotncgJOTvnSP2o2mDxxp2Hjxxn5uNHDu%2FcuFi1wXdu3Ly%2F3W5%2BijKycs9xfpTjO5YoI6%2BSC3y2qXH7mQPoD6yhd6M5tA0iF0Ro1Kch1aowH%2Fbqz8DRRpiE%2FJwSmykUSEuj4Y4PIwrxsKjxVwWZIeUcwBx1CjIv1cY0uKZZIT4mB2SSP%2ByarQC%2FD4NjVPbbNuWzAiMePB3pogA%2FdnpkcIeu59MK0JoSeXcL6kNkjG866EKe5jg6%2FSpoDi%2Fhe8E6qMK0w8xQAh3Ngg9G8snC1O%2F%2Ft%2FjICKWnn0DPoc%2FlKaWnh0kF9092FrMln4wECRL4OBC1Uf55U2mpEUgdWh2vGI4xSP7gMKV3j%2FESTYfm3XwNPkUv4MTGQGG3WfbVZ%2BFe9hoMI6UfWr3%2BBHG7RsA7NMXEFJS3Rtk8msRZdLCbigRTuH2mrXpjZMF9BBkUm2OKuxUgFgKOsG%2BeDQQ2TUurw%2BUZFvLcKvU4y3Z9xRj4RABZtk6gC9Rw8uDWdeoeq7buO8lmDA39eIFEDipEwNFbnOUE5AjSBQU9qTawdEIy0CpVj%2BAa1R6zY6BY9Qo5IhO5U%2BGTiWeVBnKF70yHT0a6CsgQ0NGfMNDH6yR1CKgAvUsXalc6oiy1ibQM8kMx7xaQgfHyXA6hRy5lCJSJVrm7%2BjJw9Y2x%2B6%2F3morIIC%2FHpTDVo2R0Een%2FNGTtPb2gi1AWHQeJ0N%2FuZkVDKDnjgYxqC4lGeWTBbJEKFwvJcxLC%2FmRFCjTjcmRyBTYT5XyypCtom0TxR4XYDrksWYEHuV1JHC878%2BjJx3vzo7te86gUUq2Vibdg7bdq3aZdd9i0blUZP90PTj%2Fl0Z5gI5VCM%2FyUPI3OJq%2F9xBY1Jf94oytjCLkGiPUO6rlnlY5XSBjzo5fmlH2ssB%2Boi98q22uVekVpSVGlaLVfouJIIV%2BJWJWlloOZwcrCxWSoUXputGuHuLKEQBSGDwaDQmAxrVFtyuDaswB2UIs4a395ueKKCcyd7g4wSX%2B%2BxJ8cWequDpMVA8nVjsiGiIEsGzReWiUrhrr0SmQOtkQMZZUtxaIvdG4xWGJbMmizmW0eo1W2aTPECjsEw3n2qDi8Cpk9ajDezr66B4NfNoqyL2CGwrf0kPRfPpRv7ZjCKe9UMEngjdRilo23UYd5hHeJmEkGVIwgwyrW74iYL%2FEi9VhBVF5RHdbgKs%2FLBqswmWdtWElQnlEc1mKEH9MN63EHPyMGS%2FKfhIjFsnzmn6hYLM2myndKNFif2yvbymbxLWyUwlfHHgy%2BjfMp5eOHpOQtHo%2FH4%2FEY7x8MZ7AAyatDDgAAAABJRU5ErkJggg%3D%3D)](https://hub.gke2.mybinder.org/user/julienguegan-notebooks_blog-z8qd9bd5/notebooks/dynamique_population.ipynb)</content><author><name>Julien Guégan</name></author><category term="blog" /><category term="équations différentielles ordinaires" /><category term="écologie" /><category term="modélisation" /><category term="équation logistique" /><summary type="html">Parmis les enjeux du 21ème siècle, l’écologie a un rôle majeure puisqu’elle est la science qui étudie les interactions des êtres vivants entre eux et avec leur milieu. Pour modéliser ces interactions, la dynamique des populations est la branche qui s’intéresse aux fluctuations démographiques des espèces. Ses applications sont nombreuses puisqu’elle peut permettre de répondre à des problèmes variés comme la gestion d’espèces menacées, la protection des cultures contre des nuisibles, le contrôle de bioréacteurs ou la prédiction des épidémies. Modèle de Verhulst À la fin du 18ème siècle, le modèle de Malthus décrit la variation d’une taille de population $y$ au cours du temps $t$ par l’équation différentielle ordinaire1 (EDO) : \[y&apos;(t) = (n-m) y(t) = r y(t)\] avec les constantes : $n$ le taux de natalité, $m$ le taux de mortalité et $r$ le taux de croissance. Ce modèle nous dit que, selon le taux de croissance $r$, la taille des populations peut soit diminuer, rester constante ou augmenter de manière exponentielle. Ce modèle ne reflète pas la réalité puisque une population n’augmentera jamais à l’infini. En 1840, Verlhust propose un modèle de croissance plus adapté en partant de l’hypothèse que le taux de croissance $r$ n’est pas une constante mais est fonction affine de la taille de population $y$ : \[y&apos;(t) = \big(n(y) - m(y)\big) y(t)\] Verlhust part notamment de l’hypothèse que plus la taille d’une population augmente alors plus son taux de natalité $n$ diminue et plus son taux de mortalité $m$ augmente. En partant de cette hypothèse et en appliquant quelques manipulations algébriques astucieuses, on peut montrer que l’équation différentielle précédente peut se réécrire sous la forme : \[y&apos;(t) = r y(t) \left(1 - \frac{y(t)}{K}\right)\] avec $K$ une constante appelée capacité d’accueil. On peut résoudre analytiquement cette équation avec la condition initiale $y(t=0)=y_0$, on obtient la solution logistique : \[y(t) = \frac{K}{1+\left(\frac{K}{y_0}-1\right)e^{-rt}}\] Résolution détaillée de l&apos;équation différentielle logistique par séparation de variable $$ \begin{align*} \int_{y_0}^{y(t)} \frac{1}{y(1-y/K)}dy &amp;amp;= \int_0^t r \ d\tau \\ \int_{y_0}^{y(t)} \frac{K}{y(K-y)}dy &amp;amp;= \int_0^t r \ d\tau \\ \int_{y_0}^{y(t)} \frac{1}{y}dy + \int_{y_0}^{y(t)} \frac{1}{K-1}dy &amp;amp;= \int_0^t r \ d\tau \\ \ln \left| \frac{y(t)}{y_0} \right| - \ln \left| \frac{K-y(t)}{K-y_0} \right| &amp;amp;= r \ t \\ \ln \left( \frac{y(t)\big(K-y_0\big)}{y_0\big(K-y(t)\big)} \right) &amp;amp;= r \ t \\ \frac{y(t)}{K-y(t)} &amp;amp;= \frac{y_0}{K-y_0}e^{rt} \\ y(t)\left(1+\frac{y_0}{K-y_0}e^{rt} \right) &amp;amp;= \frac{K y_0 e^{rt}}{K-y_0} \\ y(t) &amp;amp;= \frac{Ky_0e^{rt}}{K-y_0+y_0e^{rt}} \\ y(t) &amp;amp;= \frac{K y_0}{(K-y_0)e^{-rt}+y_0} \\ \end{align*} \\ \square $$ On remarque que $ \lim\limits_{t\to\infty} y(t) = K $. Ce qui signifie que peut importe la taille de la population initiale $y_0$, la population finira toujours par tendre vers $K$ la capacité d’accueil qu’on qualifie souvent comme le nombre d’individus maximal que le milieu peut accueillir (selon l’espace, les ressources …). Cette fonction dite logistique introduite pour la première fois par Verlhust pour modéliser la croissance des populations trouvera par la suite plein d’application dans des domaines variés comme l’économie, la chimie, les statistiques et plus récemment les réseaux de neurones artificielles. Modèle de Lotka-Volterra Les modèles de Lotka-Volterra sont des sytèmes d’équations simples qui sont apparus au début du 20ème siècle. Ils portent le nom de deux mathématiciens qui ont publié en même temps mais indépendamment sur le sujet : Volterra, en 1926, pour modéliser les populations de sardines et de leurs prédateurs et Lotka, en 1924, dans son livre Elements of Physical Biology. Contrairement au modèle de Verlhust qui s’intéresse à une seule population, les modèles de Lotka-Volterra modélisent les interactions entre plusieurs espèces, chacune ayant un impact sur le développement de l’autres. Proie-prédateur Le modèle proie-prédateur de Lotka-Volterra a permis d’expliquer des données collectées de certaines populations d’animaux comme le lynx et lièvre ainsi que le loup et l’élan aux Etats-Unis. On y représente l’évolution du nombre proies $x$ et de prédateurs $y$ au cours du temps $t$ selon le modèle suivant : \[\left\{ \begin{array}{ccc} x&apos;(t) = x(t)\ \big(\alpha - \beta y(t)\big) \\ y&apos;(t) = y(t)\ \big( \delta x(t) - \gamma\big) \end{array} \right.\] avec les paramètres $\alpha$ et $\delta$ sont les taux de reproduction respectivement des proies et des prédateurs et $\beta$ et $\gamma$ sont les taux de mortalité, respectivement, des proies et des prédateurs. Note: On parle de système autonome : le temps $t$ n’apparaît pas explicitement dans les équations. Si on développe chacune des équations, on peut plus facilement donner une interprétation. Pour les proies, on a d’une part le terme $\alpha x(t)$ qui modélise la croissance exponentielle avec une source illimitée de nourriture et d’autre part $- \beta x(t) y(t)$ qui représente la prédation proportionnelle à la fréquence de rencontre entre prédateurs et proies. L’équation des prédateurs est très semblable à celle des proies, $\delta x(t)y(t)$ est la croissance des prédateurs proportionnelle à la quantité de nourriture disponible (les proies) et $- \gamma y(t)$ représente la mort naturelle des prédateurs. On peut caculer les équilibres de ce système d’équations différentielles et également en déduire un comportement mais les solutions n’ont pas d’expression analytique simple. Néanmoins, il est possible de calculer une solution approchée numériquement (plus de détails dans la section suivante). # define ODE function to resolve r, c, m, b = 3, 4, 1, 2 def prey_predator(XY, t=0): dX = r*XY[0] - c*XY[0]*XY[1] dY = b*XY[0]*XY[1] - m*XY[1] return [dX, dY] # discretization T0 = 0 Tmax = 12 n = 200 T = np.linspace(T0, Tmax, n) On calcule ici l’évolution des 2 populations en fonction du temps pour une condition initiale fixée, on voit qu’elles ont un comportement périodique et en décalage de phase. # TEMPORAL DYNAMIC X0 = [1,1] solution = integrate.odeint(prey_predator, X0, T) # use scipy solver Ici, on calcule plusieurs solutions pour différentes conditions initiales qu’on affiche dans l’espace de phase (le temps n’appararaît pas). On peut également afficher le champ de vecteur généré par le système d’équation avec plt.quiver() pour une grille de valeur. # PHASES SPACE # some trajectories orbits = [] for i in range(5): X0 = [0.2+i*0.1, 0.2+i*0.1] orbit = integrate.odeint(prey_predator, X0, T) orbits.append(orbit) # vector field x, y = np.linspace(0, 2.5, 20), np.linspace(0, 2, 20) X_grid, Y_grid = np.meshgrid(x, y) DX_grid, DY_grid = prey_predator([X_grid, Y_grid]) N = np.sqrt(DX_grid ** 2 + DY_grid ** 2) N[N==0] = 1 DX_grid, DY_grid = DX_grid/N, DY_grid/N Attention: Les unités des simulations ne reflète pas la réalité, il faut des populations suffisamment grandes pour que la modélisation soit correcte. Dans le modèle utilisé, les prédateurs prospèrent lorsque les proies sont nombreuses, mais finissent par épuiser leurs ressources et déclinent. Lorsque la population de prédateurs a suffisamment diminué, les proies profitant du répit se reproduisent et leur population augmente de nouveau. Cette dynamique se poursuit en un cycle de croissance et déclin. Il existe 2 équilibres : le point $(0,0)$ est un point de selle instable qui montre que l’extinction des 2 espèce est en fait quasiment impossible à obtenir et le point $(\frac{\gamma}{\delta}, \frac{\alpha}{\beta})$ est un centre stable, les populations oscillent autour cet état. Note: Cette modélisation reste assez simple, un grande nombre de variante existe. On peut rajouter des termes de disparition des 2 espèces (dus à la pêche, chasse, pesticide …), tenir compte de la capacité d’accueil du milieu en utilisant un terme logistique. Compétition Le modèle de compétition de Lotka-Volterra est une variante du modèle de prédation où les 2 espèces n’ont pas une hierarchie de proies et prédateurs mais sont en compétition l’une et l’autre. De plus, la dynamique de base n’est plus une simple croissance exponentielle mais logistique (avec les paramètres $r_i$ et $K_i$) : \[\left\{ \begin{array}{ccc} x_1&apos;(t) = r_1x_1(t)\left(1- \frac{x_1(t)+\alpha_{12}x_2(t)}{K_1}\right) \\ x_2&apos;(t) = r_2x_2(t)\left(1- \frac{x_2(t)+\alpha_{21}x_1(t)}{K_2}\right) \end{array} \right.\] avec $\alpha_{12}$ l’effet de l’espèce 2 sur la population de l’espèce 1 et réciproquement $\alpha_{21}$ l’effet de l’espèce 2 sur l’espèce 1. Par exemple, pour l’équation de l’espèce 1, le coefficient $\alpha_{12}$ est multiplié par la taille de la population $x_2$. Quand $\alpha_{12} &amp;lt; 1$ alors l’effet de l’espèce 2 sur l’espèce 1 est plus petit que l’effet de l’espèce 1 sur ces propres membres. Et inversement, quand $\alpha_{12} &amp;gt; 1$, l’effet de l’espèce 2 sur l’espèce 1 est supérieur à l’effet de l’espèce 1 sur ces propres membres. Pour comprendre plus en détails les prédictions du modèles, il est utile de tracer comme précédemment les diagrammes d’espace de phase $(x_1,x_2)$. On peut distinguer 4 scénarios selon les valeurs des coefficients de compétition, j’affiche ci-dessous les champs de vecteurs de ces scénarios avec plt.streamplot() ainsi que les isoclines, les courbes pour lesquelles \(x_1&apos;(t)=0\) ou \(x_2&apos;(t)=0\): # define ODE to resolve r1, K1 = 3, 1 r2, K2 = 3, 1 def competition(X1X2, a1, a2): dX1 = r1*X1X2[0] * (1-(X1X2[0]+a1*X1X2[1])/K1) dX2 = r2*X1X2[1] * (1-(X1X2[1]+a2*X1X2[0])/K2) return [dX1, dX2] # compute derivatives for each scenario N = 20 x, y = np.linspace(0, 2.5, N), np.linspace(0, 2, N) X_grid, Y_grid = np.meshgrid(x, y) DX_grid, DY_grid = np.zeros((4,N,N)), np.zeros((4,N,N)) coeffs = np.array([[1.5,1.5],[0.5,0.5],[1.5,0.5],[0.5,1.5]]) for k,(a1,a2) in enumerate(coeffs): DX_grid[k,:], DY_grid[k,:] = competition([X_grid, Y_grid], a1, a2) Au final, les 4 comportements possibles en fonction de $\alpha_{12}$ et $\alpha_{21}$ sont les suivants : Exclusion compétitive d’une des deux espèces en fonction des conditions initiales. Coexistence stable des deux espèces. Exclusion compétitive de l’espèce 1 par l’espèce 2. Exclusion compétitive de l’espèce 2 par l’espèce 1. La coexistence stable des 2 espèces n’est possible que si $\alpha_{12} &amp;lt; 1$ et $\alpha_{21} &amp;lt; 1$, c’est-à-dire qu’il faut que la compétition interspécifique soit plus faible que la compétition intraspécifique. Méthode numérique pour les EDO Cette section est un petit peu à part du réel sujet de ce post puisque j’y introduis les méthodes numériques pour résoudre les équations différentielles. En effet, il est possible de déduire de nombreuses propriétés d’un système d’EDO en se basant sur les théorèmes mathématiques pour la théorie des systèmes dynamiques (méthode de Lyapunov, invariance de LaSalle, théorème de Poincaré-Bendixon …) mais seul un nombre restreint d’équations différentielles admettent une solution analytique. En pratique, on préfère souvent avoir une méthode qui calcule une solution approximative du problème à tout temps $t$. On considère le problème \(y&apos;(t) = f\big(t,y(t)\big)\) avec $y(t_0)=y_0$. L’idée des méthodes numériques est de résoudre le problème sur un ensemble discret de points $(t_n,y_n)$ avec $h_n=t_{n+1}-t_n$, un pas de temps fixé. Euler La méthode d’Euler est la plus basique des méthodes numériques pour EDO, elle utilise l’équation différentielle pour calculer la pente de la tangente à n’importe quel point de la courbe solution. La solution est approchée en partant du point initial $y_0$ connu pour lequel on calcule la tangente, on fait ensuite un pas de temps le long de cette tangente on obtient alors un nouveau point $y_1$. L’idée est de répéter ce processus, pour un pas de temps de $t_n$ à $t_{n+1}$ on peut l’écrire comme $y_{n+1} = y_n + h f(t_n,y_n)$. Cette méthode est très simple à mettre en place, par exemple en python : def Euler_method(f, y0, t): y = np.zeros((len(t), len(y0))) y[0,:] = y0 for i in range(len(t)-1): y[i+1] = y[i] + h*f(y[i], t[i]) return y Runge-Kutta Les méthodes de Runge-Kutta sont une famille de méthodes, la plus connue est celle d’ordre 4. Elle est plus précise que la méthode d’Euler en faisant une moyenne pondérée de 4 termes correspondant à différentes pentes de la courbe dans l’intervalle de temps fixé. On la définit par : \[y_{n+1} = y_n + \frac{h}{6} (k_1+2k_2+2k_3+k_4)\] avec : $k_1=f(t_n,y_n)$ $k_2=f(t_n+h/2,y_n+hk_1/2)$ $k_3=f(t_n+h/2,y_n+hk_2/2)$ $k_4=f(t_n,y_n+hk_3)$ def RungeKutta4_method(f, y0, t): y = np.zeros((len(t), len(y0))) y[0] = y0 for i in range(len(t)-1): k1 = f(y[i], t[i]) k2 = f(y[i]+k1*h/2, t[i]+h/2) k3 = f(y[i]+k2*h/2, t[i]+h/2) k4 = f(y[i]+k3*h, t[i]+h) y[i+1] = y[i] + (h/6) * (k1 + 2*k2 + 2*k3 + k4) return y Exemple Pour vérifier le comportement de ces méthodes, j’ai choisi de les testées sur un modèle bien connu de la physique : l’oscillateur harmonique. Dans sa forme la plus simple, on peut calculer exactement la solution du problème et donc comparer nos méthodes de résolution approchées. Le problème peut s’écrire : \[\left\{ \begin{array}{ccc} y&apos;&apos;(t) + y(t) = 0 \\ y(0) = y_0 \end{array} \right.\] et admet pour solution $ y(t) = y_0 \cos(t) $. # initial condition y0 = [2, 0] # discretization t = np.linspace(0, 5*pi, 100) h = t[1] - t[0] # ODE formulation def problem(y, t): return np.array([y[1], -y[0]]) # analytic solution def exact_solution(t): return y0[0]*np.cos(t) y_exact = exact_solution(t) y_euler = Euler_method(problem, y0, t)[:, 0] y_rk4 = RungeKutta4_method(problem, y0, t)[:, 0] Comme attendu la méthode de Runge-Kutta est bien plus précise que la méthode d’Euler (mais plus lente). L’erreur des méthodes numériques diminue en fonction de la taille du pas de temps $h$ mais plus $h$ est petit et plus le temps de calcul est long. En théorie, pour analyser les méthodes numériques on se base sur 3 critères : la convergence qui garantit que la solution approchée est proche de la solution réelle. l’ordre qui quantifie la qualité de l’approximation pour une itération. la stabilité qui juge du comportement de l’erreur. En pratique : Dans les problèmes précédents , j’utilise la fonction integrate.odeint(f, y0, t) de scipy qui est un solveur d’EDO plus avancé qui utilise la méthode d’Adams–Bashforth Le terme ordinaire est utilisé par opposition au terme équation différentielle partielle (ou équation aux dérivées partielles) où la ou les fonctions inconnues peuvent dépendre de plusieurs variables. &amp;#8617;</summary></entry><entry xml:lang="fr"><title type="html">Objet Fractal : Dimension, Auto-similarité, Infini</title><link href="http://localhost:4000/posts/fr/2021-08-02-objet_fractal/" rel="alternate" type="text/html" title="Objet Fractal : Dimension, Auto-similarité, Infini" /><published>2021-08-02T21:34:30+02:00</published><updated>2021-08-02T21:34:30+02:00</updated><id>http://localhost:4000/posts/fr/objet_fractal</id><content type="html" xml:base="http://localhost:4000/posts/fr/2021-08-02-objet_fractal/">&gt; *Elles sont présentes dans les forêts tropicales, à la pointe de la recherche médicale, dans les films et partout où reigne la communication sans fil. Ce mystère de la nature a enfin été percé à jour. &quot;Bon sang ! Mais c&apos;est bien sûr !&quot;. Peut-être n&apos;avez vous jamais entendu parler de ces formes étranges, pourtant elles sont partout autour de vous. Leur nom : les fractales.*

&lt;cite&gt; reportage ARTE &lt;/cite&gt; -- à la recherche de la dimension cachée
{: .small}

{% include video id=&quot;Tpsu2uz9rCE&quot; provider=&quot;youtube&quot; %}

## Introduction

Comme vous l&apos;aurez compris si vous avez regardé l&apos;excellent documentaire d&apos;ARTE présenté ci-dessus, les fractales sont des objets géométriques *infiniment morcelés* qui ont la particularité de présenter des structures similaires à toutes les échelles. Ce type de géométrie permet de modéliser avec de simples formules récursives des figures infiniment complexes mais aussi de décrire des phénomènes naturels (motifs des flocons,  chemin pris par la foudre, forme d&apos;un choux de romanesco, structure des poumons ...) et de trouver des applications dans des domaines technologiques (antennes, transistors, génération graphique de paysages ...).

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/fractals_in_nature.png&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

**Note:** Les fractales qu&apos;on trouve dans la nature sont des approximations finies des vrais objets mathématiques.
{: .notice--primary}

Les fractales sont notamment caractérisées par la notion contre intuitive de **dimension non entière**. En effet, on peut définir une régle générale de mise à l&apos;échelle qui relie la mesure $N$, un facteur d&apos;échelle $\varepsilon$ et la dimension $D$ :

$$ N = \varepsilon^{-D} $$

Par exemple, pour une figure géométrique usuelle comme le carré, sa dimension est $D=2$ et si on le subdivise en $3$ son aire est $N=9$, on a bien $9=\frac{1}{3}^{-2}$. On peut appliquer ce même raisonnement pour un cube ou même une ligne.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/scaling_rule.png&quot; width=&quot;60%&quot;/&gt;
&lt;/p&gt;

Maintenant, on cherche à trouver la dimension d&apos;une figure fractale simple. La formule précédente nous donne :

$$ D = -\frac{\log N}{\log \varepsilon} $$

Si on s&apos;intéresse à une figure telle que la courbe de Von Koch qui consiste, à partir d&apos;un segment, construire récursivement des triangles équilatéraux sur chaque sous-segment (cf animation ci-dessous). En comptant les segments à chaque nouvelle mise à l&apos;échelle, on comprends que la longueur de la courbe de Koch est multipliée par $4$ pour chaque mise à l&apos;échelle $\varepsilon=\frac{1}{3}$ (on divise les segments par 3). On trouve donc que sa dimension est $D = \frac{\log 4}{\log 3} \approx 1.26$. Il ne s&apos;agit pas d&apos;une simple courbe unidimensionelle, ni d&apos;une surface mais quelque chose &quot;entre les deux&quot;.
{: .text-justify}

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/von_koch.gif&quot; width=&quot;60%&quot;/&gt;
&lt;/p&gt;

**Note:** L&apos;approche présentée précédemment est conceptuelle. Une définition rigoureuse et définie pour tout ensemble est la [dimension de Hausdorff](https://fr.wikipedia.org/wiki/Dimension_de_Hausdorff). Elle est peu aisée à mettre en oeuvre...
{: .notice--primary}

On peut différencier 3 catégories principales de fractale :

- les systèmes de **fonctions itérées**. Ils ont une règle géométrique fixe comme le flocon de Von Koch, le tapis de Sierpinski, la courbe de Peano.
- les fractales **aléatoires**. Elles sont générées par un processus stochastiques comme dans la nature ou les paysages fractales.
- les ensembles définies par une **relation de récurrence** en chaque point d&apos;un espace. On peut citer l&apos;ensemble de Julia, de mandelbrot, de lyapunov. On les appelle parfois en anglais des *Escape-time fractals*.

## Ensemble de Julia

L&apos;ensemble de Julia associé à un nombre complexe $c$ fixé est l&apos;ensemble des valeurs initiales $z_0$ pour lesquelles la suite suivante est bornée :

$$
\left\{
  \begin{array}{ll}
    z_0 \in \mathbb{C} \\
    z_{n+1} = z_n^2 + c
  \end{array}
\right.
$$

Pour générer un ensemble de Julia informatiquement, l&apos;idée est de discrétiser l&apos;espace dans un intervalle fixé pour avoir un nombre fini de valeur $z_0$ pour lesquelle on va tester la convergence de la suite.

```python
# INITIALIZATION
# value of c fixed
c_reel, c_imag = 0.3, 0.5 
# interval limit
x_min, x_max = -1, 1
y_min, y_max = -1, 1
# discretization
size = 5000 
x_step = (x_max - x_min)/size
y_step = (y_max - y_min)/size
M = np.zeros((size,size))
```

Pour pouvoir travailler avec des nombres complexes, j&apos;ai choisi de décomposer la partie réelle `z_reel` et la partie imaginaire `z_image`. Ensuite, on teste la convergence pour un point donné en regardant si on a pas dépassé un nombre fini d&apos;itération `n_iter &lt; itermax`. On peut également, en plus, vérifier que la suite $(z_n)$ est divergente si son module est strictement supérieur à $2$, `z_reel**2 + z_imag**2 &lt; 4` (cf [demonstration](https://fr.wikipedia.org/wiki/Ensemble_de_Mandelbrot#Barri%C3%A8re_du_module_%C3%A9gal_%C3%A0_2)). Finalement, on peut remplir une matrice `M` de $0$ ou de $1$ selon le test de convergence. Mais, pour un rendu visuelle final plus estéthique on peut également remplir la matrice `M` selon le taux de convergence estimé avec `n_iter/itermax`.
{: .text-justify}

```python
# LOOP ON ALL PIXEL = COMPLEX PLANE
for i in (range(size)):
    for j in range(size):
        n_iter = 0
        # convert pixel position to a complex number
        z_reel = i * x_step + x_min
        z_imag = j * y_step + y_min
        # update sequence until convergence
        while (z_reel**2 + z_imag**2 &lt; 4) and (n_iter &lt; itermax):
            z_reel, z_imag = z_reel**2 - z_imag**2 + c_reel, 2*z_imag*z_reel + c_imag
            n_iter = n_iter + 1
        # color image according to convergence rate
        M[j,i] = 255*n_iter/itermax
```

**Astuce:** En python, on aurait pu directement utiliser la fonction `complex()` pour avoir un objet complexe. Dans ce cas, les variables `z_reel` et `z_imag` seraient inutiles et on pourrait directement récupérer la valeur absolue et mettre au carré une unique variable complexe `z`.
{: .notice--info}

Finalement, on peut générer des ensembles de Julia pour différentes valeurs de $c$ fixées et pour changer le visuel on peut s&apos;amuser à tester différentes *colormap*. Ci-dessous quelques résultats que j&apos;ai généré.

{% include gallery %}

On remarque que les figures obtenues varient grandement en fonction de la valeur du complexe $c$ choisie. En fait, on peut générer les ensembles de julia pour une suite de complexes consécutifs pour voir comment les figures évoluent et en faire une animation.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/fractal_julia.gif&quot; width=&quot;40%&quot;/&gt;
&lt;/p&gt;

## Ensemble de Mandelbrot

L&apos;ensemble de Mandelbrot est fortement lié aux ensembles de Julia, en effet on peut définir l&apos;ensemble de Mandelbrot $M$ comme l&apos;ensemble des complexes $c$ pour lesquels l&apos;ensemble de Julia $J_c$ correspondant est **connexe**, c&apos;est-à-dire qu&apos;il est fait d&apos;un seul morceau. On peut dire que l&apos;ensemble de Mandelbrot représente une carte des ensembles de Julia. Et, contrairement au nom qu&apos;il porte, c&apos;est les mathématiciens Julia et Fatou qui l&apos;ont découvert et qui ont montré que la définition précédente est équivalente à l&apos;ensemble des points $c$ du plan complexe $\mathbb{C}$ pour lesquels la suite suivante est bornée :
{: .text-justify}

$$
\left\{
  \begin{array}{ll}
    z_0 = 0 \\
    z_{n+1} = z_n^2 + c
  \end{array}
\right.
$$

Cette définition est très similaire à celle de l&apos;ensemble de Julia à la différence qu&apos;on s&apos;intéresse à la variable $c$. Dans le code précédent, il faudrait modifier la ligne `z_reel = i * x_step + x_min` par `c_reel = i * x_step + x_min` et fixé `z_reel = 0` (idem pour la partie imaginaire). On obtient la figure suivante :

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/mandelbrot.png&quot; width=&quot;40%&quot;/&gt;
&lt;/p&gt;

**Note:** Benoît Mandelbrot est le fondateur de la théorie fractale avec notamment son article _&quot;How Long Is the Coast of Britain ? Statistical Self-Similarity and Fractional Dimension&quot;_ en 1967. C&apos;est également lui qui obtient pour la première fois, une visualisation par ordinateur de cet ensemble.
{: .notice--primary}

## Logiciels

La génération de fractale n&apos;est pas une tâche facile : beaucoup de paramètres peuvent être à prendre en compte et les temps de calcul sont souvent long. Dans les figures que j&apos;ai généré, on ne voit pas au premiers abords le caractère auto-similaire des fractales, il faudrait changer d&apos;échelle en zoomant de plus en plus profond sur un point du plan.
{: .text-justify}

Il existe de nombreux logiciels générateur de fractal gratuits disponibles. Ils sont souvent optimisés pour faire du multi-processing ou du calcul sur GPU, possèdent une interface graphique pour gérer les nombreux paramètres et sont parfois capables de créer des objets 3D (comme les 3 affichés ci-dessous). Une liste assez complète est disponible [ici](https://en.wikipedia.org/wiki/Fractal-generating_software#Programs).
{: .text-justify}

| ![image](/assets/images/mandelbulb3d.png)     | ![image](/assets/images/mandelbulber.png) | ![image](/assets/images/fragmentarium.png) |
|:---------------------------------------------:| :----------------------------------------:| :-----------------------------------------: |
| [Mandelbul3D](https://www.mandelbulb.com/2014/mandelbulb-3d-mb3d-fractal-rendering-software/)| [Mandelbuler](https://mandelbulber.com/) |  [Fragmentarium](https://syntopia.github.io/Fragmentarium/get.html) |

Et pour ceux qui ne veulent pas se compliquer et juste se laisser porter par la géométrie psychadélique des fractales sans effort, vous pourrez trouver sur internet des gens qui ont déjà fait le travail à votre place. On trouve sur youtube une floppée de vidéos comme par exemple *The Hardest Trip - Mandelbrot Fractal Zoom* qui zoome pendant 2h30 sur un point précis du plan complexe.

{% include video id=&quot;LhOSM6uCWxk&quot; provider=&quot;youtube&quot; %}

---

[![Generic badge](https://img.shields.io/badge/écrit_avec-Jupyter_notebook-orange.svg?style=plastic&amp;logo=Jupyter)](https://jupyter.org/try) [![Generic badge](https://img.shields.io/badge/License-MIT-blue.svg?style=plastic)](https://lbesson.mit-license.org/) [![Generic badge](https://img.shields.io/badge/acces_au_code-github-black.svg?style=plastic&amp;logo=github)](https://github.com/julienguegan/notebooks_blog/blob/main/fractale.ipynb) [![Generic badge](https://img.shields.io/badge/execute_le_code-binder-ff69b4.svg?style=plastic&amp;logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAMAAAC%2BRQ9vAAACOlBMVEX%2F%2F%2F9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olJXmsq%2FdJX1olLVa4pXmsrmZYH1olL1olJXmspXmsrmZYH1olJXmsr1olJXmspXmsr1olJXmsr1olJXmsrmZYH1olL1olL1olJXmspXmsrmZYH1olL1olL1olJXmsrmZYH1olL1olL1olJXmsrmZYHqdnT1olJXmsq6dZf1olJXmsrKk3rmZYH1olJXmsrCc5RXmsr0n1TtgWz1olJXmspXmsrmZYH1olJXmsqNhq%2Fzmlj1olJXmspZmshXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olL1olJXmsr1olJXmsrtgGz1olL1olJXmsr1olJXmsrmZYH1olJXmsrbaYf1olJXmsr1olJXmsr1olLIcJFXmsr1olJXmsr1olJXmsr1olJXmsr1olL1olJXmspZmshZmsldmsZemsVfl8Zgl8Zom71pk8Frm7tvm7dxkL1ykLx0m7R4m7F6jbh7jbh8nK6CnKmDirOEibOGnKaInKWNhq%2BNnKGSnZ2Vg6qegKaff6WfnZSnfKGnno6ofKGvnoeweZyxeZy3noG5dpjCcpPDcpPGn3bLb4%2FPoG%2FVa4rXoGnYoGjdaIbeaIXhoWHmZYHnaX7obXvpcHjqdHXreHLroVrtgGzuhGnuh2bxk17yl1vzm1j0nlX1olIgJPdZAAAAfnRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hYWFtgYGBkZnBwcHFxdHx8fn6AgICHiIuQkJCSnKCgoKavsLCwsLO4uMDAwMDBwcTFxsjO0NDQ09TW1tjY3Nzd4ODg4uLl5%2Bjo6uvr7O3v8PDw8%2FPz9vb39%2Fj5%2Bfv7%2FPz9%2Ff5K%2BfZ5AAAI4ElEQVR42uzWAWfDQBjG8Yc4qoihEApBIIoOOpaiFAUBBB3EjFDKRImZy0d7vtuYYWN36Zq4u5v7fYO%2FB%2B%2BLwENBEARBEAR32Zc0gpcWRXmS%2FO7SHPI5PDIvaip01TrypKGlXr2B6%2FKaV%2BirGA67v%2FBa9dKrCLWXGA5anvhXlYBjopI36DdwStrxNo2AO%2Fa8WZ%2FBEaLhGHs4YdFxnGME%2B5KeY7UCtq160v%2BOFUn%2FOxLyH3QkPafSwhrxzukcYcsrp7SFHSWnlcGGnEOaQ57i0ywrqo4DpIB5QlLruI7w07w4U%2BsZ5j1R420n8Ju46qmxhmkZ1WQBJVHq6gUM66hUCujEJ3e%2B3YIqMsWQLZVmMCmSVDgLDEskFR5h0m7kLRatC3NEckSFosPCHA%2FqitEdMxjzwbxZN7eRNGG8tcpr%2BS2vA3KFmZODoFLlDaOS4%2FXxleVj9OqYacLMzMzYR%2BHsZwtz5hnvSNOSf%2F97Vc%2F0NI%2B%2FBwM0q%2FQJMsjoynXfYFr%2BPxe9SgtVijdiLT3Jjrmxlu5UIf5wlLq%2BraqTD9dfqbSjFrhY1T5jLNkzMdbRUMVy6nsqgdpYx4TKbMViHXA2bm%2BOJqoEY7QlNpVEfayDKoD3eqzhBSqNpqo4R7dcyJdjDX%2BHuW7Ouq%2BhshqCiG9yTfPDV%2FgmUWCvpLbCmSMzqsC3%2BSvWcInvEOUyZEeL5mtzxUQEfI9%2FYw3%2F8X2mZsuOVUVxEUDGP%2FwQeZ%2BSM7pSocrL8cNciDXwowQeJaWhQjK6RfwIFzU%2Fe5UfIxpiI0M%2B4npTmduWcZmfIJ%2FU1yshIxtxiTI46tZuZAxhTipDQ659yPACLksG5712IMMLuUwZHHriMuxVYBlXGBD50pHKXgWWEbNJh72MtKgKnMX%2Fxjq8KmZxrALXVNb%2BIV9TBQyAFS4mrFqFO4oNxMDHIUGV%2Bo0sGwDdHxvoT5ChcmNcL2ITl2INF9hAlKlGLz6VjXwSgxoXE%2BI7JRZvu7GJwO8Y63jRaMJRpGcCnlNJXqkgg6aGX3ij7K9Vuig2NQwYkvcNe4GhlMkzZCrOfSKbgQxDhpjGhvH7RNQfWzKLPUMi%2BeUTVEd%2Fwgc4fggtifc0Alkjm6SmeEd%2FivWgikHmGCC3bQoSqKCBsZamtKbXwuaoL4rdqQxUATYcmusQJjNHuikW227kWEvBS7YXH22qjgOQvwX24iDS%2BI%2FHe%2FQqasBtk4KveNoCXcDB%2B6NIC2IMsEc3%2FBl4o%2B7RIFZN5eETAw0T0%2FA74YOEAVW4aDU81pKx%2Bo%2BNpvp7BQ38UPdijKgXKQpxWfdZjCiOJhpluFXp6TFkolg5FXlgooFpafAiWFiNLsaQopMSvWAzwpweG5g7je9y5sgtztw5EUoPbRF%2FUOyhCw2LbMw1PrJnx9qV6gEr1%2B48MAf%2FDfZvJ66RJ0T3GHJi21KlZ%2Fn2U%2FhK1crNQ%2FoTZEKs5dia%2BcrEos2n5GpCFO0zdrv589sWqrZZtPu83FOREKaspO5xeo1KyPz156S2yDZxSldrn16tbHhUSFNaQAZ0Dezm5zcoS%2BZvPw8zRulkEzQJuIPbP1%2FZs%2BjYg85RVIZHiXScX6FKY%2FN5tyqADDJyr847tECVysITcdxUS5WTgf18iyqHvRbeLSgj9ZYqj%2BepHcjo8Lkql5dTVZfR4RtVPp%2Bn5GXIq8A6xPMGUFF9HR5r6Gb27i%2BVK94mV6BGHPOuskY%2BXhVA1wSZp1wyjtyQt%2FTxkcotncgJOTvnSP2o2mDxxp2Hjxxn5uNHDu%2FcuFi1wXdu3Ly%2F3W5%2BijKycs9xfpTjO5YoI6%2BSC3y2qXH7mQPoD6yhd6M5tA0iF0Ro1Kch1aowH%2Fbqz8DRRpiE%2FJwSmykUSEuj4Y4PIwrxsKjxVwWZIeUcwBx1CjIv1cY0uKZZIT4mB2SSP%2ByarQC%2FD4NjVPbbNuWzAiMePB3pogA%2FdnpkcIeu59MK0JoSeXcL6kNkjG866EKe5jg6%2FSpoDi%2Fhe8E6qMK0w8xQAh3Ngg9G8snC1O%2F%2Ft%2FjICKWnn0DPoc%2FlKaWnh0kF9092FrMln4wECRL4OBC1Uf55U2mpEUgdWh2vGI4xSP7gMKV3j%2FESTYfm3XwNPkUv4MTGQGG3WfbVZ%2BFe9hoMI6UfWr3%2BBHG7RsA7NMXEFJS3Rtk8msRZdLCbigRTuH2mrXpjZMF9BBkUm2OKuxUgFgKOsG%2BeDQQ2TUurw%2BUZFvLcKvU4y3Z9xRj4RABZtk6gC9Rw8uDWdeoeq7buO8lmDA39eIFEDipEwNFbnOUE5AjSBQU9qTawdEIy0CpVj%2BAa1R6zY6BY9Qo5IhO5U%2BGTiWeVBnKF70yHT0a6CsgQ0NGfMNDH6yR1CKgAvUsXalc6oiy1ibQM8kMx7xaQgfHyXA6hRy5lCJSJVrm7%2BjJw9Y2x%2B6%2F3morIIC%2FHpTDVo2R0Een%2FNGTtPb2gi1AWHQeJ0N%2FuZkVDKDnjgYxqC4lGeWTBbJEKFwvJcxLC%2FmRFCjTjcmRyBTYT5XyypCtom0TxR4XYDrksWYEHuV1JHC878%2BjJx3vzo7te86gUUq2Vibdg7bdq3aZdd9i0blUZP90PTj%2Fl0Z5gI5VCM%2FyUPI3OJq%2F9xBY1Jf94oytjCLkGiPUO6rlnlY5XSBjzo5fmlH2ssB%2Boi98q22uVekVpSVGlaLVfouJIIV%2BJWJWlloOZwcrCxWSoUXputGuHuLKEQBSGDwaDQmAxrVFtyuDaswB2UIs4a395ueKKCcyd7g4wSX%2B%2BxJ8cWequDpMVA8nVjsiGiIEsGzReWiUrhrr0SmQOtkQMZZUtxaIvdG4xWGJbMmizmW0eo1W2aTPECjsEw3n2qDi8Cpk9ajDezr66B4NfNoqyL2CGwrf0kPRfPpRv7ZjCKe9UMEngjdRilo23UYd5hHeJmEkGVIwgwyrW74iYL%2FEi9VhBVF5RHdbgKs%2FLBqswmWdtWElQnlEc1mKEH9MN63EHPyMGS%2FKfhIjFsnzmn6hYLM2myndKNFif2yvbymbxLWyUwlfHHgy%2BjfMp5eOHpOQtHo%2FH4%2FEY7x8MZ7AAyatDDgAAAABJRU5ErkJggg%3D%3D)](https://hub.gke2.mybinder.org/user/julienguegan-notebooks_blog-z8qd9bd5/notebooks/fractale.ipynb)</content><author><name>Julien Guégan</name></author><category term="blog" /><category term="Fractal" /><category term="Python" /><category term="Julia" /><category term="Mandelbrot" /><category term="Récursivité" /><category term="Infini" /><summary type="html">Elles sont présentes dans les forêts tropicales, à la pointe de la recherche médicale, dans les films et partout où reigne la communication sans fil. Ce mystère de la nature a enfin été percé à jour. “Bon sang ! Mais c’est bien sûr !”. Peut-être n’avez vous jamais entendu parler de ces formes étranges, pourtant elles sont partout autour de vous. Leur nom : les fractales. reportage ARTE – à la recherche de la dimension cachée Introduction Comme vous l’aurez compris si vous avez regardé l’excellent documentaire d’ARTE présenté ci-dessus, les fractales sont des objets géométriques infiniment morcelés qui ont la particularité de présenter des structures similaires à toutes les échelles. Ce type de géométrie permet de modéliser avec de simples formules récursives des figures infiniment complexes mais aussi de décrire des phénomènes naturels (motifs des flocons, chemin pris par la foudre, forme d’un choux de romanesco, structure des poumons …) et de trouver des applications dans des domaines technologiques (antennes, transistors, génération graphique de paysages …). Note: Les fractales qu’on trouve dans la nature sont des approximations finies des vrais objets mathématiques. Les fractales sont notamment caractérisées par la notion contre intuitive de dimension non entière. En effet, on peut définir une régle générale de mise à l’échelle qui relie la mesure $N$, un facteur d’échelle $\varepsilon$ et la dimension $D$ : \[N = \varepsilon^{-D}\] Par exemple, pour une figure géométrique usuelle comme le carré, sa dimension est $D=2$ et si on le subdivise en $3$ son aire est $N=9$, on a bien $9=\frac{1}{3}^{-2}$. On peut appliquer ce même raisonnement pour un cube ou même une ligne. Maintenant, on cherche à trouver la dimension d’une figure fractale simple. La formule précédente nous donne : \[D = -\frac{\log N}{\log \varepsilon}\] Si on s’intéresse à une figure telle que la courbe de Von Koch qui consiste, à partir d’un segment, construire récursivement des triangles équilatéraux sur chaque sous-segment (cf animation ci-dessous). En comptant les segments à chaque nouvelle mise à l’échelle, on comprends que la longueur de la courbe de Koch est multipliée par $4$ pour chaque mise à l’échelle $\varepsilon=\frac{1}{3}$ (on divise les segments par 3). On trouve donc que sa dimension est $D = \frac{\log 4}{\log 3} \approx 1.26$. Il ne s’agit pas d’une simple courbe unidimensionelle, ni d’une surface mais quelque chose “entre les deux”. Note: L’approche présentée précédemment est conceptuelle. Une définition rigoureuse et définie pour tout ensemble est la dimension de Hausdorff. Elle est peu aisée à mettre en oeuvre… On peut différencier 3 catégories principales de fractale : les systèmes de fonctions itérées. Ils ont une règle géométrique fixe comme le flocon de Von Koch, le tapis de Sierpinski, la courbe de Peano. les fractales aléatoires. Elles sont générées par un processus stochastiques comme dans la nature ou les paysages fractales. les ensembles définies par une relation de récurrence en chaque point d’un espace. On peut citer l’ensemble de Julia, de mandelbrot, de lyapunov. On les appelle parfois en anglais des Escape-time fractals. Ensemble de Julia L’ensemble de Julia associé à un nombre complexe $c$ fixé est l’ensemble des valeurs initiales $z_0$ pour lesquelles la suite suivante est bornée : \[\left\{ \begin{array}{ll} z_0 \in \mathbb{C} \\ z_{n+1} = z_n^2 + c \end{array} \right.\] Pour générer un ensemble de Julia informatiquement, l’idée est de discrétiser l’espace dans un intervalle fixé pour avoir un nombre fini de valeur $z_0$ pour lesquelle on va tester la convergence de la suite. # INITIALIZATION # value of c fixed c_reel, c_imag = 0.3, 0.5 # interval limit x_min, x_max = -1, 1 y_min, y_max = -1, 1 # discretization size = 5000 x_step = (x_max - x_min)/size y_step = (y_max - y_min)/size M = np.zeros((size,size)) Pour pouvoir travailler avec des nombres complexes, j’ai choisi de décomposer la partie réelle z_reel et la partie imaginaire z_image. Ensuite, on teste la convergence pour un point donné en regardant si on a pas dépassé un nombre fini d’itération n_iter &amp;lt; itermax. On peut également, en plus, vérifier que la suite $(z_n)$ est divergente si son module est strictement supérieur à $2$, z_reel**2 + z_imag**2 &amp;lt; 4 (cf demonstration). Finalement, on peut remplir une matrice M de $0$ ou de $1$ selon le test de convergence. Mais, pour un rendu visuelle final plus estéthique on peut également remplir la matrice M selon le taux de convergence estimé avec n_iter/itermax. # LOOP ON ALL PIXEL = COMPLEX PLANE for i in (range(size)): for j in range(size): n_iter = 0 # convert pixel position to a complex number z_reel = i * x_step + x_min z_imag = j * y_step + y_min # update sequence until convergence while (z_reel**2 + z_imag**2 &amp;lt; 4) and (n_iter &amp;lt; itermax): z_reel, z_imag = z_reel**2 - z_imag**2 + c_reel, 2*z_imag*z_reel + c_imag n_iter = n_iter + 1 # color image according to convergence rate M[j,i] = 255*n_iter/itermax Astuce: En python, on aurait pu directement utiliser la fonction complex() pour avoir un objet complexe. Dans ce cas, les variables z_reel et z_imag seraient inutiles et on pourrait directement récupérer la valeur absolue et mettre au carré une unique variable complexe z. Finalement, on peut générer des ensembles de Julia pour différentes valeurs de $c$ fixées et pour changer le visuel on peut s’amuser à tester différentes colormap. Ci-dessous quelques résultats que j’ai généré. On remarque que les figures obtenues varient grandement en fonction de la valeur du complexe $c$ choisie. En fait, on peut générer les ensembles de julia pour une suite de complexes consécutifs pour voir comment les figures évoluent et en faire une animation. Ensemble de Mandelbrot L’ensemble de Mandelbrot est fortement lié aux ensembles de Julia, en effet on peut définir l’ensemble de Mandelbrot $M$ comme l’ensemble des complexes $c$ pour lesquels l’ensemble de Julia $J_c$ correspondant est connexe, c’est-à-dire qu’il est fait d’un seul morceau. On peut dire que l’ensemble de Mandelbrot représente une carte des ensembles de Julia. Et, contrairement au nom qu’il porte, c’est les mathématiciens Julia et Fatou qui l’ont découvert et qui ont montré que la définition précédente est équivalente à l’ensemble des points $c$ du plan complexe $\mathbb{C}$ pour lesquels la suite suivante est bornée : \[\left\{ \begin{array}{ll} z_0 = 0 \\ z_{n+1} = z_n^2 + c \end{array} \right.\] Cette définition est très similaire à celle de l’ensemble de Julia à la différence qu’on s’intéresse à la variable $c$. Dans le code précédent, il faudrait modifier la ligne z_reel = i * x_step + x_min par c_reel = i * x_step + x_min et fixé z_reel = 0 (idem pour la partie imaginaire). On obtient la figure suivante : Note: Benoît Mandelbrot est le fondateur de la théorie fractale avec notamment son article “How Long Is the Coast of Britain ? Statistical Self-Similarity and Fractional Dimension” en 1967. C’est également lui qui obtient pour la première fois, une visualisation par ordinateur de cet ensemble. Logiciels La génération de fractale n’est pas une tâche facile : beaucoup de paramètres peuvent être à prendre en compte et les temps de calcul sont souvent long. Dans les figures que j’ai généré, on ne voit pas au premiers abords le caractère auto-similaire des fractales, il faudrait changer d’échelle en zoomant de plus en plus profond sur un point du plan. Il existe de nombreux logiciels générateur de fractal gratuits disponibles. Ils sont souvent optimisés pour faire du multi-processing ou du calcul sur GPU, possèdent une interface graphique pour gérer les nombreux paramètres et sont parfois capables de créer des objets 3D (comme les 3 affichés ci-dessous). Une liste assez complète est disponible ici. Mandelbul3D Mandelbuler Fragmentarium Et pour ceux qui ne veulent pas se compliquer et juste se laisser porter par la géométrie psychadélique des fractales sans effort, vous pourrez trouver sur internet des gens qui ont déjà fait le travail à votre place. On trouve sur youtube une floppée de vidéos comme par exemple The Hardest Trip - Mandelbrot Fractal Zoom qui zoome pendant 2h30 sur un point précis du plan complexe.</summary></entry><entry xml:lang="fr"><title type="html">Créer un blog avec Jekyll : Markdown, Github, Latex</title><link href="http://localhost:4000/posts/fr/2021-07-25-creer_un_blog/" rel="alternate" type="text/html" title="Créer un blog avec Jekyll : Markdown, Github, Latex" /><published>2021-07-25T21:34:30+02:00</published><updated>2021-07-25T21:34:30+02:00</updated><id>http://localhost:4000/posts/fr/creer_un_blog</id><content type="html" xml:base="http://localhost:4000/posts/fr/2021-07-25-creer_un_blog/">J&apos;écris le 1er post de ce blog pour parler tout simplement de comment créer un blog comme celui-ci. La raison principale qui m&apos;a poussé à utiliser Jekyll pour créer un blog est la possibilité d&apos;écrire facilement des équations $\LaTeX$. En effet, j&apos;avais auparavant essayé avec WordPress mais aucune des solutions que j&apos;avais pu testé m&apos;ont véritablement convaincu. En faisant donc quelques recherches sur le sujet, je suis tombé sur [Jekyll](https://jekyllrb.com/) qui semble être utilisé par un bon nombre de blogger scientifique et informatique. Jekyll est un générateur de site statique c&apos;est-à-dire que les pages web créées ne changent pas en fonction de l&apos;internaute qui les visite : tout le monde voit le même contenu. A l&apos;inverse d&apos;un site dynamique qui génère son contenu selon des caractéristiques de la demande (heure, adresse IP, compte utilisateur, formulaire ...). De plus, Jekyll permet d&apos;éditer du texte en **Markdown** en se basant sur la librairie *Kramdown* qui convertie automatiquement du texte Markdown en HTML. Jekyll est donc une solution tout à fait adaptée pour l&apos;écriture d&apos;un blog web scientifique, mais sachez tout de même que ce n&apos;est pas l&apos;unique solution sur le sujet puisque [Hugo](https://gohugo.io/) est également un Framework populaire similaire à Jekyll.

## Installation

Pour utiliser Jekyll sous Windows, une façon de faire est de passer par le [Sous-système Windows pour Linux](https://docs.microsoft.com/fr-fr/windows/wsl/about) qui permet de profiter d&apos;un environnement Linux. Pour ce faire, vous pouvez simplement télécharger l&apos;application WSL en passant par le Microsoft Store.

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/windows_subsytem_linux.png&quot; width=&quot;60%&quot;/&gt;
&lt;/p&gt;

Ensuite, il faut installer **Ruby** qui est le langage de programmation utilisé par Jekyll pour fonctionner. En ouvrant l&apos;application Ubuntu précédemment téléchargée et en rentrant les commandes suivantes les unes après les autres dans la console, Ruby devrait être présent sur votre ordinateur:

```bash
sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y
sudo apt-add-repository ppa:brightbox/ruby-ng
sudo apt-get update
sudo apt-get install ruby2.5 ruby2.5-dev build-essential dh-autoreconf
```

**Note:** Pour ceux qui ne veulent pas passer par le sous-système Linux, vous pouvez plus simplement télécharger l&apos;installateur de Ruby ici : [https://rubyinstaller.org/downloads/](https://rubyinstaller.org/downloads/). Vous pourrez ensuite lancer les commandes Jekyll qui suivent dans une fenêtre d&apos;invite de commande Windows.
{: .notice--primary}

Finalement, mettez à jour la commande **gem** et installez **Jekyll** :

```bash
gem update
gem install jekyll bundler
```

## Déploiement

Vous avez désormais tous les pré-requis minimums pour créer un blog avec Jekyll. Commencez par créer un répertoire où vos fichiers de blog seront stockés, créez votre blog avec Jekyll puis construisez le site et rendez le disponible sur un serveur local :

```bash
mkdir mon_blog
jekyll new mon_blog
cd mon_blog
bundle exec jekyll serve
```

Pour naviguer sur votre site en local, rendez-vous sur l&apos;adresse [http://localhost:4000](http://localhost:4000).

**Astuce:** Utilisez l&apos;option ```--livereload``` pour rafraîchir automatiquement la page à chaque changement que vous faites dans les fichiers sources : ```bundle exec jekyll serve --livereload```
{: .notice--info}

Vous avez maintenant générer un site statique avec Jekyll : Bravo, vous pouvez être fier de vous ! Mais je suppose que ça ne vous suffit pas, vous voulez également le rendre disponible à tous le monde. L&apos;une des manières de faire est de l&apos;héberger sur **Github**. En effet, Jekyll a été développé par le fondateur de Github et le déploiement d&apos;un site est possible en utilisant l&apos;outil [Github-Pages](https://pages.github.com/). Il suffit de créer un répertoire git ayant pour nom ```&lt;username_github&gt;.github.io```, générer votre site Jekyll dans ce répertoire et pousser ce répertoire sur Github. Votre blog ```&lt;username_github&gt;.github.io``` sera désormais visible depuis n&apos;importe quel web explorer.

**Attention:** Votre répertoire git doit être de visibilité public. Pour plus d&apos;informations sur le déploiement avec Github, visitez ce [site](https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/creating-a-github-pages-site-with-jekyll)
{: .notice--warning}

## Utilisation

Maintenant que vous avez généré votre site, Jekyll a normalement dû créer automatiquement des sous-répertoires et des fichiers dans votre répertoire principal. C&apos;est en modifiant ces fichiers que vous pourrez configurer et personnaliser votre site. Ci-dessous, quelques précisions sur la fonction de certains de ces répertoires et fichiers :

| FICHIER/REPERTOIRE |      DESCRIPTION      |
|--------------------|-----------------------|
| **_config.yml**    | Stocke les données de configuration. Modifier ici le nom du site, les informations de l&apos;auteurs, les plugins, les extensions ...|
| **_includes**      | Les fichiers externes qui permettent d&apos;ajouter des fonctionnalités et être utiliser par les fichiers de Template. |
| **_layouts**       | Les modèles d&apos;affichage qui enveloppent les posts du blog. On peut choisir des mises en page différentes pour chaque post. |
| **_posts**         | Le contenu de votre blog, c&apos;est-à-dire les posts que vous allez écrire. Leurs noms doivent suivre le format : `YEAR-MONTH-DAY-title.MARKUP`|
| **_data**          | Les données externes automatiquement chargées et qui sont utilisées par votre site doivent être stockées ici. |

Par défaut, Jekyll génère le site avec le thème [minima](https://github.com/jekyll/minima) qui permet d&apos;avoir une version simple et épuré mais il existe un grand nombre de [templates](http://jekyllthemes.org/) qui vous permet de personnaliser l&apos;apparence de votre site. Pour ma part, j&apos;ai choisi d&apos;utiliser [minimal-mistakes](https://mmistakes.github.io/minimal-mistakes/) qui est assez simple tout en offrant un grand nombre de possibilité.

**Exemple:** Minimal Mistakes met à disposition un [starter](https://github.com/mmistakes/mm-github-pages-starter/generate) qui permet de rapidement et automatiquement mettre en place les fichiers sur votre compte Github et avoir un site hébergé par Github Pages.
{: .notice--info}

Une fonctionnalité qui m&apos;intéressait particulièrement pour mon blog est de pouvoir facilement ajouter des équations. Le langage le plus connu pour écrire des mathématiques est **Latex** qui est généralement utiliser avec son compilateur pour générer des pdf. Pour le web, la bibliothèque populaire **MathJax** écrite en Javascript est capable d&apos;afficher des équations Latex sur la plupart des navigateurs web courant. Cependant, j&apos;ai préféré choisir la librairie **Katex** qui a l&apos;avantage d&apos;être plus rapide à charger que MathJax quand il y a beaucoup d&apos;équation à convertir (voir exemple ci-dessous, Katex à gauche et MathJax à droite).

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;/assets/images/katex_vs_latex.gif&quot; width=&quot;100%&quot;/&gt;
&lt;/p&gt;

Pour installer Katex sur votre blog, il faut copier/coller les lignes suivantes dans le fichier *head.html*, elle permettent de charger la librairie Katex sur votre site :

```javascript
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css&quot; integrity=&quot;sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn&quot; crossorigin=&quot;anonymous&quot;&gt;
&lt;script defer src=&quot;https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js&quot; integrity=&quot;sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;
&lt;script defer src=&quot;https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js&quot; integrity=&quot;sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl&quot; crossorigin=&quot;anonymous&quot; onload=&quot;renderMathInElement(document.body);&quot;&gt;&lt;/script&gt;
```

Voilà, c&apos;est tout pour moi. Il y a sûrement des détails à ajouter et des explication supplémentaires à donner, vous pouvez trouver plus d&apos;informations dans les sites ci-dessous et vous pouvez également me poser des questions dans la section commentaire.

## References

- [jekyll](https://jekyllrb.com/)
- [github-pages](https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/creating-a-github-pages-site-with-jekyll)
- [minimal-mistakes](https://mmistakes.github.io/minimal-mistakes/)
- [katex](https://katex.org/docs/autorender.html)
- [kramdown](https://kramdown.gettalong.org/index.html)</content><author><name>Julien Guégan</name></author><category term="blog" /><category term="Jekyll" /><category term="Ruby" /><category term="Markdown" /><category term="Kramdown" /><category term="Github" /><category term="Latex" /><category term="Katex" /><category term="Blog" /><summary type="html">J’écris le 1er post de ce blog pour parler tout simplement de comment créer un blog comme celui-ci. La raison principale qui m’a poussé à utiliser Jekyll pour créer un blog est la possibilité d’écrire facilement des équations $\LaTeX$. En effet, j’avais auparavant essayé avec WordPress mais aucune des solutions que j’avais pu testé m’ont véritablement convaincu. En faisant donc quelques recherches sur le sujet, je suis tombé sur Jekyll qui semble être utilisé par un bon nombre de blogger scientifique et informatique. Jekyll est un générateur de site statique c’est-à-dire que les pages web créées ne changent pas en fonction de l’internaute qui les visite : tout le monde voit le même contenu. A l’inverse d’un site dynamique qui génère son contenu selon des caractéristiques de la demande (heure, adresse IP, compte utilisateur, formulaire …). De plus, Jekyll permet d’éditer du texte en Markdown en se basant sur la librairie Kramdown qui convertie automatiquement du texte Markdown en HTML. Jekyll est donc une solution tout à fait adaptée pour l’écriture d’un blog web scientifique, mais sachez tout de même que ce n’est pas l’unique solution sur le sujet puisque Hugo est également un Framework populaire similaire à Jekyll. Installation Pour utiliser Jekyll sous Windows, une façon de faire est de passer par le Sous-système Windows pour Linux qui permet de profiter d’un environnement Linux. Pour ce faire, vous pouvez simplement télécharger l’application WSL en passant par le Microsoft Store. Ensuite, il faut installer Ruby qui est le langage de programmation utilisé par Jekyll pour fonctionner. En ouvrant l’application Ubuntu précédemment téléchargée et en rentrant les commandes suivantes les unes après les autres dans la console, Ruby devrait être présent sur votre ordinateur: sudo apt-get update -y &amp;amp;&amp;amp; sudo apt-get upgrade -y sudo apt-add-repository ppa:brightbox/ruby-ng sudo apt-get update sudo apt-get install ruby2.5 ruby2.5-dev build-essential dh-autoreconf Note: Pour ceux qui ne veulent pas passer par le sous-système Linux, vous pouvez plus simplement télécharger l’installateur de Ruby ici : https://rubyinstaller.org/downloads/. Vous pourrez ensuite lancer les commandes Jekyll qui suivent dans une fenêtre d’invite de commande Windows. Finalement, mettez à jour la commande gem et installez Jekyll : gem update gem install jekyll bundler Déploiement Vous avez désormais tous les pré-requis minimums pour créer un blog avec Jekyll. Commencez par créer un répertoire où vos fichiers de blog seront stockés, créez votre blog avec Jekyll puis construisez le site et rendez le disponible sur un serveur local : mkdir mon_blog jekyll new mon_blog cd mon_blog bundle exec jekyll serve Pour naviguer sur votre site en local, rendez-vous sur l’adresse http://localhost:4000. Astuce: Utilisez l’option --livereload pour rafraîchir automatiquement la page à chaque changement que vous faites dans les fichiers sources : bundle exec jekyll serve --livereload Vous avez maintenant générer un site statique avec Jekyll : Bravo, vous pouvez être fier de vous ! Mais je suppose que ça ne vous suffit pas, vous voulez également le rendre disponible à tous le monde. L’une des manières de faire est de l’héberger sur Github. En effet, Jekyll a été développé par le fondateur de Github et le déploiement d’un site est possible en utilisant l’outil Github-Pages. Il suffit de créer un répertoire git ayant pour nom &amp;lt;username_github&amp;gt;.github.io, générer votre site Jekyll dans ce répertoire et pousser ce répertoire sur Github. Votre blog &amp;lt;username_github&amp;gt;.github.io sera désormais visible depuis n’importe quel web explorer. Attention: Votre répertoire git doit être de visibilité public. Pour plus d’informations sur le déploiement avec Github, visitez ce site Utilisation Maintenant que vous avez généré votre site, Jekyll a normalement dû créer automatiquement des sous-répertoires et des fichiers dans votre répertoire principal. C’est en modifiant ces fichiers que vous pourrez configurer et personnaliser votre site. Ci-dessous, quelques précisions sur la fonction de certains de ces répertoires et fichiers : FICHIER/REPERTOIRE DESCRIPTION _config.yml Stocke les données de configuration. Modifier ici le nom du site, les informations de l’auteurs, les plugins, les extensions … _includes Les fichiers externes qui permettent d’ajouter des fonctionnalités et être utiliser par les fichiers de Template. _layouts Les modèles d’affichage qui enveloppent les posts du blog. On peut choisir des mises en page différentes pour chaque post. _posts Le contenu de votre blog, c’est-à-dire les posts que vous allez écrire. Leurs noms doivent suivre le format : YEAR-MONTH-DAY-title.MARKUP _data Les données externes automatiquement chargées et qui sont utilisées par votre site doivent être stockées ici. Par défaut, Jekyll génère le site avec le thème minima qui permet d’avoir une version simple et épuré mais il existe un grand nombre de templates qui vous permet de personnaliser l’apparence de votre site. Pour ma part, j’ai choisi d’utiliser minimal-mistakes qui est assez simple tout en offrant un grand nombre de possibilité. Exemple: Minimal Mistakes met à disposition un starter qui permet de rapidement et automatiquement mettre en place les fichiers sur votre compte Github et avoir un site hébergé par Github Pages. Une fonctionnalité qui m’intéressait particulièrement pour mon blog est de pouvoir facilement ajouter des équations. Le langage le plus connu pour écrire des mathématiques est Latex qui est généralement utiliser avec son compilateur pour générer des pdf. Pour le web, la bibliothèque populaire MathJax écrite en Javascript est capable d’afficher des équations Latex sur la plupart des navigateurs web courant. Cependant, j’ai préféré choisir la librairie Katex qui a l’avantage d’être plus rapide à charger que MathJax quand il y a beaucoup d’équation à convertir (voir exemple ci-dessous, Katex à gauche et MathJax à droite). Pour installer Katex sur votre blog, il faut copier/coller les lignes suivantes dans le fichier head.html, elle permettent de charger la librairie Katex sur votre site : &amp;lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css&quot; integrity=&quot;sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn&quot; crossorigin=&quot;anonymous&quot;&amp;gt; &amp;lt;script defer src=&quot;https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js&quot; integrity=&quot;sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8&quot; crossorigin=&quot;anonymous&quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script defer src=&quot;https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js&quot; integrity=&quot;sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl&quot; crossorigin=&quot;anonymous&quot; onload=&quot;renderMathInElement(document.body);&quot;&amp;gt;&amp;lt;/script&amp;gt; Voilà, c’est tout pour moi. Il y a sûrement des détails à ajouter et des explication supplémentaires à donner, vous pouvez trouver plus d’informations dans les sites ci-dessous et vous pouvez également me poser des questions dans la section commentaire. References jekyll github-pages minimal-mistakes katex kramdown</summary></entry></feed>